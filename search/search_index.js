var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>      Probabilistic programming with (parallel &amp; differentiable) programmable inference.    </p>"},{"location":"index.html#what-is-genjax","title":"\ud83d\udd0e What is GenJAX?","text":"<p>Gen is a multi-paradigm (generative, differentiable, incremental) language for probabilistic programming focused on generative functions: computational objects which represent probability measures over structured sample spaces.</p> <p>GenJAX is an implementation of Gen on top of JAX - exposing the ability to programmatically construct and manipulate generative functions, as well as JIT compile + auto-batch inference computations using generative functions onto GPU devices.</p> Jump into the notebooks! <p>GenJAX is part of a larger ecosystem of probabilistic programming tools based upon Gen. Explore more...</p>"},{"location":"index.html#quickstart","title":"Quickstart","text":"<p>To install GenJAX, run</p> <pre><code>pip install genjax\n</code></pre> <p>Then install JAX using this guide to choose the command for the architecture you're targeting. To run GenJAX without GPU support:</p> <pre><code>pip install jax[cpu]~=0.4.24\n</code></pre> <p>On a Linux machine with a GPU, run the following command:</p> <pre><code>pip install jax[cuda12]~=0.4.24\n</code></pre>"},{"location":"index.html#quick-example","title":"Quick example","text":"<p>The following code snippet defines a generative function called <code>beta_bernoulli</code> that</p> <ul> <li>takes a shape parameter <code>beta</code></li> <li>uses this to create and draw a value <code>p</code> from a Beta   distribution</li> <li>Flips a coin that returns 1 with probability <code>p</code>, 0 with probability <code>1-p</code> and   returns that value</li> </ul> <p>Then, we create an inference problem (by specifying a posterior target), and utilize sampling importance resampling to give produce single sample estimator of <code>p</code>.</p> <p>We can JIT compile that entire process, run it in parallel, etc - which we utilize to produce an estimate for <code>p</code> over 50 independent trials of SIR (with K = 50 particles).</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\nfrom genjax import beta, flip, gen, Target, ChoiceMap\nfrom genjax.inference.smc import ImportanceK\n\n# Create a generative model.\n@gen\ndef beta_bernoulli(\u03b1, \u03b2):\n    p = beta(\u03b1, \u03b2) @ \"p\"\n    v = flip(p) @ \"v\"\n    return v\n\n@jax.jit\ndef run_inference(obs: bool):\n    # Create an inference query - a posterior target - by specifying\n    # the model, arguments to the model, and constraints.\n    posterior_target = Target(beta_bernoulli, # the model\n                              (2.0, 2.0), # arguments to the model\n                              ChoiceMap.d({\"v\": obs}), # constraints\n                            )\n\n    # Use a library algorithm, or design your own - more on that in the docs!\n    alg = ImportanceK(posterior_target, k_particles=50)\n\n    # Everything is JAX compatible by default.\n    # JIT, vmap, to your heart's content.\n    key = jax.random.key(314159)\n    sub_keys = jax.random.split(key, 50)\n    _, p_chm = jax.vmap(alg.random_weighted, in_axes=(0, None))(\n        sub_keys, posterior_target\n    )\n\n    # An estimate of `p` over 50 independent trials of SIR (with K = 50 particles).\n    return jnp.mean(p_chm[\"p\"])\n\n(run_inference(True), run_inference(False))\n</code></pre> <pre><code>(Array(0.6039314, dtype=float32), Array(0.3679334, dtype=float32))\n</code></pre>"},{"location":"index.html#references","title":"References","text":"<p>Many bits of knowledge have gone into this project -- you can find many of these bits at the MIT Probabilistic Computing Project page under publications. Here's an abbreviated list of high value references:</p> <ul> <li>Marco Cusumano-Towner's thesis on Gen</li> <li>The main Gen.jl repository</li> <li>(Trace types) (Lew et al) trace types</li> <li>(RAVI) (Lew et al) Recursive auxiliary-variable inference</li> <li>(GenSP) Alex Lew's Gen.jl implementation of GenSP</li> <li>(ADEV) (Lew &amp; Huot, et al) Automatic differentiation of expected values of probabilistic programs</li> </ul>"},{"location":"index.html#jax-influences","title":"JAX influences","text":"<p>This project has several JAX-based influences. Here's an abbreviated list:</p> <ul> <li>This notebook on static dispatch (Dan Piponi)</li> <li>Equinox (Patrick Kidger's work on neural networks via callable Pytrees)</li> <li>Oryx (interpreters and interpreter design)</li> </ul>"},{"location":"index.html#acknowledgements","title":"Acknowledgements","text":"<p>The maintainers of this library would like to acknowledge the JAX and Oryx maintainers for useful discussions and reference code for interpreter-based transformation patterns.</p>  Created and maintained by the MIT Probabilistic Computing Project. All code is licensed under the Apache 2.0 License."},{"location":"developing.html","title":"Developer's Guide","text":"<p>This guide describes how to complete various tasks you'll encounter when working on the GenJAX codebase.</p>"},{"location":"developing.html#development-environment","title":"Development environment","text":"<p>This project uses:</p> <ul> <li>poetry for dependency management</li> <li>nox to automate testing/linting/building.</li> <li>mkdocs to generate static documentation.</li> </ul>"},{"location":"developing.html#commit-hooks","title":"Commit Hooks","text":"<p>We use pre-commit to manage a series of git pre-commit hooks for the project; for example, each time you commit code, the hooks will make sure that your python is formatted properly. If your code isn't, the hook will format it, so when you try to commit the second time you'll get past the hook.</p> <p>All hooks are defined in <code>.pre-commit-config.yaml</code>. To install these hooks, install <code>pre-commit</code> if you don't yet have it. I prefer using pipx so that <code>pre-commit</code> stays globally available.</p> <pre><code>pipx install pre-commit\n</code></pre> <p>Then install the hooks with this command:</p> <pre><code>pre-commit install\n</code></pre> <p>Now they'll run on every commit. If you want to run them manually, run the following command:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"developing.html#option-1-development-environment-setup-with-poetry","title":"(Option 1): Development environment setup with <code>poetry</code>","text":""},{"location":"developing.html#step-1-setting-up-the-environment-with-poetry","title":"Step 1: Setting up the environment with <code>poetry</code>","text":"<p>First, install <code>poetry</code> to your system.</p> <p>Assuming you have <code>poetry</code>, here's a simple script to setup a compatible development environment - if you can run this script, you have a working development environment which can be used to execute tests, build and serve the documentation, etc.</p> <pre><code>conda create --name genjax-py311 python=3.11 --channel=conda-forge\nconda activate genjax-py311\npip install nox\npip install nox-poetry\ngit clone https://github.com/genjax-dev/genjax-chi\ncd genjax\npoetry self add \"poetry-dynamic-versioning[plugin]\"\npoetry install\npoetry run jupyter-lab\n</code></pre> <p>You can test your environment with:</p> <pre><code>nox -r\n</code></pre>"},{"location":"developing.html#step-2-choose-a-jaxlib","title":"Step 2: Choose a <code>jaxlib</code>","text":"<p>GenJAX does not manage the version of <code>jaxlib</code> that you use in your execution environment. The exact version of <code>jaxlib</code> can change depending upon the target deployment hardware (CUDA, CPU, Metal). It is your responsibility to install a version of <code>jaxlib</code> which is compatible with the JAX bounds (<code>jax = \"^0.4.24\"</code> currently) in GenJAX (as specified in <code>pyproject.toml</code>).</p> <p>For further information, see this discussion.</p> <p>You can likely install CUDA compatible versions by following environment setup above with a <code>pip</code> installation of the CUDA-enabled JAX.</p> <p>When running any of the <code>nox</code> commands, append <code>-- &lt;jax_specifier&gt;</code> to install the proper <code>jaxlib</code> into the session. For example,</p> <pre><code>nox -s tests -- cpu\n</code></pre> <p>will run the tests with the CPU <code>jaxlib</code> installed, while</p> <pre><code>nox -s tests -- cuda12\n</code></pre> <p>will install the CUDA bindings. By default, the CPU bindings will be installed.</p>"},{"location":"developing.html#option-2-self-managed-development-environment-with-requirementstxt","title":"(Option 2): Self-managed development environment with <code>requirements.txt</code>","text":""},{"location":"developing.html#using-requirementstxt","title":"Using <code>requirements.txt</code>","text":"<p>This is not the recommended way to develop on <code>genjax</code>, but may be required if you want to avoid environment collisions with <code>genjax</code> installing specific versions of <code>jax</code> and <code>jaxlib</code>.</p> <p><code>genjax</code> includes a <code>requirements.txt</code> file which is exported from the <code>pyproject.toml</code> dependency requirements -- but with <code>jax</code> and <code>jaxlib</code> removed.</p> <p>If you wish to setup a usable environment this way, you must ensure that you have <code>jax</code> and <code>jaxlib</code> installed in your environment, then:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>This should install a working environment - subject to the conditions that your version of <code>jax</code> and <code>jaxlib</code> resolve with the versions of packages in the <code>requirements.txt</code></p>"},{"location":"developing.html#documentation-environment-setup","title":"Documentation environment setup","text":"<p>GenJAX builds documentation using an insiders-only version of mkdocs-material. GenJAX will attempt to fetch this repository during the documentation build step.</p> <p>Run the following command to fully build the documentation:</p> <pre><code>nox -r -s docs-build\n</code></pre> <p>This command will use <code>mkdocs</code> to build the static site.</p> <p>To view the generated site, run:</p> <pre><code>nox -r -s docs-serve\n</code></pre> <p>or to run both commands in sequence:</p> <pre><code>nox -r -s docs-build-serve\n</code></pre>"},{"location":"developing.html#releasing-genjax","title":"Releasing GenJAX","text":"<p>Published GenJAX artifacts live on PyPI and are published automatically by GitHub with each new release.</p>"},{"location":"developing.html#release-checklist","title":"Release checklist","text":"<p>Before cutting a new release:</p> <ul> <li>Update README.md to reference new GenJAX versions</li> <li>Make sure that the referenced <code>jax</code> and <code>jaxlib</code> versions match the version   declared in <code>pyproject.toml</code></li> </ul>"},{"location":"developing.html#releasing-via-github","title":"Releasing via GitHub","text":"<ul> <li>Visit https://github.com/genjax-dev/genjax-chi/releases/new to create a new release.</li> <li>From the \"Choose a tag\" dropdown, type the new version (using the format   <code>v&lt;MAJOR&gt;.&lt;MINOR&gt;.&lt;INCREMENTAL&gt;</code>, like <code>v0.1.0</code>) and select \"Create new tag   on publish\"</li> <li>Fill out an appropriate title, and add release notes generated by looking at   PRs merged since the last release</li> <li>Click \"Publish Release\"</li> </ul> <p>This will build and publish the new version to Artifact Registry.</p>"},{"location":"developing.html#manually-publishing-to-pypi","title":"Manually publishing to PyPI","text":"<p>To publish a version manually, you'll need to be added to the GenJAX Maintainers list on PyPI, or ask a current maintainer from the project page for help. Once that's settled:</p> <ul> <li>generate an API token on your pypi account   page, scoped to all projects or   scoped specifically to genjax</li> <li>copy the token and install it on your machine by running the following   command:</li> </ul> <pre><code>poetry config pypi-token.pypi &lt;api-token&gt;\n</code></pre> <ul> <li>create a new version tag on the <code>main</code> branch of the form   <code>v&lt;MAJOR&gt;.&lt;MINOR&gt;.&lt;INCREMENTAL&gt;</code>, like <code>v0.1.0</code>, and push the tag to the   remote repository:</li> </ul> <pre><code>git tag v0.1.0\ngit push --tags\n</code></pre> <ul> <li>use Poetry to build and publish the artifact to pypi:</li> </ul> <pre><code>poetry publish --build\n</code></pre>"},{"location":"cookbook/active/choice_maps.html","title":"Choice maps","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport jax.random as random\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import (\n    bernoulli,\n    beta,\n    gen,\n    mix,\n    normal,\n    or_else,\n    pretty,\n    repeat,\n    scan,\n    vmap,\n)\n\npretty()\nkey = random.key(0)\n</pre> import jax import jax.numpy as jnp import jax.random as random  import genjax from genjax import ChoiceMapBuilder as C from genjax import (     bernoulli,     beta,     gen,     mix,     normal,     or_else,     pretty,     repeat,     scan,     vmap, )  pretty() key = random.key(0) <p>Choice maps are dictionary-like data structures that accumulate the random choices produced by generative functions which are <code>traced</code> by the system, i.e. that are indicated by <code>@ \"p\"</code>  in generative functions.</p> <p>They also serve as a set of constraints/observations when one tries to do inference: given the constraints, inference provides plausible value to complete a choice map to a full trace  of a generative model (one value per traced random sample).</p> In\u00a0[3]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(1.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    return 2 * v\n</pre> @gen def beta_bernoulli_process(u):     p = beta(1.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     return 2 * v <p>Simulating from a model produces a traces which contains a choice map.</p> In\u00a0[4]: Copied! <pre>key, subkey = jax.random.split(key)\ntrace = jax.jit(beta_bernoulli_process.simulate)(subkey, (0.5,))\n</pre> key, subkey = jax.random.split(key) trace = jax.jit(beta_bernoulli_process.simulate)(subkey, (0.5,)) <p>From that trace, we can recover the choicemap with either of the two equivalent methods:</p> In\u00a0[5]: Copied! <pre>trace.get_choices(), trace.get_choices()\n</pre> trace.get_choices(), trace.get_choices() Out[5]: <p>We can also print specific subparts of the choice map.</p> In\u00a0[6]: Copied! <pre>trace.get_choices()[\"p\"]\n</pre> trace.get_choices()[\"p\"] Out[6]: <p>Then, we can create a choice map of observations and perform diverse operations on it. We can set the value of an address in the choice map. For instance, we can add two choicemaps together, which behaves similarly to the union of two dictionaries.</p> In\u00a0[7]: Copied! <pre>chm = C[\"p\"].set(0.5) | C[\"v\"].set(1)\nchm\n</pre> chm = C[\"p\"].set(0.5) | C[\"v\"].set(1) chm Out[7]: <p>A couple of extra ways to achieve the same result.</p> In\u00a0[8]: Copied! <pre>chm_equiv_1 = (\n    C[\"p\"].set(0.5).at[\"v\"].set(1)\n)  # the at/set notation mimics JAX's array update pattern\nchm_equiv_2 = C.d({\"p\": 0.5, \"v\": 1})  # creates a dictionary directly\nassert chm == chm_equiv_1 == chm_equiv_2\n</pre> chm_equiv_1 = (     C[\"p\"].set(0.5).at[\"v\"].set(1) )  # the at/set notation mimics JAX's array update pattern chm_equiv_2 = C.d({\"p\": 0.5, \"v\": 1})  # creates a dictionary directly assert chm == chm_equiv_1 == chm_equiv_2 <p>This also works for hierarchical addresses:</p> In\u00a0[9]: Copied! <pre>chm = C[\"p\", \"v\"].set(1)\n# equivalent to\neq_chm = C.d({\"p\": C.d({\"v\": 1})})\nassert chm == eq_chm\nchm\n</pre> chm = C[\"p\", \"v\"].set(1) # equivalent to eq_chm = C.d({\"p\": C.d({\"v\": 1})}) assert chm == eq_chm chm Out[9]: <p>We can also directly set a value in the choice_map</p> In\u00a0[10]: Copied! <pre>chm = C.v(5.0)\nchm\n</pre> chm = C.v(5.0) chm Out[10]: <p>We can also create an empty choice_map</p> In\u00a0[11]: Copied! <pre>chm = C.n()\nchm\n</pre> chm = C.n() chm Out[11]: <p>Other examples of Choice map creation include iteratively adding choices to a choice map.</p> In\u00a0[12]: Copied! <pre>chm = C.n()\nfor i in range(10):\n    chm = chm ^ C[\"p\" + str(i)].set(i)\n</pre> chm = C.n() for i in range(10):     chm = chm ^ C[\"p\" + str(i)].set(i) <pre>/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n/tmp/ipykernel_6234/1087827984.py:3: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[\"p\" + str(i)].set(i)\n</pre> <p>An equivalent, more JAX-friendly way to do this</p> In\u00a0[13]: Copied! <pre>chm = jax.vmap(lambda idx: C[idx].set(idx.astype(float)))(jnp.arange(10))\n</pre> chm = jax.vmap(lambda idx: C[idx].set(idx.astype(float)))(jnp.arange(10)) <p>And in fact, we can directly use the numpy notation to create a choice map.</p> In\u00a0[14]: Copied! <pre>chm = C[:].set(jnp.arange(10.0))\nchm\n</pre> chm = C[:].set(jnp.arange(10.0)) chm Out[14]: <p>For a nested vmap combinator, the creation of a choice map can be a bit more tricky.</p> In\u00a0[15]: Copied! <pre>sample_image = genjax.vmap(in_axes=(0,))(\n    genjax.vmap(in_axes=(0,))(gen(lambda pixel: normal(pixel, 1.0) @ \"new_pixel\"))\n)\n\nimage = jnp.zeros([4, 4], dtype=jnp.float32)\nkey, subkey = jax.random.split(key)\ntrace = sample_image.simulate(subkey, (image,))\ntrace.get_choices()\n</pre> sample_image = genjax.vmap(in_axes=(0,))(     genjax.vmap(in_axes=(0,))(gen(lambda pixel: normal(pixel, 1.0) @ \"new_pixel\")) )  image = jnp.zeros([4, 4], dtype=jnp.float32) key, subkey = jax.random.split(key) trace = sample_image.simulate(subkey, (image,)) trace.get_choices() Out[15]: <p>Creating a few values for the choice map is simple.</p> In\u00a0[16]: Copied! <pre>chm = C[1, 2, \"new_pixel\"].set(1.0) ^ C[0, 2, \"new_pixel\"].set(1.0)\n\nkey, subkey = jax.random.split(key)\ntr, w = jax.jit(sample_image.importance)(subkey, chm, (image,))\nw\n</pre> chm = C[1, 2, \"new_pixel\"].set(1.0) ^ C[0, 2, \"new_pixel\"].set(1.0)  key, subkey = jax.random.split(key) tr, w = jax.jit(sample_image.importance)(subkey, chm, (image,)) w <pre>/tmp/ipykernel_6234/3889217969.py:1: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = C[1, 2, \"new_pixel\"].set(1.0) ^ C[0, 2, \"new_pixel\"].set(1.0)\n</pre> Out[16]: <p>But because of the nested <code>vmap</code>, the address hierarchy can sometimes lead to unintuitive results, e.g. as there is no bound check on the address. We seemingly adding a new constraint but we obtain the same weight as before, meaning that the new choice was not used for inference.</p> In\u00a0[17]: Copied! <pre>chm = chm ^ C[1, 5, \"new_pixel\"].set(1.0)\ntr, w = jax.jit(sample_image.importance)(\n    subkey, chm, (image,)\n)  # reusing the key to make comparisons easier\nw\n</pre> chm = chm ^ C[1, 5, \"new_pixel\"].set(1.0) tr, w = jax.jit(sample_image.importance)(     subkey, chm, (image,) )  # reusing the key to make comparisons easier w <pre>/tmp/ipykernel_6234/3063422751.py:1: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = chm ^ C[1, 5, \"new_pixel\"].set(1.0)\n</pre> Out[17]: <p>A different way to create a choicemap that is compatible with the nested vmap in this case.</p> In\u00a0[18]: Copied! <pre>chm = C[:, :, \"new_pixel\"].set(jnp.ones((4, 4), dtype=jnp.float32))\nkey, subkey = jax.random.split(key)\ntr, w = jax.jit(sample_image.importance)(subkey, chm, (image,))\nw\n</pre> chm = C[:, :, \"new_pixel\"].set(jnp.ones((4, 4), dtype=jnp.float32)) key, subkey = jax.random.split(key) tr, w = jax.jit(sample_image.importance)(subkey, chm, (image,)) w Out[18]: <p>More generally, some combinators introduce an <code>Indexed</code> choicemap. These are mainly <code>vmap, scan</code> as well as those derived from these 2, such as <code>iterate, repeat</code>. An <code>Indexed</code> choicemap introduced an integer in the hierarchy of addresses, as the place where the combinator is introduced. For instance:</p> In\u00a0[19]: Copied! <pre>@genjax.gen\ndef submodel():\n    x = genjax.exponential.vmap()(1.0 + jnp.arange(50, dtype=jnp.float32)) @ \"x\"\n    return x\n\n\n@genjax.gen\ndef model():\n    xs = submodel.repeat(n=5)() @ \"xs\"\n    return xs\n\n\nkey, subkey = jax.random.split(key)\ntr = model.simulate(subkey, ())\nchm = tr.get_choices()\nchm\n</pre> @genjax.gen def submodel():     x = genjax.exponential.vmap()(1.0 + jnp.arange(50, dtype=jnp.float32)) @ \"x\"     return x   @genjax.gen def model():     xs = submodel.repeat(n=5)() @ \"xs\"     return xs   key, subkey = jax.random.split(key) tr = model.simulate(subkey, ()) chm = tr.get_choices() chm Out[19]: <p>In this case, we can create a hierarchical choicemap as follows:</p> In\u00a0[20]: Copied! <pre>chm = C[\"xs\", :, \"x\", :].set(jnp.ones((5, 50)))\nkey, subkey = jax.random.split(key)\nmodel.importance(subkey, chm, ())\n</pre> chm = C[\"xs\", :, \"x\", :].set(jnp.ones((5, 50))) key, subkey = jax.random.split(key) model.importance(subkey, chm, ()) Out[20]: <p>We can also construct an indexed choicemap with more than one variable in it using the following syntax:</p> In\u00a0[21]: Copied! <pre>_phi, _q, _beta, _r = (0.9, 1.0, 0.5, 1.0)\n\n\n@genjax.gen\ndef step(state):\n    x_prev, z_prev = state\n    x = genjax.normal(_phi * x_prev, _q) @ \"x\"\n    z = _beta * z_prev + x\n    _ = genjax.normal(z, _r) @ \"y\"\n    return (x, z)\n\n\nmax_T = 20\nmodel = step.iterate_final(n=max_T)\n\nx_range = 1.0 * jnp.where(\n    (jnp.arange(20) &gt;= 10) &amp; (jnp.arange(20) &lt; 15), jnp.arange(20) + 1, jnp.arange(20)\n)\ny_range = 1.0 * jnp.where(\n    (jnp.arange(20) &gt;= 15) &amp; (jnp.arange(20) &lt; 20), jnp.arange(20) + 1, jnp.arange(20)\n)\nxy = C[\"x\"].set(x_range).at[\"y\"].set(y_range)\nchm4 = C[jnp.arange(20)].set(xy)\nchm4\nkey, subkey = jax.random.split(key)\nmodel.importance(subkey, chm4, ((0.5, 0.5),))\n</pre> _phi, _q, _beta, _r = (0.9, 1.0, 0.5, 1.0)   @genjax.gen def step(state):     x_prev, z_prev = state     x = genjax.normal(_phi * x_prev, _q) @ \"x\"     z = _beta * z_prev + x     _ = genjax.normal(z, _r) @ \"y\"     return (x, z)   max_T = 20 model = step.iterate_final(n=max_T)  x_range = 1.0 * jnp.where(     (jnp.arange(20) &gt;= 10) &amp; (jnp.arange(20) &lt; 15), jnp.arange(20) + 1, jnp.arange(20) ) y_range = 1.0 * jnp.where(     (jnp.arange(20) &gt;= 15) &amp; (jnp.arange(20) &lt; 20), jnp.arange(20) + 1, jnp.arange(20) ) xy = C[\"x\"].set(x_range).at[\"y\"].set(y_range) chm4 = C[jnp.arange(20)].set(xy) chm4 key, subkey = jax.random.split(key) model.importance(subkey, chm4, ((0.5, 0.5),)) Out[21]: <p>Accessing the right elements in the trace can become non-trivial when one creates hierarchical generative functions. Here are minimal examples and solutions for selection.</p> In\u00a0[22]: Copied! <pre># For `or_else` combinator\n@gen\ndef model(p):\n    branch_1 = gen(lambda p: bernoulli(p) @ \"v1\")\n    branch_2 = gen(lambda p: bernoulli(-p) @ \"v2\")\n    v = or_else(branch_1, branch_2)(p &gt; 0, (p,), (p,)) @ \"s\"\n    return v\n\n\nkey, subkey = jax.random.split(key)\ntrace = jax.jit(model.simulate)(subkey, (0.5,))\ntrace.get_choices()[\"s\", \"v1\"]\n</pre> # For `or_else` combinator @gen def model(p):     branch_1 = gen(lambda p: bernoulli(p) @ \"v1\")     branch_2 = gen(lambda p: bernoulli(-p) @ \"v2\")     v = or_else(branch_1, branch_2)(p &gt; 0, (p,), (p,)) @ \"s\"     return v   key, subkey = jax.random.split(key) trace = jax.jit(model.simulate)(subkey, (0.5,)) trace.get_choices()[\"s\", \"v1\"] Out[22]: In\u00a0[23]: Copied! <pre># For `vmap` combinator\nsample_image = vmap(in_axes=(0,))(\n    vmap(in_axes=(0,))(gen(lambda pixel: normal(pixel, 1.0) @ \"new_pixel\"))\n)\n\nimage = jnp.zeros([2, 3], dtype=jnp.float32)\nkey, subkey = jax.random.split(key)\ntrace = sample_image.simulate(subkey, (image,))\ntrace.get_choices()[:, :, \"new_pixel\"]\n</pre> # For `vmap` combinator sample_image = vmap(in_axes=(0,))(     vmap(in_axes=(0,))(gen(lambda pixel: normal(pixel, 1.0) @ \"new_pixel\")) )  image = jnp.zeros([2, 3], dtype=jnp.float32) key, subkey = jax.random.split(key) trace = sample_image.simulate(subkey, (image,)) trace.get_choices()[:, :, \"new_pixel\"] Out[23]: In\u00a0[24]: Copied! <pre># For `scan_combinator`\n@scan(n=10)\n@gen\ndef hmm(x, c):\n    z = normal(x, 1.0) @ \"z\"\n    y = normal(z, 1.0) @ \"y\"\n    return y, None\n\n\nkey, subkey = jax.random.split(key)\ntrace = hmm.simulate(subkey, (0.0, None))\ntrace.get_choices()[:, \"z\"], trace.get_choices()[3, \"y\"]\n</pre> # For `scan_combinator` @scan(n=10) @gen def hmm(x, c):     z = normal(x, 1.0) @ \"z\"     y = normal(z, 1.0) @ \"y\"     return y, None   key, subkey = jax.random.split(key) trace = hmm.simulate(subkey, (0.0, None)) trace.get_choices()[:, \"z\"], trace.get_choices()[3, \"y\"] Out[24]: In\u00a0[25]: Copied! <pre># For `repeat_combinator`\n@repeat(n=10)\n@gen\ndef model(y):\n    x = normal(y, 0.01) @ \"x\"\n    y = normal(x, 0.01) @ \"y\"\n    return y\n\n\nkey, subkey = jax.random.split(key)\ntrace = model.simulate(subkey, (0.3,))\ntrace.get_choices()[:, \"x\"]\n</pre> # For `repeat_combinator` @repeat(n=10) @gen def model(y):     x = normal(y, 0.01) @ \"x\"     y = normal(x, 0.01) @ \"y\"     return y   key, subkey = jax.random.split(key) trace = model.simulate(subkey, (0.3,)) trace.get_choices()[:, \"x\"] Out[25]: In\u00a0[26]: Copied! <pre># For `mixture_combinator`\n@gen\ndef mixture_model(p):\n    z = normal(p, 1.0) @ \"z\"\n    logits = (0.3, 0.5, 0.2)\n    arg_1 = (p,)\n    arg_2 = (p,)\n    arg_3 = (p,)\n    a = (\n        mix(\n            gen(lambda p: normal(p, 1.0) @ \"x1\"),\n            gen(lambda p: normal(p, 2.0) @ \"x2\"),\n            gen(lambda p: normal(p, 3.0) @ \"x3\"),\n        )(logits, arg_1, arg_2, arg_3)\n        @ \"a\"\n    )\n    return a + z\n\n\nkey, subkey = jax.random.split(key)\ntrace = mixture_model.simulate(subkey, (0.4,))\n# The combinator uses a fixed address \"mixture_component\" for the components of the mixture model.\ntrace.get_choices()[\"a\", \"mixture_component\"]\n</pre> # For `mixture_combinator` @gen def mixture_model(p):     z = normal(p, 1.0) @ \"z\"     logits = (0.3, 0.5, 0.2)     arg_1 = (p,)     arg_2 = (p,)     arg_3 = (p,)     a = (         mix(             gen(lambda p: normal(p, 1.0) @ \"x1\"),             gen(lambda p: normal(p, 2.0) @ \"x2\"),             gen(lambda p: normal(p, 3.0) @ \"x3\"),         )(logits, arg_1, arg_2, arg_3)         @ \"a\"     )     return a + z   key, subkey = jax.random.split(key) trace = mixture_model.simulate(subkey, (0.4,)) # The combinator uses a fixed address \"mixture_component\" for the components of the mixture model. trace.get_choices()[\"a\", \"mixture_component\"] Out[26]: <p>Similarly, if traces were created as a batch using <code>jax.vmap</code>, in general it will not create a valid batched trace, e.g. the score will not be defined as a single float. It can be very useful for inference though.</p> In\u00a0[27]: Copied! <pre>@genjax.gen\ndef random_walk_step(prev, _):\n    x = genjax.normal(prev, 1.0) @ \"x\"\n    return x, None\n\n\nrandom_walk = random_walk_step.scan(n=1000)\n\ninit = 0.5\nkeys = jax.random.split(key, 10)\n\n\ntrs = jax.vmap(random_walk.simulate, (0, None))(keys, (init, None))\ntry:\n    if isinstance(trs.get_score(), float):\n        trs.get_score()\n    else:\n        raise ValueError(\"Expected a float value for the score.\")\nexcept Exception as e:\n    print(e)\n</pre> @genjax.gen def random_walk_step(prev, _):     x = genjax.normal(prev, 1.0) @ \"x\"     return x, None   random_walk = random_walk_step.scan(n=1000)  init = 0.5 keys = jax.random.split(key, 10)   trs = jax.vmap(random_walk.simulate, (0, None))(keys, (init, None)) try:     if isinstance(trs.get_score(), float):         trs.get_score()     else:         raise ValueError(\"Expected a float value for the score.\") except Exception as e:     print(e) <pre>Expected a float value for the score.\n</pre> <p>However, with a little extra step we can recover information in individual traces.</p> In\u00a0[28]: Copied! <pre>jax.vmap(lambda tr: tr.get_choices())(trs)\n</pre> jax.vmap(lambda tr: tr.get_choices())(trs) Out[28]: <p>Note that this limitation is dependent on the model, and the simpler thing may work anyway for some classes' models.</p> In\u00a0[29]: Copied! <pre>jitted = jax.jit(jax.vmap(model.simulate, in_axes=(0, None)))\nkeys = random.split(key, 10)\ntraces = jitted(keys, (0.5,))\n\n\ntraces.get_choices()\n</pre> jitted = jax.jit(jax.vmap(model.simulate, in_axes=(0, None))) keys = random.split(key, 10) traces = jitted(keys, (0.5,))   traces.get_choices() Out[29]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"cookbook/active/choice_maps.html#choice-maps","title":"Choice maps \u00b6","text":""},{"location":"cookbook/active/debugging.html","title":"Debugging","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\n\nfrom genjax import bernoulli, beta, gen\n\nkey = jax.random.key(0)\n</pre> import jax  from genjax import bernoulli, beta, gen  key = jax.random.key(0) <p>TLDR: inside of generative functions, use <code>jax.debug.print</code> and <code>jax.debug.breakpoint()</code> instead of <code>print()</code> statements. We also recommend looking at the official JAX debug doc which applies to GenJAX as well: https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html</p> <p>Example of printing</p> In\u00a0[3]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(0.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    print(\"Bad looking printing:\", v)  # will print a traced Value, not what you want\n    jax.debug.print(\"Better looking printing: {v}\", v=v)\n    return v\n\n\nnon_jitted = beta_bernoulli_process.simulate\nkey, subkey = jax.random.split(key)\ntr = non_jitted(subkey, (1.0,))\nkey, subkey = jax.random.split(key)\njitted = jax.jit(beta_bernoulli_process.simulate)\ntr = jitted(subkey, (1.0,))\n</pre> @gen def beta_bernoulli_process(u):     p = beta(0.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     print(\"Bad looking printing:\", v)  # will print a traced Value, not what you want     jax.debug.print(\"Better looking printing: {v}\", v=v)     return v   non_jitted = beta_bernoulli_process.simulate key, subkey = jax.random.split(key) tr = non_jitted(subkey, (1.0,)) key, subkey = jax.random.split(key) jitted = jax.jit(beta_bernoulli_process.simulate) tr = jitted(subkey, (1.0,)) <pre>Bad looking printing: Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace&gt;\n</pre> <pre>Better looking printing: 0\nBad looking printing: Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace&gt;\n</pre> <pre>Better looking printing: 0\n</pre> <p>Inside generative functions, <code>jax.debug.print</code> is available and compatible with all the JAX transformations and higher-order functions like <code>jax.jit</code>, <code>jax.grad</code>, <code>jax.vmap</code>, <code>jax.lax.scan</code>, etc.</p> <p>Running the cell below will open a pdb-like interface in the terminal where you can inspect the values of the variables in the scope of the breakpoint. You can continue the execution of the program by typing c and pressing Enter. You can also inspect the values of the variables in the scope of the breakpoint by typing the name of the variable and pressing Enter. You can exit the breakpoint by typing q and pressing Enter. You can see the commands available in the breakpoint by typing h and pressing Enter. It also works with jitted functions, but may affect performance. It is compatible with all the JAX transformations and higher-order functions too but you can expect some sharp edges.</p> <pre># Example of breakpoint\n@gen\ndef beta_bernoulli_process(u):\n    p = beta(0.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    jax.debug.breakpoint()\n    return v\n\n\nnon_jitted = beta_bernoulli_process.simulate\nkey, subkey = jax.random.split(key)\ntr = non_jitted(subkey, (1.0,))\n</pre>"},{"location":"cookbook/active/debugging.html#debugging","title":"Debugging \u00b6","text":"<p>How can I debug my code? I want to add break points or print statements in my Jax/GenJax code but it  doesn't seem to work because of traced values and/or jit compilation.</p>"},{"location":"cookbook/active/generative_function_interface.html","title":"The generative function interface","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nfrom jax import jit\n\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import (\n    Diff,\n    NoChange,\n    UnknownChange,\n    bernoulli,\n    beta,\n    gen,\n    pretty,\n)\nfrom genjax._src.generative_functions.static import MissingAddress\n\nkey = jax.random.key(0)\npretty()\n\n\n# Define a generative function\n@gen\ndef beta_bernoulli_process(u):\n    p = beta(1.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    return 2 * v\n</pre> import jax from jax import jit  from genjax import ChoiceMapBuilder as C from genjax import (     Diff,     NoChange,     UnknownChange,     bernoulli,     beta,     gen,     pretty, ) from genjax._src.generative_functions.static import MissingAddress  key = jax.random.key(0) pretty()   # Define a generative function @gen def beta_bernoulli_process(u):     p = beta(1.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     return 2 * v <ol> <li>Generate a traced sample and constructs choicemaps</li> </ol> <p>There's an entire cookbook entry on this in <code>choicemap_creation_selection</code>.</p> In\u00a0[3]: Copied! <pre>key, subkey = jax.random.split(key)\ntrace = jax.jit(beta_bernoulli_process.simulate)(subkey, (0.5,))\n</pre> key, subkey = jax.random.split(key) trace = jax.jit(beta_bernoulli_process.simulate)(subkey, (0.5,)) <ol> <li>Compute log probabilities</li> </ol> <p>2.1 Print the log probability of the trace</p> In\u00a0[4]: Copied! <pre>trace.get_score()\n</pre> trace.get_score() Out[4]: <p>2.2 Print the log probability of an observation encoded as a ChoiceMap under the model</p> <p>It returns both the log probability and the return value</p> In\u00a0[5]: Copied! <pre>chm = C[\"p\"].set(0.5) ^ C[\"v\"].set(1)\nargs = (0.5,)\nbeta_bernoulli_process.assess(chm, args)\n</pre> chm = C[\"p\"].set(0.5) ^ C[\"v\"].set(1) args = (0.5,) beta_bernoulli_process.assess(chm, args) <pre>/tmp/ipykernel_6508/3994869088.py:1: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = C[\"p\"].set(0.5) ^ C[\"v\"].set(1)\n</pre> Out[5]: <p>Note that the ChoiceMap should be complete, i.e. all random choices should be observed</p> In\u00a0[6]: Copied! <pre>chm_2 = C[\"v\"].set(1)\ntry:\n    beta_bernoulli_process.assess(chm_2, args)\nexcept MissingAddress as e:\n    print(e)\n</pre> chm_2 = C[\"v\"].set(1) try:     beta_bernoulli_process.assess(chm_2, args) except MissingAddress as e:     print(e) <pre>p\n</pre> <ol> <li>Generate a sample conditioned on the observations</li> </ol> <p>We can also use a partial ChoiceMap as a constraint/observation and generate a full trace with these constraints.</p> In\u00a0[7]: Copied! <pre>key, subkey = jax.random.split(key)\npartial_chm = C[\"v\"].set(1)  # Creates a ChoiceMap of observations\nargs = (0.5,)\ntrace, weight = beta_bernoulli_process.importance(\n    subkey, partial_chm, args\n)  # Runs importance sampling\n</pre> key, subkey = jax.random.split(key) partial_chm = C[\"v\"].set(1)  # Creates a ChoiceMap of observations args = (0.5,) trace, weight = beta_bernoulli_process.importance(     subkey, partial_chm, args )  # Runs importance sampling <p>This returns a pair containing the new trace and the log probability of produced trace under the model</p> In\u00a0[8]: Copied! <pre>trace.get_choices()\n</pre> trace.get_choices() Out[8]: In\u00a0[9]: Copied! <pre>weight\n</pre> weight Out[9]: <ol> <li>Update a trace.</li> </ol> <p>We can also update a trace. This is for instance useful as a performance optimization in Metropolis-Hastings algorithms where often most of the trace doesn't change between time steps.</p> <p>We first define a model for which changing the argument will force a change in the trace.</p> In\u00a0[10]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(1.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    return 2 * v\n</pre> @gen def beta_bernoulli_process(u):     p = beta(1.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     return 2 * v <p>We then create an trace to be updated and constraints.</p> In\u00a0[11]: Copied! <pre>key, subkey = jax.random.split(key)\njitted = jit(beta_bernoulli_process.simulate)\nold_trace = jitted(subkey, (1.0,))\nconstraint = C[\"v\"].set(1)\n</pre> key, subkey = jax.random.split(key) jitted = jit(beta_bernoulli_process.simulate) old_trace = jitted(subkey, (1.0,)) constraint = C[\"v\"].set(1) <p>Now the update uses a form of incremental computation. It works by tracking the differences between the old new values for arguments. Just like for differentiation, it can be achieved by providing for each argument a tuple containing the new value and its change compared to the old value.</p> <p>If there's no change for an argument, the change is set to NoChange.</p> In\u00a0[12]: Copied! <pre>arg_diff = (Diff(1.0, NoChange),)\n</pre> arg_diff = (Diff(1.0, NoChange),) <p>If there is any change, the change is set to UnknownChange.</p> In\u00a0[13]: Copied! <pre>arg_diff = (Diff(5.0, UnknownChange),)\n</pre> arg_diff = (Diff(5.0, UnknownChange),) <p>We finally use the update method by passing it a key, the trace to be updated, and the update to be performed.</p> In\u00a0[14]: Copied! <pre>jitted_update = jit(beta_bernoulli_process.update)\n\nkey, subkey = jax.random.split(key)\nnew_trace, weight_diff, ret_diff, discard_choice = jitted_update(\n    subkey, old_trace, constraint, arg_diff\n)\n</pre> jitted_update = jit(beta_bernoulli_process.update)  key, subkey = jax.random.split(key) new_trace, weight_diff, ret_diff, discard_choice = jitted_update(     subkey, old_trace, constraint, arg_diff ) <p>We can compare the old and new values for the samples and notice that they have not changed.</p> In\u00a0[15]: Copied! <pre>old_trace.get_choices() == new_trace.get_choices()\n</pre> old_trace.get_choices() == new_trace.get_choices() Out[15]: <p>We can also see that the weight has changed. In fact we can check that the following relation holds <code>new_weight</code> = <code>old_weight</code> + <code>weight_diff</code>.</p> In\u00a0[16]: Copied! <pre>weight_diff, old_trace.get_score() + weight_diff == new_trace.get_score()\n</pre> weight_diff, old_trace.get_score() + weight_diff == new_trace.get_score() Out[16]: <ol> <li>A few more convenient methods</li> </ol> <p>5.1 <code>propose</code></p> <p>It uses the same inputs as <code>simulate</code> but returns the sample, the score and the return value</p> In\u00a0[17]: Copied! <pre>key, subkey = jax.random.split(key)\nsample, score, retval = jit(beta_bernoulli_process.propose)(subkey, (0.5,))\nsample, score, retval\n</pre> key, subkey = jax.random.split(key) sample, score, retval = jit(beta_bernoulli_process.propose)(subkey, (0.5,)) sample, score, retval Out[17]: <p>5.2 <code>get_gen_fn</code></p> <p>It returns the generative function that produced the trace.</p> In\u00a0[18]: Copied! <pre>trace.get_gen_fn()\n</pre> trace.get_gen_fn() Out[18]: <p>5.3 <code>get_args</code></p> <p>It returns the arguments passed to the generative function used to produce the trace</p> In\u00a0[19]: Copied! <pre>trace.get_args()\n</pre> trace.get_args() Out[19]: <p>5.4 <code>get_subtrace</code></p> <p>It takes a <code>StaticAddress</code> as argument and returns the sub-trace of a trace rooted at these addresses</p> In\u00a0[20]: Copied! <pre>subtrace = trace.get_subtrace(\"p\")\nsubtrace, subtrace.get_choices()\n</pre> subtrace = trace.get_subtrace(\"p\") subtrace, subtrace.get_choices() Out[20]:"},{"location":"cookbook/active/generative_function_interface.html#the-generative-function-interface","title":"The generative function interface \u00b6","text":""},{"location":"cookbook/active/intro.html","title":"Introduction","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>GenJAX is a swiss army knife for probabilistic machine learning: it's designed to support probabilistic modeling workflows, and to make the resulting code extremely fast and parallelizable via JAX.</p> <p>In this introduction, we'll focus on one such workflow: writing a latent variable model (we often say: a generative model) which describes a probability distribution over latent variables and data, and then asking questions about the conditional distribution over the latent variables given data.</p> <p>In the following, we'll often shorten GenJAX to Gen -- because GenJAX implements Gen.</p> In\u00a0[2]: Copied! <pre>import genstudio.plot as Plot\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom jax import jit, vmap\nfrom jax import random as jrand\n\nimport genjax\nfrom genjax import gen, normal, pretty\n\nsns.set_theme(style=\"white\")\nplt.rcParams[\"figure.facecolor\"] = \"none\"\nplt.rcParams[\"savefig.transparent\"] = True\n%config InlineBackend.figure_format = 'svg'\n\npretty()  # pretty print the types\n</pre> import genstudio.plot as Plot import jax.numpy as jnp import matplotlib.pyplot as plt import seaborn as sns from jax import jit, vmap from jax import random as jrand  import genjax from genjax import gen, normal, pretty  sns.set_theme(style=\"white\") plt.rcParams[\"figure.facecolor\"] = \"none\" plt.rcParams[\"savefig.transparent\"] = True %config InlineBackend.figure_format = 'svg'  pretty()  # pretty print the types In\u00a0[3]: Copied! <pre>@gen\ndef model():\n    x = normal(0.0, 1.0) @ \"x\"\n    normal(x, 1.0) @ \"y\"\n\n\nmodel\n</pre> @gen def model():     x = normal(0.0, 1.0) @ \"x\"     normal(x, 1.0) @ \"y\"   model Out[3]: <p>In Gen, probabilistic models are represented by a computational object called a generative function. Once we create one of these objects, we can use one of several interfaces to gain access to probabilistic effects.</p> <p>Here's one interface: <code>simulate</code> -- this samples from the probability distribution which the program represents, and stores the result, along with other data about the invocation of the function, in a data structure called a <code>Trace</code>.</p> In\u00a0[4]: Copied! <pre>key = jrand.key(0)\ntr = model.simulate(key, ())\ntr\n</pre> key = jrand.key(0) tr = model.simulate(key, ()) tr Out[4]: <p>We can dig around in this object uses its interfaces:</p> In\u00a0[5]: Copied! <pre>chm = tr.get_choices()\nchm\n</pre> chm = tr.get_choices() chm Out[5]: <p>A <code>ChoiceMap</code> is a representation of the sample from the probability distribution which the generative function represents. We can ask what values were sampled at the addresses (the <code>\"x\"</code> and <code>\"y\"</code> syntax in our model):</p> In\u00a0[6]: Copied! <pre>(chm[\"x\"], chm[\"y\"])\n</pre> (chm[\"x\"], chm[\"y\"]) Out[6]: <p>Neat -- all of our interfaces are JAX compatible, so we could sample 1000 times just by using <code>jax.vmap</code>:</p> In\u00a0[7]: Copied! <pre>sub_keys = jrand.split(jrand.key(0), 1000)\ntr = jit(vmap(model.simulate, in_axes=(0, None)))(sub_keys, ())\ntr\n</pre> sub_keys = jrand.split(jrand.key(0), 1000) tr = jit(vmap(model.simulate, in_axes=(0, None)))(sub_keys, ()) tr Out[7]: <p>Let's plot our samples to get a sense of the distribution we wrote down.</p> In\u00a0[8]: Copied! <pre>chm = tr.get_choices()\nPlot.dot({\"x\": chm[\"x\"], \"y\": chm[\"y\"]})\n</pre> chm = tr.get_choices() Plot.dot({\"x\": chm[\"x\"], \"y\": chm[\"y\"]}) Out[8]: <p>Traces also keep track of other data, like the score of the execution (which is a value which estimates the joint probability of the random choices under the distribution):</p> In\u00a0[9]: Copied! <pre>tr.get_score()\n</pre> tr.get_score() Out[9]: In\u00a0[10]: Copied! <pre># A regression distribution.\n@gen\ndef regression(x, coefficients, sigma):\n    basis_value = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(basis_value * coefficients)\n    y = genjax.normal(polynomial_value, sigma) @ \"v\"\n    return y\n\n\n# Regression, with an outlier random variable.\n@gen\ndef regression_with_outlier(x, coefficients):\n    is_outlier = genjax.flip(0.1) @ \"is_outlier\"\n    sigma = jnp.where(is_outlier, 30.0, 0.3)\n    is_outlier = jnp.array(is_outlier, dtype=int)\n    return regression(x, coefficients, sigma) @ \"y\"\n\n\n# The full model, sample coefficients for a curve, and then use\n# them in independent draws from the regression submodel.\n@gen\ndef full_model(xs):\n    coefficients = (\n        genjax.mv_normal(\n            jnp.zeros(3, dtype=float),\n            2.0 * jnp.identity(3),\n        )\n        @ \"alpha\"\n    )\n    ys = regression_with_outlier.vmap(in_axes=(0, None))(xs, coefficients) @ \"ys\"\n    return ys\n</pre> # A regression distribution. @gen def regression(x, coefficients, sigma):     basis_value = jnp.array([1.0, x, x**2])     polynomial_value = jnp.sum(basis_value * coefficients)     y = genjax.normal(polynomial_value, sigma) @ \"v\"     return y   # Regression, with an outlier random variable. @gen def regression_with_outlier(x, coefficients):     is_outlier = genjax.flip(0.1) @ \"is_outlier\"     sigma = jnp.where(is_outlier, 30.0, 0.3)     is_outlier = jnp.array(is_outlier, dtype=int)     return regression(x, coefficients, sigma) @ \"y\"   # The full model, sample coefficients for a curve, and then use # them in independent draws from the regression submodel. @gen def full_model(xs):     coefficients = (         genjax.mv_normal(             jnp.zeros(3, dtype=float),             2.0 * jnp.identity(3),         )         @ \"alpha\"     )     ys = regression_with_outlier.vmap(in_axes=(0, None))(xs, coefficients) @ \"ys\"     return ys <p>Now, let's examine a sample from this model:</p> In\u00a0[11]: Copied! <pre>data = jnp.arange(0, 10, 0.5)\nfull_model.simulate(key, (data,)).get_choices()[\"ys\", :, \"y\", \"v\"]\n</pre> data = jnp.arange(0, 10, 0.5) full_model.simulate(key, (data,)).get_choices()[\"ys\", :, \"y\", \"v\"] Out[11]: <p>We can plot a few such samples.</p> In\u00a0[12]: Copied! <pre>key, *sub_keys = jrand.split(key, 10)\ntraces = vmap(lambda k: full_model.simulate(k, (data,)))(jnp.array(sub_keys))\nys = traces.get_choices()[\"ys\", :, \"y\", \"v\"]\n\n(\n    Plot.dot(\n        Plot.dimensions(ys, [\"sample\", \"ys\"], leaves=\"y\"),\n        {\"x\": Plot.repeat(data), \"y\": \"y\", \"facetGrid\": \"sample\"},\n    )\n    + Plot.frame()\n)\n</pre> key, *sub_keys = jrand.split(key, 10) traces = vmap(lambda k: full_model.simulate(k, (data,)))(jnp.array(sub_keys)) ys = traces.get_choices()[\"ys\", :, \"y\", \"v\"]  (     Plot.dot(         Plot.dimensions(ys, [\"sample\", \"ys\"], leaves=\"y\"),         {\"x\": Plot.repeat(data), \"y\": \"y\", \"facetGrid\": \"sample\"},     )     + Plot.frame() ) Out[12]: <p>These are samples from the distribution over curves which our generative function represents.</p> In\u00a0[13]: Copied! <pre>x = jnp.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0])\ny = 2.0 * x + 1.5 + x**2\ny = y.at[2].set(50.0)\ny\n</pre> x = jnp.array([0.3, 0.7, 1.1, 1.4, 2.3, 2.5, 3.0, 4.0, 5.0]) y = 2.0 * x + 1.5 + x**2 y = y.at[2].set(50.0) y Out[13]: <p>We've explored how generative functions represent joint distributions over random variables, but what about distributions induced by inference problems?</p> <p>We can create an inference problem by pairing a generative function with arguments, and a constraint.</p> <p>First, let's learn how to create one type of constraint -- a choice map sample, just like the choice maps we saw earlier.</p> In\u00a0[14]: Copied! <pre>from genjax import ChoiceMapBuilder as C\n\nchm = C[\"ys\", :, \"y\", \"v\"].set(y)\nchm[\"ys\", :, \"y\", \"v\"]\n</pre> from genjax import ChoiceMapBuilder as C  chm = C[\"ys\", :, \"y\", \"v\"].set(y) chm[\"ys\", :, \"y\", \"v\"] Out[14]: <p>The choice map holds the value constraint for the distributions we used in our generative function. Choice maps are a lot like arrays, with a bit of extra metadata.</p> <p>Now, we can specify an inference target.</p> In\u00a0[15]: Copied! <pre>from genjax import Target\n\ntarget = Target(full_model, (x,), chm)\ntarget\n</pre> from genjax import Target  target = Target(full_model, (x,), chm) target Out[15]: <p>A <code>Target</code> represents an unnormalized distribution -- in this case, the posterior of the distribution represented by our generative function with arguments <code>args = (x, )</code>.</p> <p>Now, we can approximate the solution to the inference problem using an inference algorithm. GenJAX exposes a standard library of approximate inference algorithms: let's use $K$-particle importance sampling for this one.</p> In\u00a0[16]: Copied! <pre>from genjax.inference.smc import ImportanceK\n\nalg = ImportanceK(target, k_particles=100)\nalg\n</pre> from genjax.inference.smc import ImportanceK  alg = ImportanceK(target, k_particles=100) alg Out[16]: In\u00a0[17]: Copied! <pre>sub_keys = jrand.split(key, 50)\nposterior_samples = jit(vmap(alg(target)))(sub_keys)\n</pre> sub_keys = jrand.split(key, 50) posterior_samples = jit(vmap(alg(target)))(sub_keys) <p>With samples from our approximate posterior in hand, we can check queries like \"estimate the probability that a point is an outlier\":</p> In\u00a0[18]: Copied! <pre>posterior_samples[\"ys\", :, \"is_outlier\"]\n</pre> posterior_samples[\"ys\", :, \"is_outlier\"] Out[18]: <p>Here, we see that our approximate posterior assigns high probability to the query \"the 3rd data point is an outlier\". Remember, we set this point to be far away from the other points.</p> In\u00a0[19]: Copied! <pre>posterior_samples[\"ys\", :, \"is_outlier\"].mean(axis=0)\n</pre> posterior_samples[\"ys\", :, \"is_outlier\"].mean(axis=0) Out[19]: <p>We can also plot the sampled curves against the data.</p> In\u00a0[20]: Copied! <pre>def polynomial_at_x(x, coefficients):\n    basis_values = jnp.array([1.0, x, x**2])\n    polynomial_value = jnp.sum(coefficients * basis_values)\n    return polynomial_value\n\n\njitted = jit(vmap(polynomial_at_x, in_axes=(None, 0)))\n\ncoefficients = posterior_samples[\"alpha\"]\nevaluation_points = jnp.arange(0, 5, 0.01)\n\npoints = [(x, y) for x in evaluation_points for y in jitted(x, coefficients).tolist()]\n(\n    Plot.dot(points, fill=\"gold\", opacity=0.25, r=0.5)\n    + Plot.dot({\"x\": x, \"y\": y})\n    + Plot.frame()\n)\n</pre> def polynomial_at_x(x, coefficients):     basis_values = jnp.array([1.0, x, x**2])     polynomial_value = jnp.sum(coefficients * basis_values)     return polynomial_value   jitted = jit(vmap(polynomial_at_x, in_axes=(None, 0)))  coefficients = posterior_samples[\"alpha\"] evaluation_points = jnp.arange(0, 5, 0.01)  points = [(x, y) for x in evaluation_points for y in jitted(x, coefficients).tolist()] (     Plot.dot(points, fill=\"gold\", opacity=0.25, r=0.5)     + Plot.dot({\"x\": x, \"y\": y})     + Plot.frame() ) Out[20]:"},{"location":"cookbook/active/intro.html#introduction","title":"Introduction \u00b6","text":""},{"location":"cookbook/active/intro.html#generative-functions","title":"Generative functions\u00b6","text":""},{"location":"cookbook/active/intro.html#composition-of-generative-functions","title":"Composition of generative functions\u00b6","text":"<p>Generative functions are probabilistic building blocks. You can combine them into larger probability distributions:</p>"},{"location":"cookbook/active/intro.html#inference-in-generative-functions","title":"Inference in generative functions\u00b6","text":"<p>So we've written a regression model, a distribution over curves. Our model includes an outlier component. If we observe some data for <code>\"y\"</code>, can we predict which points might be outliers?</p>"},{"location":"cookbook/active/intro.html#summary","title":"Summary\u00b6","text":"<p>We\u2019ve covered a lot of ground in this notebook. Please reflect, re-read, and post issues!</p> <ul> <li>We discussed generative functions - the main computational object of Gen, and how these objects represent probability distributions.</li> <li>We showed how to create generative functions.</li> <li>We showed how to use interfaces on generative functions to compute with common operations on distributions.</li> <li>We created a generative function to model a data-generating process based on sampling and evaluating random polynomials on input data - representing regression task.</li> <li>We showed how to create inference problems from generative functions.</li> <li>We created an inference problem from our regression model.</li> <li>We showed how to create approximate inference solutions to inference problems, and sample from them.</li> <li>We investigated the approximate posterior samples, and visually inspected that they match the inferences that we might draw - both for the polynomials we expected to produce the data, as well as what data points might be outliers.</li> </ul> <p>This is just the beginning! There\u2019s a lot more to learn, but this is plenty to chew (for now).</p>"},{"location":"cookbook/active/jax_basics.html","title":"JAX Basics","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import multiprocessing\nimport subprocess\nimport time\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import jit, random\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import beta, gen, pretty\n\nkey = jax.random.key(0)\npretty()\n</pre> import multiprocessing import subprocess import time  import jax import jax.numpy as jnp import numpy as np from jax import jit, random  import genjax from genjax import ChoiceMapBuilder as C from genjax import beta, gen, pretty  key = jax.random.key(0) pretty() <ol> <li>JAX expects arrays/tuples everywhere</li> </ol> In\u00a0[3]: Copied! <pre>@gen\ndef f(p):\n    v = genjax.bernoulli(probs=p) @ \"v\"\n    return v\n\n\n# First way of failing\nkey, subkey = jax.random.split(key)\ntry:\n    f.simulate(key, 0.5)\nexcept Exception as e:\n    print(e)\n\n# Second way of failing\nkey, subkey = jax.random.split(key)\ntry:\n    f.simulate(subkey, [0.5])\nexcept Exception as e:\n    print(e)\n\n# Third way of failing\nkey, subkey = jax.random.split(key)\ntry:\n    f.simulate(subkey, (0.5))\nexcept Exception as e:\n    print(e)\n\n# Correct way\nkey, subkey = jax.random.split(key)\nf.simulate(subkey, (0.5,)).get_retval()\n</pre> @gen def f(p):     v = genjax.bernoulli(probs=p) @ \"v\"     return v   # First way of failing key, subkey = jax.random.split(key) try:     f.simulate(key, 0.5) except Exception as e:     print(e)  # Second way of failing key, subkey = jax.random.split(key) try:     f.simulate(subkey, [0.5]) except Exception as e:     print(e)  # Third way of failing key, subkey = jax.random.split(key) try:     f.simulate(subkey, (0.5)) except Exception as e:     print(e)  # Correct way key, subkey = jax.random.split(key) f.simulate(subkey, (0.5,)).get_retval() <pre>Method genjax._src.generative_functions.static.StaticGenerativeFunction.simulate() parameter args=0.5 violates type hint tuple[typing.Any, ...], as float 0.5 not instance of tuple.\nMethod genjax._src.generative_functions.static.StaticGenerativeFunction.simulate() parameter args=[0.5] violates type hint tuple[typing.Any, ...], as list [0.5] not instance of tuple.\nMethod genjax._src.generative_functions.static.StaticGenerativeFunction.simulate() parameter args=0.5 violates type hint tuple[typing.Any, ...], as float 0.5 not instance of tuple.\n</pre> Out[3]: <ol> <li>GenJAX relies on Tensor Flow Probability and it sometimes does unintuitive things.</li> </ol> <p>The Bernoulli distribution uses logits instead of probabilities</p> In\u00a0[4]: Copied! <pre>@gen\ndef g(p):\n    v = genjax.bernoulli(probs=p) @ \"v\"\n    return v\n\n\nkey, subkey = jax.random.split(key)\narg = (3.0,)  # 3 is not a valid probability but a valid logit\nkeys = jax.random.split(subkey, 30)\n# simulate 30 times\njax.vmap(g.simulate, in_axes=(0, None))(keys, arg).get_choices()\n</pre> @gen def g(p):     v = genjax.bernoulli(probs=p) @ \"v\"     return v   key, subkey = jax.random.split(key) arg = (3.0,)  # 3 is not a valid probability but a valid logit keys = jax.random.split(subkey, 30) # simulate 30 times jax.vmap(g.simulate, in_axes=(0, None))(keys, arg).get_choices() Out[4]: <p>Values which are stricter than $0$ are considered to be the value True. This means that observing that the value of <code>\"v\"</code> is $4$ will be considered possible while intuitively <code>\"v\"</code> should only have support on $0$ and $1$.</p> In\u00a0[5]: Copied! <pre>chm = C[\"v\"].set(3)\ng.assess(chm, (0.5,))[0]  # This should be -inf.\n</pre> chm = C[\"v\"].set(3) g.assess(chm, (0.5,))[0]  # This should be -inf. Out[5]: <p>Alternatively, we can use the flip function which uses probabilities instead of logits.</p> In\u00a0[6]: Copied! <pre>@gen\ndef h(p):\n    v = genjax.flip(p) @ \"v\"\n    return v\n\n\nkey, subkey = jax.random.split(key)\narg = (0.3,)  # 0.3 is a valid probability\nkeys = jax.random.split(subkey, 30)\n# simulate 30 times\njax.vmap(h.simulate, in_axes=(0, None))(keys, arg).get_choices()\n</pre> @gen def h(p):     v = genjax.flip(p) @ \"v\"     return v   key, subkey = jax.random.split(key) arg = (0.3,)  # 0.3 is a valid probability keys = jax.random.split(subkey, 30) # simulate 30 times jax.vmap(h.simulate, in_axes=(0, None))(keys, arg).get_choices() Out[6]: <p>Categorical distributions also use logits instead of probabilities</p> In\u00a0[7]: Copied! <pre>@gen\ndef i(p):\n    v = genjax.categorical(p) @ \"v\"\n    return v\n\n\nkey, subkey = jax.random.split(key)\narg = ([3.0, 1.0, 2.0],)  # lists of 3 logits\nkeys = jax.random.split(subkey, 30)\n# simulate 30 times\njax.vmap(i.simulate, in_axes=(0, None))(keys, arg).get_choices()\n</pre> @gen def i(p):     v = genjax.categorical(p) @ \"v\"     return v   key, subkey = jax.random.split(key) arg = ([3.0, 1.0, 2.0],)  # lists of 3 logits keys = jax.random.split(subkey, 30) # simulate 30 times jax.vmap(i.simulate, in_axes=(0, None))(keys, arg).get_choices() Out[7]: <ol> <li>JAX code can be compiled for better performance.</li> </ol> <p><code>jit</code> is the way to force JAX to compile the code. It can be used as a decorator.</p> In\u00a0[8]: Copied! <pre>@jit\ndef f_v1(p):\n    return jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n</pre> @jit def f_v1(p):     return jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p) <p>Or as a function</p> In\u00a0[9]: Copied! <pre>f_v2 = jit(lambda p: jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p))\n</pre> f_v2 = jit(lambda p: jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)) <p>Testing the effect. Notice that the first and second have the same performance while the third is much slower (~50x on a mac m2 cpu)</p> In\u00a0[10]: Copied! <pre># Baseline\ndef f_v3(p):\n    jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n\n\narg = jax.numpy.eye(500)\n# Warmup to force jit compilation\nf_v1(arg)\nf_v2(arg)\n# Runtime comparison\n%timeit f_v1(arg)\n%timeit f_v2(arg)\n%timeit f_v3(arg)\n#\n</pre> # Baseline def f_v3(p):     jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)   arg = jax.numpy.eye(500) # Warmup to force jit compilation f_v1(arg) f_v2(arg) # Runtime comparison %timeit f_v1(arg) %timeit f_v2(arg) %timeit f_v3(arg) # <pre>150 \u03bcs \u00b1 744 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre> <pre>147 \u03bcs \u00b1 2.15 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre> <pre>22.3 ms \u00b1 75.1 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> <ol> <li>Going from Python to JAX</li> </ol> <p>4.1 For loops</p> In\u00a0[11]: Copied! <pre>def python_loop(x):\n    for i in range(100):\n        x = 2 * x\n    return x\n\n\ndef jax_loop(x):\n    jax.lax.fori_loop(0, 100, lambda i, x: 2 * x, x)\n</pre> def python_loop(x):     for i in range(100):         x = 2 * x     return x   def jax_loop(x):     jax.lax.fori_loop(0, 100, lambda i, x: 2 * x, x) <p>4.2 Conditional statements</p> In\u00a0[12]: Copied! <pre>def python_cond(x):\n    if x.sum() &gt; 0:\n        return x * x\n    else:\n        return x\n\n\ndef jax_cond(x):\n    jax.lax.cond(x.sum(), lambda x: x * x, lambda x: x, x)\n</pre> def python_cond(x):     if x.sum() &gt; 0:         return x * x     else:         return x   def jax_cond(x):     jax.lax.cond(x.sum(), lambda x: x * x, lambda x: x, x) <p>4.3 While loops</p> In\u00a0[13]: Copied! <pre>def python_while(x):\n    while x.sum() &gt; 0:\n        x = x * x\n    return x\n\n\ndef jax_while(x):\n    jax.lax.while_loop(lambda x: x.sum() &gt; 0, lambda x: x * x, x)\n</pre> def python_while(x):     while x.sum() &gt; 0:         x = x * x     return x   def jax_while(x):     jax.lax.while_loop(lambda x: x.sum() &gt; 0, lambda x: x * x, x) <ol> <li>Is my thing compiling or is it blocked at traced time?</li> </ol> <p>In Jax, the first time you run a function, it is traced, which produces a Jaxpr, a representation of the computation that Jax can optimize.</p> <p>So in order to debug whether a function is running or not, if it passes the first check that Python let's you write it, you can check if it is running by checking if it is traced, before actually running it on data.</p> <p>This is done by calling <code>make_jaxpr</code> on the function. If it returns a Jaxpr, then the function is traced and ready to be run on data.</p> In\u00a0[14]: Copied! <pre>def im_fine(x):\n    return x * x\n\n\njax.make_jaxpr(im_fine)(1.0)\n</pre> def im_fine(x):     return x * x   jax.make_jaxpr(im_fine)(1.0) Out[14]: <pre>{ lambda ; a:f32[]. let b:f32[] = mul a a in (b,) }</pre> In\u00a0[15]: Copied! <pre>def i_wont_be_so_fine(x):\n    return jax.lax.while_loop(lambda x: x &gt; 0, lambda x: x * x, x)\n\n\njax.make_jaxpr(i_wont_be_so_fine)(1.0)\n</pre> def i_wont_be_so_fine(x):     return jax.lax.while_loop(lambda x: x &gt; 0, lambda x: x * x, x)   jax.make_jaxpr(i_wont_be_so_fine)(1.0) Out[15]: <pre>{ lambda ; a:f32[]. let\n    b:f32[] = while[\n      body_jaxpr={ lambda ; c:f32[]. let d:f32[] = mul c c in (d,) }\n      body_nconsts=0\n      cond_jaxpr={ lambda ; e:f32[]. let f:bool[] = gt e 0.0 in (f,) }\n      cond_nconsts=0\n    ] a\n  in (b,) }</pre> <p>Try running the function for 8 seconds</p> In\u00a0[16]: Copied! <pre>def run_process():\n    ctx = multiprocessing.get_context(\"spawn\")\n    p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))\n    p.start()\n    time.sleep(5000)\n    if p.is_alive():\n        print(\"I'm still running\")\n        p.terminate()\n        p.join()\n\n\nresult = subprocess.run(\n    [\"python\", \"genjax/docs/sharp-edges-notebooks/basics/script.py\"],\n    capture_output=True,\n    text=True,\n)\n\n# Print the output\nresult.stdout\n</pre> def run_process():     ctx = multiprocessing.get_context(\"spawn\")     p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))     p.start()     time.sleep(5000)     if p.is_alive():         print(\"I'm still running\")         p.terminate()         p.join()   result = subprocess.run(     [\"python\", \"genjax/docs/sharp-edges-notebooks/basics/script.py\"],     capture_output=True,     text=True, )  # Print the output result.stdout Out[16]: <ol> <li>Using random keys for generative functions</li> </ol> <p>In GenJAX, we use explicit random keys to generate random numbers. This is done by splitting a key into multiple keys, and using them to generate random numbers.</p> In\u00a0[17]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(0.0, u) @ \"p\"\n    v = genjax.bernoulli(probs=p) @ \"v\"  # sweet\n    return v\n\n\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, 20)\njitted = jit(beta_bernoulli_process.simulate)\n\njax.vmap(jitted, in_axes=(0, None))(keys, (0.5,)).get_choices()\n</pre> @gen def beta_bernoulli_process(u):     p = beta(0.0, u) @ \"p\"     v = genjax.bernoulli(probs=p) @ \"v\"  # sweet     return v   key, subkey = jax.random.split(key) keys = jax.random.split(subkey, 20) jitted = jit(beta_bernoulli_process.simulate)  jax.vmap(jitted, in_axes=(0, None))(keys, (0.5,)).get_choices() Out[17]: <ol> <li>JAX uses 32-bit floats by default</li> </ol> In\u00a0[18]: Copied! <pre>key, subkey = jax.random.split(key)\nx = random.uniform(subkey, (1000,), dtype=jnp.float64)\nprint(\"surprise surprise: \", x.dtype)\n</pre> key, subkey = jax.random.split(key) x = random.uniform(subkey, (1000,), dtype=jnp.float64) print(\"surprise surprise: \", x.dtype) <pre>surprise surprise:  float32\n</pre> <pre>/tmp/ipykernel_6751/1255632939.py:2: UserWarning: Explicitly requested dtype &lt;class 'jax.numpy.float64'&gt;  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  x = random.uniform(subkey, (1000,), dtype=jnp.float64)\n</pre> <p>A common TypeError occurs when one tries using np instead of jnp, which is the JAX version of numpy, the former uses 64-bit floats by default, while the JAX version uses 32-bit floats by default.</p> <p>This on its own gives a UserWarning</p> In\u00a0[19]: Copied! <pre>jnp.array([1, 2, 3], dtype=np.float64)\n</pre> jnp.array([1, 2, 3], dtype=np.float64) <pre>/tmp/ipykernel_6751/403521608.py:1: UserWarning: Explicitly requested dtype &lt;class 'numpy.float64'&gt; requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  jnp.array([1, 2, 3], dtype=np.float64)\n</pre> Out[19]: <p>Using an array from <code>numpy</code> instead of <code>jax.numpy</code> will truncate the array to 32-bit floats and also give a UserWarning when used in JAX code</p> In\u00a0[20]: Copied! <pre>innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n\n\n@jax.jit\ndef innocent_looking_function(x):\n    return jax.lax.cond(x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x)\n\n\ninput = jnp.array([1.0, 2.0, 3.0])\ninnocent_looking_function(input)\n\ntry:\n    # This looks fine so far but...\n    innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)\n\n    # This actually raises a TypeError, as one branch has type float32\n    # while the other has type float64\n    @jax.jit\n    def innocent_looking_function(x):\n        return jax.lax.cond(\n            x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x\n        )\n\n    input = jnp.array([1, 2, 3])\n    innocent_looking_function(input)\nexcept Exception as e:\n    print(e)\n</pre> innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)   @jax.jit def innocent_looking_function(x):     return jax.lax.cond(x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x)   input = jnp.array([1.0, 2.0, 3.0]) innocent_looking_function(input)  try:     # This looks fine so far but...     innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)      # This actually raises a TypeError, as one branch has type float32     # while the other has type float64     @jax.jit     def innocent_looking_function(x):         return jax.lax.cond(             x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x         )      input = jnp.array([1, 2, 3])     innocent_looking_function(input) except Exception as e:     print(e) <pre>true_fun output and false_fun output must have identical types, got\nDIFFERENT ShapedArray(int32[3]) vs. ShapedArray(float32[3]).\n</pre> <ol> <li>Beware to OOM on the GPU which happens faster than you might think</li> </ol> <p>Here's a simple HMM model that can be run on the GPU. By simply changing $N$ from $300$ to $1000$, the code will typically run out of memory on the GPU as it will take ~300GB of memory</p> In\u00a0[21]: Copied! <pre>N = 300\nn_repeats = 100\nvariance = jnp.eye(N)\nkey, subkey = jax.random.split(key)\ninitial_state = jax.random.normal(subkey, (N,))\n\n\n@genjax.gen\ndef hmm_step(x, _):\n    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n    return new_x, None\n\n\nhmm = hmm_step.scan(n=100)\n\nkey, subkey = jax.random.split(key)\njitted = jit(hmm.repeat(n=n_repeats).simulate)\ntrace = jitted(subkey, (initial_state, None))\nkey, subkey = jax.random.split(key)\n%timeit jitted(subkey, (initial_state, None))\n</pre> N = 300 n_repeats = 100 variance = jnp.eye(N) key, subkey = jax.random.split(key) initial_state = jax.random.normal(subkey, (N,))   @genjax.gen def hmm_step(x, _):     new_x = genjax.mv_normal(x, variance) @ \"new_x\"     return new_x, None   hmm = hmm_step.scan(n=100)  key, subkey = jax.random.split(key) jitted = jit(hmm.repeat(n=n_repeats).simulate) trace = jitted(subkey, (initial_state, None)) key, subkey = jax.random.split(key) %timeit jitted(subkey, (initial_state, None)) <pre>506 ms \u00b1 1.43 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>If you are running out of memory, you can try de-batching one of the computations, or using a smaller batch size. For instance, in this example, we can de-batch the <code>repeat</code> combinator, which will reduce the memory usage by a factor of $100$, at the cost of some performance.</p> In\u00a0[22]: Copied! <pre>jitted = jit(hmm.simulate)\n\n\ndef hmm_debatched(key, initial_state):\n    keys = jax.random.split(key, n_repeats)\n    traces = {}\n    for i in range(n_repeats):\n        trace = jitted(keys[i], (initial_state, None))\n        traces[i] = trace\n    return traces\n\n\nkey, subkey = jax.random.split(key)\n# About 4x slower on arm64 CPU and 40x on a Google Colab GPU\n%timeit hmm_debatched(subkey, initial_state)\n</pre> jitted = jit(hmm.simulate)   def hmm_debatched(key, initial_state):     keys = jax.random.split(key, n_repeats)     traces = {}     for i in range(n_repeats):         trace = jitted(keys[i], (initial_state, None))         traces[i] = trace     return traces   key, subkey = jax.random.split(key) # About 4x slower on arm64 CPU and 40x on a Google Colab GPU %timeit hmm_debatched(subkey, initial_state) <pre>962 ms \u00b1 33.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <ol> <li>Fast sampling can be inaccurate and yield Nan/wrong results.</li> </ol> <p>As an example, truncating a normal distribution outside 5.5 standard deviations from its mean can yield NaNs. Many default TFP/JAX implementations that run on the GPU use fast implementations on 32bits. If one really wants that, one could use slower implementations that use 64bits and an exponential tilting Monte Carlo algorithm.</p> In\u00a0[23]: Copied! <pre>genjax.truncated_normal.sample(\n    jax.random.key(2), 0.5382424, 0.05, 0.83921564 - 0.03, 0.83921564 + 0.03\n)\n\nminv = 0.83921564 - 0.03\nmaxv = 0.83921564 + 0.03\nmean = 0.5382424\nstd = 0.05\n\n\ndef raw_jax_truncated(key, minv, maxv, mean, std):\n    low = (minv - mean) / std\n    high = (maxv - mean) / std\n    return std * jax.random.truncated_normal(key, low, high, (), jnp.float32) + mean\n\n\nraw_jax_truncated(jax.random.key(2), minv, maxv, mean, std)\n# ==&gt; Array(0.80921566, dtype=float32)\n\njax.jit(raw_jax_truncated)(jax.random.key(2), minv, maxv, mean, std)\n# ==&gt; Array(nan, dtype=float32)\n</pre> genjax.truncated_normal.sample(     jax.random.key(2), 0.5382424, 0.05, 0.83921564 - 0.03, 0.83921564 + 0.03 )  minv = 0.83921564 - 0.03 maxv = 0.83921564 + 0.03 mean = 0.5382424 std = 0.05   def raw_jax_truncated(key, minv, maxv, mean, std):     low = (minv - mean) / std     high = (maxv - mean) / std     return std * jax.random.truncated_normal(key, low, high, (), jnp.float32) + mean   raw_jax_truncated(jax.random.key(2), minv, maxv, mean, std) # ==&gt; Array(0.80921566, dtype=float32)  jax.jit(raw_jax_truncated)(jax.random.key(2), minv, maxv, mean, std) # ==&gt; Array(nan, dtype=float32) Out[23]:"},{"location":"cookbook/active/jax_basics.html#jax-basics","title":"JAX Basics \u00b6","text":""},{"location":"cookbook/inactive/generative_fun.html","title":"Generative functions","text":"In\u00a0[1]: Copied! <pre>import jax\n\nfrom genjax import bernoulli, beta, gen, pretty\n\npretty()\n</pre> import jax  from genjax import bernoulli, beta, gen, pretty  pretty() <p>The following is a simple  of a beta-bernoulli process. We use the <code>@gen</code> decorator to create generative functions.</p> In\u00a0[2]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(1.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    return v\n</pre> @gen def beta_bernoulli_process(u):     p = beta(1.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     return v <p>We can now call the generative function with a specified random key</p> In\u00a0[3]: Copied! <pre>key = jax.random.key(0)\n</pre> key = jax.random.key(0) <p>Running the function will return a trace, which records the arguments, random choices made, and the return value</p> In\u00a0[4]: Copied! <pre>key, subkey = jax.random.split(key)\ntr = beta_bernoulli_process.simulate(subkey, (1.0,))\n</pre> key, subkey = jax.random.split(key) tr = beta_bernoulli_process.simulate(subkey, (1.0,)) <p>We can print the trace to see what happened</p> In\u00a0[5]: Copied! <pre>tr.args, tr.get_retval(), tr.get_choices()\n</pre> tr.args, tr.get_retval(), tr.get_choices() Out[5]: <p>GenJAX functions can be accelerated with <code>jit</code> compilation.</p> <p>The non-optimal way is within the <code>@gen</code> decorator.</p> In\u00a0[6]: Copied! <pre>@gen\n@jax.jit\ndef fast_beta_bernoulli_process(u):\n    p = beta(0.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"  # sweet\n    return v\n</pre> @gen @jax.jit def fast_beta_bernoulli_process(u):     p = beta(0.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"  # sweet     return v <p>And the better way is to <code>jit</code> the final function we aim to run</p> In\u00a0[7]: Copied! <pre>jitted = jax.jit(beta_bernoulli_process.simulate)\n</pre> jitted = jax.jit(beta_bernoulli_process.simulate) <p>We can then compare the speed of the three functions. To fairly compare we need to run the functions once to compile them.</p> In\u00a0[8]: Copied! <pre>key, subkey = jax.random.split(key)\nfast_beta_bernoulli_process.simulate(subkey, (1.0,))\nkey, subkey = jax.random.split(key)\njitted(subkey, (1.0,))\n</pre> key, subkey = jax.random.split(key) fast_beta_bernoulli_process.simulate(subkey, (1.0,)) key, subkey = jax.random.split(key) jitted(subkey, (1.0,)) Out[8]: In\u00a0[9]: Copied! <pre>key, subkey = jax.random.split(key)\n%timeit beta_bernoulli_process.simulate(subkey, (1.0,))\nkey, subkey = jax.random.split(key)\n%timeit fast_beta_bernoulli_process.simulate(subkey, (1.0,))\nkey, subkey = jax.random.split(key)\n%timeit jitted(subkey, (1.0,))\n</pre> key, subkey = jax.random.split(key) %timeit beta_bernoulli_process.simulate(subkey, (1.0,)) key, subkey = jax.random.split(key) %timeit fast_beta_bernoulli_process.simulate(subkey, (1.0,)) key, subkey = jax.random.split(key) %timeit jitted(subkey, (1.0,)) <pre>434 ms \u00b1 4.85 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> <pre>469 \u03bcs \u00b1 2.26 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n</pre> <pre>81.4 \u03bcs \u00b1 314 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre>"},{"location":"cookbook/inactive/generative_fun.html#what-is-a-generative-function-and-how-to-use-it","title":"What is a generative function and how to use it?\u00b6","text":""},{"location":"cookbook/inactive/differentiation/adev_example.html","title":"ADEV Example","text":"In\u00a0[1]: Copied! <pre># jupyter:\n#   jupytext:\n#     formats: ipynb,py:percent\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.16.4\n#   kernelspec:\n#     display_name: .venv\n#     language: python\n#     name: python3\n# ---\n</pre> # jupyter: #   jupytext: #     formats: ipynb,py:percent #     text_representation: #       extension: .py #       format_name: percent #       format_version: '1.3' #       jupytext_version: 1.16.4 #   kernelspec: #     display_name: .venv #     language: python #     name: python3 # --- In\u00a0[2]: Copied! <pre># pyright: reportUnusedExpression=false\n</pre> # pyright: reportUnusedExpression=false In\u00a0[3]: Copied! <pre># Import and constants\nimport genstudio.plot as Plot\nimport jax\nimport jax.numpy as jnp\nfrom genstudio.plot import js\n\nfrom genjax._src.adev.core import Dual, expectation\nfrom genjax._src.adev.primitives import flip_enum, normal_reparam\n\nkey = jax.random.key(314159)\nEPOCHS = 400\ndefault_sigma = 0.05\n</pre> # Import and constants import genstudio.plot as Plot import jax import jax.numpy as jnp from genstudio.plot import js  from genjax._src.adev.core import Dual, expectation from genjax._src.adev.primitives import flip_enum, normal_reparam  key = jax.random.key(314159) EPOCHS = 400 default_sigma = 0.05 <p>We are often interested in the average returned value of a probabilistic program. For instance, it could be that a run of the program represents a run of a simulation of some form, and we would like to maximize the average reward across many simulations (or equivalently minimize a loss).</p> In\u00a0[4]: Copied! <pre># Model\ndef noisy_jax_model(key, theta, sigma):\n    b = jax.random.bernoulli(key, theta)\n    return jax.lax.cond(\n        b,\n        lambda theta: jax.random.normal(key) * sigma * theta,\n        lambda theta: jax.random.normal(key) * sigma + theta / 2,\n        theta,\n    )\n\n\ndef expected_val(theta):\n    return (theta - theta**2) / 2\n</pre> # Model def noisy_jax_model(key, theta, sigma):     b = jax.random.bernoulli(key, theta)     return jax.lax.cond(         b,         lambda theta: jax.random.normal(key) * sigma * theta,         lambda theta: jax.random.normal(key) * sigma + theta / 2,         theta,     )   def expected_val(theta):     return (theta - theta**2) / 2 <p>We can see that the simulation can have two \"modes\" that split further appart over time.</p> In\u00a0[5]: Copied! <pre># Samples\nthetas = jnp.arange(0.0, 1.0, 0.0005)\n\n\ndef make_samples(key, thetas, sigma):\n    return jax.vmap(noisy_jax_model, in_axes=(0, 0, None))(\n        jax.random.split(key, len(thetas)), thetas, sigma\n    )\n\n\nkey, samples_key = jax.random.split(key)\nnoisy_samples = make_samples(samples_key, thetas, default_sigma)\n\nplot_options = Plot.new(\n    Plot.color_legend(),\n    {\"x\": {\"label\": \"\u03b8\"}, \"y\": {\"label\": \"y\"}},\n    Plot.aspect_ratio(1),\n    Plot.grid(),\n    Plot.clip(),\n)\n\nsamples_color_map = Plot.color_map({\"Samples\": \"rgba(0, 128, 128, 0.5)\"})\n\n\ndef make_samples_plot(thetas, samples):\n    return (\n        Plot.dot({\"x\": thetas, \"y\": samples}, fill=Plot.constantly(\"Samples\"), r=2)\n        + samples_color_map\n        + plot_options\n        + Plot.clip()\n    )\n\n\nsamples_plot = make_samples_plot(thetas, noisy_samples)\n\nsamples_plot\n</pre> # Samples thetas = jnp.arange(0.0, 1.0, 0.0005)   def make_samples(key, thetas, sigma):     return jax.vmap(noisy_jax_model, in_axes=(0, 0, None))(         jax.random.split(key, len(thetas)), thetas, sigma     )   key, samples_key = jax.random.split(key) noisy_samples = make_samples(samples_key, thetas, default_sigma)  plot_options = Plot.new(     Plot.color_legend(),     {\"x\": {\"label\": \"\u03b8\"}, \"y\": {\"label\": \"y\"}},     Plot.aspect_ratio(1),     Plot.grid(),     Plot.clip(), )  samples_color_map = Plot.color_map({\"Samples\": \"rgba(0, 128, 128, 0.5)\"})   def make_samples_plot(thetas, samples):     return (         Plot.dot({\"x\": thetas, \"y\": samples}, fill=Plot.constantly(\"Samples\"), r=2)         + samples_color_map         + plot_options         + Plot.clip()     )   samples_plot = make_samples_plot(thetas, noisy_samples)  samples_plot Out[5]: <p>We can also easily imagine a more noisy version of the same idea.</p> In\u00a0[6]: Copied! <pre>def more_noisy_jax_model(key, theta, sigma):\n    b = jax.random.bernoulli(key, theta)\n    return jax.lax.cond(\n        b,\n        lambda _: jax.random.normal(key) * sigma * theta**2 / 3,\n        lambda _: (jax.random.normal(key) * sigma + theta) / 2,\n        None,\n    )\n\n\nmore_thetas = jnp.arange(0.0, 1.0, 0.0005)\nkey, *keys = jax.random.split(key, len(more_thetas) + 1)\n\nnoisy_sample_plot = (\n    Plot.dot(\n        {\n            \"x\": more_thetas,\n            \"y\": jax.vmap(more_noisy_jax_model, in_axes=(0, 0, None))(\n                jnp.array(keys), more_thetas, default_sigma\n            ),\n        },\n        fill=Plot.constantly(\"Samples\"),\n        r=2,\n    )\n    + samples_color_map\n)\n\nnoisy_sample_plot + plot_options\n</pre> def more_noisy_jax_model(key, theta, sigma):     b = jax.random.bernoulli(key, theta)     return jax.lax.cond(         b,         lambda _: jax.random.normal(key) * sigma * theta**2 / 3,         lambda _: (jax.random.normal(key) * sigma + theta) / 2,         None,     )   more_thetas = jnp.arange(0.0, 1.0, 0.0005) key, *keys = jax.random.split(key, len(more_thetas) + 1)  noisy_sample_plot = (     Plot.dot(         {             \"x\": more_thetas,             \"y\": jax.vmap(more_noisy_jax_model, in_axes=(0, 0, None))(                 jnp.array(keys), more_thetas, default_sigma             ),         },         fill=Plot.constantly(\"Samples\"),         r=2,     )     + samples_color_map )  noisy_sample_plot + plot_options Out[6]: <p>As we can see better on the noisy version, the samples divide into two groups. One tends to go up as theta increases while the other stays relatively stable around 0 with a higher variance. For simplicity of the analysis, in the rest of this notebook we will stick to the simpler first example.</p> <p>In that simple case, we can compute the exact average value of the random process as a function of $\\theta$. We have probability $\\theta$ to return $0$ and probablity $1-\\theta$ to return $\\frac{\\theta}{2}$. So overall the expected value is $$\\theta*0 + (1-\\theta)*\\frac{\\theta}{2} = \\frac{\\theta-\\theta^2}{2}$$</p> <p>We can code this and plot the result for comparison.</p> In\u00a0[7]: Copied! <pre># Adding exact expectation\nthetas_sparse = jnp.linspace(0.0, 1.0, 20)  # fewer points, for the plot\nexact_vals = jax.vmap(expected_val)(thetas_sparse)\n\nexpected_value_plot = (\n    Plot.line(\n        {\"x\": thetas_sparse, \"y\": exact_vals},\n        strokeWidth=2,\n        stroke=Plot.constantly(\"Expected value\"),\n        curve=\"natural\",\n    )\n    + Plot.color_map({\"Expected value\": \"black\"})\n    + plot_options,\n)\n\nsamples_plot + expected_value_plot\n</pre> # Adding exact expectation thetas_sparse = jnp.linspace(0.0, 1.0, 20)  # fewer points, for the plot exact_vals = jax.vmap(expected_val)(thetas_sparse)  expected_value_plot = (     Plot.line(         {\"x\": thetas_sparse, \"y\": exact_vals},         strokeWidth=2,         stroke=Plot.constantly(\"Expected value\"),         curve=\"natural\",     )     + Plot.color_map({\"Expected value\": \"black\"})     + plot_options, )  samples_plot + expected_value_plot Out[7]: <p>We can see that the curve in yellow is a perfectly reasonable differentiable function. We can use JAX to compute its derivative (more generally its gradient) at various points.</p> In\u00a0[8]: Copied! <pre>grad_exact = jax.jit(jax.grad(expected_val))\ntheta_tangent_points = [0.1, 0.3, 0.45]\n\n\ncolor1 = \"rgba(255,165,0,0.5)\"\ncolor2 = \"#FB575D\"\n\n\ndef tangent_line_plot(theta_tan):\n    slope = grad_exact(theta_tan)\n    y_intercept = expected_val(theta_tan) - slope * theta_tan\n    label = f\"Tangent at \u03b8={theta_tan}\"\n\n    return Plot.line(\n        [[0, y_intercept], [1, slope + y_intercept]],\n        stroke=Plot.constantly(label),\n    ) + Plot.color_map({\n        label: Plot.js(\n            f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta_tan}/{theta_tangent_points[-1]})\"\"\"\n        )\n    })\n\n\n(\n    plot_options\n    + [tangent_line_plot(theta_tan) for theta_tan in theta_tangent_points]\n    + expected_value_plot\n    + Plot.domain([0, 1], [0, 0.4])\n    + Plot.title(\"Expectation curve and its Tangent Lines\")\n)\n</pre> grad_exact = jax.jit(jax.grad(expected_val)) theta_tangent_points = [0.1, 0.3, 0.45]   color1 = \"rgba(255,165,0,0.5)\" color2 = \"#FB575D\"   def tangent_line_plot(theta_tan):     slope = grad_exact(theta_tan)     y_intercept = expected_val(theta_tan) - slope * theta_tan     label = f\"Tangent at \u03b8={theta_tan}\"      return Plot.line(         [[0, y_intercept], [1, slope + y_intercept]],         stroke=Plot.constantly(label),     ) + Plot.color_map({         label: Plot.js(             f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta_tan}/{theta_tangent_points[-1]})\"\"\"         )     })   (     plot_options     + [tangent_line_plot(theta_tan) for theta_tan in theta_tangent_points]     + expected_value_plot     + Plot.domain([0, 1], [0, 0.4])     + Plot.title(\"Expectation curve and its Tangent Lines\") ) Out[8]: <p>A popular technique from optimization is to use iterative methods such as (stochastic) gradient ascent. Starting from any location, say 0.2, we can use JAX to find the maximum of the function.</p> In\u00a0[9]: Copied! <pre>arg = 0.2\nvals = []\narg_list = []\nfor _ in range(EPOCHS):\n    grad_val = grad_exact(arg)\n    arg_list.append(arg)\n    vals.append(expected_val(arg))\n    arg = arg + 0.01 * grad_val\n    if arg &lt; 0:\n        arg = 0\n        break\n    elif arg &gt; 1:\n        arg = 1\n</pre> arg = 0.2 vals = [] arg_list = [] for _ in range(EPOCHS):     grad_val = grad_exact(arg)     arg_list.append(arg)     vals.append(expected_val(arg))     arg = arg + 0.01 * grad_val     if arg &lt; 0:         arg = 0         break     elif arg &gt; 1:         arg = 1 <p>We can plot the evolution of the value of the function over the iterations of the algorithms.</p> In\u00a0[10]: Copied! <pre>(\n    Plot.line({\"x\": list(range(EPOCHS)), \"y\": vals})\n    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n)\n</pre> (     Plot.line({\"x\": list(range(EPOCHS)), \"y\": vals})     + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}} ) Out[10]: <p>We can also directly visualize the points on the curve.</p> In\u00a0[11]: Copied! <pre>(\n    expected_value_plot\n    + Plot.dot(\n        {\"x\": arg_list, \"y\": vals},\n        fill=Plot.js(\n            f\"\"\"(_, i) =&gt; d3.interpolateHsl('{color1}', '{color2}')(i/{len(arg_list)})\"\"\"\n        ),\n    )\n    + Plot.subtitle(\"Gradient descent from start to end\")\n    + Plot.aspect_ratio(0.25)\n    + {\"width\": 600}\n    + Plot.color_map({\"start\": color1, \"end\": color2})\n)\n</pre> (     expected_value_plot     + Plot.dot(         {\"x\": arg_list, \"y\": vals},         fill=Plot.js(             f\"\"\"(_, i) =&gt; d3.interpolateHsl('{color1}', '{color2}')(i/{len(arg_list)})\"\"\"         ),     )     + Plot.subtitle(\"Gradient descent from start to end\")     + Plot.aspect_ratio(0.25)     + {\"width\": 600}     + Plot.color_map({\"start\": color1, \"end\": color2}) ) Out[11]: <p>We have this in this example that we can compute the average value exactly. But will not be the case in general. One popular technique to approximate an average value is to use Monte Carlo Integration: we sample a bunch from the program and take the average value.</p> <p>As we use more and more samples we will converge to the correct result by the Central limit theorem.</p> In\u00a0[12]: Copied! <pre>number_of_samples = sorted([1, 3, 5, 10, 20, 50, 100, 200, 500, 1000] * 7)\nmeans = []\nfor n in number_of_samples:\n    key, subkey = jax.random.split(key)\n    keys = jax.random.split(key, n)\n    samples = jax.vmap(noisy_jax_model, in_axes=(0, None, None))(\n        keys, 0.3, default_sigma\n    )\n    mean = jnp.mean(samples)\n    means.append(mean)\n\n(\n    Plot.dot(\n        {\"x\": number_of_samples, \"y\": means},\n        fill=Plot.js(\n            f\"\"\"(_, i) =&gt; d3.interpolateHsl('{color1}', '{color2}')(i/{len(number_of_samples)})\"\"\"\n        ),\n    )\n    + Plot.ruleY(\n        [expected_val(0.3)],\n        opacity=0.2,\n        strokeWidth=2,\n        stroke=Plot.constantly(\"True value\"),\n    )\n    + Plot.color_map({\"Mean estimate\": color1, \"True value\": \"green\"})\n    + Plot.color_legend()\n    + {\"x\": {\"label\": \"Number of samples\", \"type\": \"log\"}, \"y\": {\"label\": \"y\"}}\n)\n</pre> number_of_samples = sorted([1, 3, 5, 10, 20, 50, 100, 200, 500, 1000] * 7) means = [] for n in number_of_samples:     key, subkey = jax.random.split(key)     keys = jax.random.split(key, n)     samples = jax.vmap(noisy_jax_model, in_axes=(0, None, None))(         keys, 0.3, default_sigma     )     mean = jnp.mean(samples)     means.append(mean)  (     Plot.dot(         {\"x\": number_of_samples, \"y\": means},         fill=Plot.js(             f\"\"\"(_, i) =&gt; d3.interpolateHsl('{color1}', '{color2}')(i/{len(number_of_samples)})\"\"\"         ),     )     + Plot.ruleY(         [expected_val(0.3)],         opacity=0.2,         strokeWidth=2,         stroke=Plot.constantly(\"True value\"),     )     + Plot.color_map({\"Mean estimate\": color1, \"True value\": \"green\"})     + Plot.color_legend()     + {\"x\": {\"label\": \"Number of samples\", \"type\": \"log\"}, \"y\": {\"label\": \"y\"}} ) Out[12]: <p>As we just discussed, most of the time we will not be able to compute the average value and then compute the gradient using JAX. One thing we may want to try is to use JAX on the probabilistic program to get a gradient estimate, and hope that by using more and more samples this will converge to the correct gradient that we could use in optimization. Let's try it in JAX.</p> In\u00a0[13]: Copied! <pre>theta_tan = 0.3\n\nslope = grad_exact(theta_tan)\ny_intercept = expected_val(theta_tan) - slope * theta_tan\n\nexact_tangent_plot = Plot.line(\n    [[0, y_intercept], [1, slope + y_intercept]],\n    strokeWidth=2,\n    stroke=Plot.constantly(\"Exact tangent at \u03b8=0.3\"),\n)\n\n\ndef slope_estimate_plot(slope_est):\n    y_intercept = expected_val(theta_tan) - slope_est * theta_tan\n    return Plot.line(\n        [[0, y_intercept], [1, slope_est + y_intercept]],\n        strokeWidth=2,\n        stroke=Plot.constantly(\"Tangent estimate\"),\n    )\n\n\nslope_estimates = [slope + i / 20 for i in range(-4, 4)]\n\n(\n    samples_plot\n    + expected_value_plot\n    + [slope_estimate_plot(slope_est) for slope_est in slope_estimates]\n    + exact_tangent_plot\n    + Plot.title(\"Expectation curve and Tangent Estimates at \u03b8=0.3\")\n    + Plot.color_map({\n        \"Tangent estimate\": color1,\n        \"Exact tangent at \u03b8=0.3\": color2,\n    })\n    + Plot.domain([0, 1], [0, 0.4])\n)\n</pre> theta_tan = 0.3  slope = grad_exact(theta_tan) y_intercept = expected_val(theta_tan) - slope * theta_tan  exact_tangent_plot = Plot.line(     [[0, y_intercept], [1, slope + y_intercept]],     strokeWidth=2,     stroke=Plot.constantly(\"Exact tangent at \u03b8=0.3\"), )   def slope_estimate_plot(slope_est):     y_intercept = expected_val(theta_tan) - slope_est * theta_tan     return Plot.line(         [[0, y_intercept], [1, slope_est + y_intercept]],         strokeWidth=2,         stroke=Plot.constantly(\"Tangent estimate\"),     )   slope_estimates = [slope + i / 20 for i in range(-4, 4)]  (     samples_plot     + expected_value_plot     + [slope_estimate_plot(slope_est) for slope_est in slope_estimates]     + exact_tangent_plot     + Plot.title(\"Expectation curve and Tangent Estimates at \u03b8=0.3\")     + Plot.color_map({         \"Tangent estimate\": color1,         \"Exact tangent at \u03b8=0.3\": color2,     })     + Plot.domain([0, 1], [0, 0.4]) ) Out[13]: In\u00a0[14]: Copied! <pre>jax_grad = jax.jit(jax.grad(noisy_jax_model, argnums=1))\n\narg = 0.2\nvals = []\nfor _ in range(EPOCHS):\n    key, subkey = jax.random.split(key)\n    grad_val = jax_grad(subkey, arg, default_sigma)\n    arg = arg + 0.01 * grad_val\n    vals.append(expected_val(arg))\n</pre> jax_grad = jax.jit(jax.grad(noisy_jax_model, argnums=1))  arg = 0.2 vals = [] for _ in range(EPOCHS):     key, subkey = jax.random.split(key)     grad_val = jax_grad(subkey, arg, default_sigma)     arg = arg + 0.01 * grad_val     vals.append(expected_val(arg)) <p>JAX seems happy to compute something and we can use the iterative technique from before, but let's see if we managed to minimize the function.</p> In\u00a0[15]: Copied! <pre>(\n    Plot.line(\n        {\"x\": list(range(EPOCHS)), \"y\": vals},\n        stroke=Plot.constantly(\"Attempting gradient ascent with JAX\"),\n    )\n    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n    + Plot.domainX([0, EPOCHS])\n    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n    + Plot.color_legend()\n)\n</pre> (     Plot.line(         {\"x\": list(range(EPOCHS)), \"y\": vals},         stroke=Plot.constantly(\"Attempting gradient ascent with JAX\"),     )     + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}     + Plot.domainX([0, EPOCHS])     + Plot.title(\"Maximization of the expected value of a probabilistic function\")     + Plot.color_legend() ) Out[15]: <p>Woops! We seemed to start ok but then for some reason the curve goes back down and we end up minimizing the function instead of maximizing it!</p> <p>The reason is that we failed to account from the change of contribution of the coin flip from  <code>bernoulli</code> in the differentiation process, and we will come back to this in more details in follow up notebooks.</p> <p>For now, let's just get a sense of what the gradient estimates computed by JAX look like.</p> In\u00a0[16]: Copied! <pre>theta_tangents = jnp.linspace(0, 1, 20)\n\n\ndef plot_tangents(gradients, title):\n    tangents_plots = Plot.new(Plot.aspectRatio(0.5))\n\n    for theta, slope in gradients:\n        y_intercept = expected_val(theta) - slope * theta\n        tangents_plots += Plot.line(\n            [[0, y_intercept], [1, slope + y_intercept]],\n            stroke=Plot.js(\n                f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta}/{theta_tangents[-1]})\"\"\"\n            ),\n            opacity=0.75,\n        )\n    return Plot.new(\n        expected_value_plot,\n        Plot.domain([0, 1], [0, 0.4]),\n        tangents_plots,\n        Plot.title(title),\n        Plot.color_map({\n            f\"Tangent at \u03b8={theta_tangents[0]}\": color1,\n            f\"Tangent at \u03b8={theta_tangents[-1]}\": color2,\n        }),\n    )\n\n\ngradients = []\nfor theta in theta_tangents:\n    key, subkey = jax.random.split(key)\n    gradients.append((theta, jax_grad(subkey, theta, default_sigma)))\n\nplot_tangents(gradients, \"Expectation curve and JAX-computed tangent estimates\")\n</pre> theta_tangents = jnp.linspace(0, 1, 20)   def plot_tangents(gradients, title):     tangents_plots = Plot.new(Plot.aspectRatio(0.5))      for theta, slope in gradients:         y_intercept = expected_val(theta) - slope * theta         tangents_plots += Plot.line(             [[0, y_intercept], [1, slope + y_intercept]],             stroke=Plot.js(                 f\"\"\"d3.interpolateHsl(\"{color1}\", \"{color2}\")({theta}/{theta_tangents[-1]})\"\"\"             ),             opacity=0.75,         )     return Plot.new(         expected_value_plot,         Plot.domain([0, 1], [0, 0.4]),         tangents_plots,         Plot.title(title),         Plot.color_map({             f\"Tangent at \u03b8={theta_tangents[0]}\": color1,             f\"Tangent at \u03b8={theta_tangents[-1]}\": color2,         }),     )   gradients = [] for theta in theta_tangents:     key, subkey = jax.random.split(key)     gradients.append((theta, jax_grad(subkey, theta, default_sigma)))  plot_tangents(gradients, \"Expectation curve and JAX-computed tangent estimates\") Out[16]: <p>Ouch. They do not look even remotely close to valid gradient estimates.</p> <p>ADEV is a new algorithm that computes correct gradient estimates of expectations of probabilistic programs. It  accounts for the change to the expectation coming from a change to the randomness present in the expectation.</p> <p>GenJAX implements ADEV. Slightly rewriting the example from above using GenJAX, we can see how different the behaviour of the optimization process with the corrected gradient estimates is.</p> In\u00a0[17]: Copied! <pre>@expectation\ndef flip_approx_loss(theta, sigma):\n    b = flip_enum(theta)\n    return jax.lax.cond(\n        b,\n        lambda theta: normal_reparam(0.0, sigma) * theta,\n        lambda theta: normal_reparam(theta / 2, sigma),\n        theta,\n    )\n\n\nadev_grad = jax.jit(flip_approx_loss.jvp_estimate)\n\n\ndef compute_jax_vals(key, initial_theta, sigma):\n    current_theta = initial_theta\n    out = []\n    for _ in range(EPOCHS):\n        key, subkey = jax.random.split(key)\n        gradient = jax_grad(subkey, current_theta, sigma)\n        out.append((current_theta, expected_val(current_theta), gradient))\n        current_theta = current_theta + 0.01 * gradient\n    return out\n\n\ndef compute_adev_vals(key, initial_theta, sigma):\n    current_theta = initial_theta\n    out = []\n    for _ in range(EPOCHS):\n        key, subkey = jax.random.split(key)\n        gradient = adev_grad(\n            subkey, (Dual(current_theta, 1.0), Dual(sigma, 0.0))\n        ).tangent\n        out.append((current_theta, expected_val(current_theta), gradient))\n        current_theta = current_theta + 0.01 * gradient\n    return out\n</pre> @expectation def flip_approx_loss(theta, sigma):     b = flip_enum(theta)     return jax.lax.cond(         b,         lambda theta: normal_reparam(0.0, sigma) * theta,         lambda theta: normal_reparam(theta / 2, sigma),         theta,     )   adev_grad = jax.jit(flip_approx_loss.jvp_estimate)   def compute_jax_vals(key, initial_theta, sigma):     current_theta = initial_theta     out = []     for _ in range(EPOCHS):         key, subkey = jax.random.split(key)         gradient = jax_grad(subkey, current_theta, sigma)         out.append((current_theta, expected_val(current_theta), gradient))         current_theta = current_theta + 0.01 * gradient     return out   def compute_adev_vals(key, initial_theta, sigma):     current_theta = initial_theta     out = []     for _ in range(EPOCHS):         key, subkey = jax.random.split(key)         gradient = adev_grad(             subkey, (Dual(current_theta, 1.0), Dual(sigma, 0.0))         ).tangent         out.append((current_theta, expected_val(current_theta), gradient))         current_theta = current_theta + 0.01 * gradient     return out In\u00a0[18]: Copied! <pre>def select_evenly_spaced(items, num_samples=5):\n    \"\"\"Select evenly spaced items from a list.\"\"\"\n    if num_samples &lt;= 1:\n        return [items[0]]\n\n    result = [items[0]]\n    step = (len(items) - 1) / (num_samples - 1)\n\n    for i in range(1, num_samples - 1):\n        index = int(i * step)\n        result.append(items[index])\n\n    result.append(items[-1])\n    return result\n\n\nINITIAL_VAL = 0.2\nSLIDER_STEP = 0.01\nANIMATION_STEP = 4\n\n\nbutton_classes = (\n    \"px-3 py-1 bg-blue-500 text-white text-sm font-medium rounded-md hover:bg-blue-600\"\n)\n\n\ndef input_slider(label, value, min, max, step, on_change, default):\n    return [\n        \"label.flex.flex-col.gap-2\",\n        [\"div\", label, [\"span.font-bold.px-1\", value]],\n        [\n            \"input\",\n            {\n                \"type\": \"range\",\n                \"min\": min,\n                \"max\": max,\n                \"step\": step,\n                \"defaultValue\": default,\n                \"onChange\": on_change,\n                \"class\": \"outline-none focus:outline-none\",\n            },\n        ],\n    ]\n\n\ndef input_checkbox(label, value, on_change):\n    return [\n        \"label.flex.items-center.gap-2\",\n        [\n            \"input\",\n            {\n                \"type\": \"checkbox\",\n                \"checked\": value,\n                \"onChange\": on_change,\n                \"class\": \"h-4 w-4 rounded border-gray-300 text-blue-600 focus:ring-blue-500\",\n            },\n        ],\n        label,\n        [\"span.font-bold.px-1\", value],\n    ]\n\n\ndef render_plot(initial_val, initial_sigma):\n    SLIDER_STEP = 0.01\n    ANIMATION_STEP = 4\n    COMPARISON_HEIGHT = 200\n    currentKey = key\n\n    def computeState(val, sigma):\n        jax_key, adev_key, samples_key = jax.random.split(currentKey, num=3)\n        return {\n            \"JAX_gradients\": compute_jax_vals(jax_key, val, sigma),\n            \"ADEV_gradients\": compute_adev_vals(adev_key, val, sigma),\n            \"samples\": make_samples(samples_key, thetas, sigma),\n            \"val\": val,\n            \"sigma\": sigma,\n            \"frame\": 0,\n        }\n\n    initialState = Plot.initialState(\n        computeState(initial_val, initial_sigma) | {\"show_expected_value\": True},\n        sync={\"sigma\", \"val\"},\n    )\n\n    def refresh(widget):\n        nonlocal currentKey\n        currentKey = jax.random.split(currentKey)[0]\n        widget.state.update(computeState(widget.state.val, widget.state.sigma))\n\n    onChange = Plot.onChange({\n        \"val\": lambda widget, e: widget.state.update(\n            computeState(float(e[\"value\"]), widget.state.sigma)\n        ),\n        \"sigma\": lambda widget, e: widget.state.update(\n            computeState(widget.state.val, float(e[\"value\"]))\n        ),\n    })\n\n    samples_plot = make_samples_plot(thetas, js(\"$state.samples\"))\n\n    def plot_tangents(gradients_id):\n        tangents_plots = Plot.new(Plot.aspectRatio(0.5))\n        color = \"blue\" if gradients_id == \"ADEV\" else \"orange\"\n\n        orange_to_red_plot = Plot.dot(\n            js(f\"$state.{gradients_id}_gradients\"),\n            x=\"0\",\n            y=\"1\",\n            fill=js(\n                f\"\"\"(_, i) =&gt; d3.interpolateHsl('transparent', '{color}')(i/{EPOCHS})\"\"\"\n            ),\n            filter=(js(\"(d, i) =&gt; i &lt;= $state.frame\")),\n        )\n\n        tangents_plots += orange_to_red_plot\n\n        tangents_plots += Plot.line(\n            js(f\"\"\"$state.{gradients_id}_gradients.flatMap(([theta, expected_val, slope], i) =&gt; {{\n                        const y_intercept = expected_val - slope * theta\n                        return [[0, y_intercept, i], [1, slope + y_intercept, i]]\n                    }})\n                    \"\"\"),\n            z=\"2\",\n            stroke=Plot.constantly(f\"{gradients_id} Tangent\"),\n            opacity=js(\"(data) =&gt; data[2] === $state.frame ? 1 : 0.5\"),\n            strokeWidth=js(\"(data) =&gt; data[2] === $state.frame ? 3 : 1\"),\n            filter=js(f\"\"\"(data) =&gt; {{\n                const index = data[2];\n                if (index === $state.frame) return true;\n                if (index &lt; $state.frame) {{\n                    const step = Math.floor({EPOCHS} / 10);\n                    return (index % step === 0);\n                }}\n                return false;\n            }}\"\"\"),\n        )\n\n        return Plot.new(\n            js(\"$state.show_expected_value ? %1 : null\", expected_value_plot),\n            Plot.domain([0, 1], [0, 0.4]),\n            tangents_plots,\n            Plot.title(f\"{gradients_id} Gradient Estimates\"),\n            Plot.color_map({\"JAX Tangent\": \"orange\", \"ADEV Tangent\": \"blue\"}),\n        )\n\n    comparison_plot = (\n        Plot.line(\n            js(\"$state.JAX_gradients.slice(0, $state.frame+1)\"),\n            x=Plot.index(),\n            y=\"2\",\n            stroke=Plot.constantly(\"Gradients from JAX\"),\n        )\n        + Plot.line(\n            js(\"$state.ADEV_gradients.slice(0, $state.frame+1)\"),\n            x=Plot.index(),\n            y=\"2\",\n            stroke=Plot.constantly(\"Gradients from ADEV\"),\n        )\n        + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n        + Plot.domainX([0, EPOCHS])\n        + Plot.title(\"Comparison of computed gradients JAX vs ADEV\")\n        + Plot.color_legend()\n        + {\"height\": COMPARISON_HEIGHT}\n    )\n\n    optimization_plot = Plot.new(\n        Plot.line(\n            js(\"$state.JAX_gradients\"),\n            x=Plot.index(),\n            y=\"1\",\n            stroke=Plot.constantly(\"Gradient ascent with JAX\"),\n            filter=js(\"(d, i) =&gt; i &lt;= $state.frame\"),\n        )\n        + Plot.line(\n            js(\"$state.ADEV_gradients\"),\n            x=Plot.index(),\n            y=\"1\",\n            stroke=Plot.constantly(\"Gradient ascent with ADEV\"),\n            filter=js(\"(d, i) =&gt; i &lt;= $state.frame\"),\n        )\n        + {\n            \"x\": {\"label\": \"Iteration\"},\n            \"y\": {\"label\": \"Expected Value\"},\n        }\n        + Plot.domainX([0, EPOCHS])\n        + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n        + Plot.color_legend()\n        + {\"height\": COMPARISON_HEIGHT}\n    )\n\n    jax_tangents_plot = samples_plot + plot_tangents(\"JAX\")\n    adev_tangents_plot = samples_plot + plot_tangents(\"ADEV\")\n\n    frame_slider = Plot.Slider(\n        key=\"frame\",\n        init=0,\n        range=[0, EPOCHS],\n        step=ANIMATION_STEP,\n        fps=30,\n        label=\"Iteration:\",\n    )\n\n    controls = Plot.html([\n        \"div.flex.mb-3.gap-4.bg-gray-200.rounded-md.p-3\",\n        [\n            \"div.flex.flex-col.gap-1.w-32\",\n            input_slider(\n                label=\"Initial Value:\",\n                value=js(\"$state.val\"),\n                min=0,\n                max=1,\n                step=SLIDER_STEP,\n                on_change=js(\"(e) =&gt; $state.val = parseFloat(e.target.value)\"),\n                default=initial_val,\n            ),\n            input_slider(\n                label=\"Sigma:\",\n                value=js(\"$state.sigma\"),\n                min=0,\n                max=0.2,\n                step=0.01,\n                on_change=js(\"(e) =&gt; $state.sigma = parseFloat(e.target.value)\"),\n                default=initial_sigma,\n            ),\n        ],\n        [\n            \"div.flex.flex-col.gap-2.flex-auto\",\n            input_checkbox(\n                label=\"Show expected value\",\n                value=js(\"$state.show_expected_value\"),\n                on_change=js(\"(e) =&gt; $state.show_expected_value = e.target.checked\"),\n            ),\n            Plot.katex(r\"\"\"\ny(\\theta) = \\mathbb{E}_{x\\sim P(\\theta)}[x] = \\int_{\\mathbb{R}}\\left[\\theta^2\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x}{\\sigma}\\right)^2} + (1-\\theta)\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x-0.5\\theta}{\\sigma}\\right)^2}\\right]dx =\\frac{\\theta-\\theta^2}{2}\n        \"\"\"),\n            [\n                \"button.w-32\",\n                {\n                    \"onClick\": lambda widget, e: refresh(widget),\n                    \"class\": button_classes,\n                },\n                \"Refresh\",\n            ],\n        ],\n    ])\n\n    jax_code = \"\"\"def noisy_jax_model(key, theta, sigma):\n    b = jax.random.bernoulli(key, theta)\n    return jax.lax.cond(\n        b,\n        lambda theta: jax.random.normal(key) * sigma * theta,\n        lambda theta: jax.random.normal(key) * sigma + theta / 2,\n        theta,\n    )\"\"\"\n\n    adev_code = \"\"\"@expectation\ndef flip_approx_loss(theta, sigma):\n    b = flip_enum(theta)\n    return jax.lax.cond(\n        b,\n        lambda theta: normal_reparam(0.0, sigma) * theta,\n        lambda theta: normal_reparam(theta / 2, sigma),\n        theta,\n    )\"\"\"\n\n    GRID = \"div.grid.grid-cols-2.gap-4\"\n    PRE = \"pre.whitespace-pre-wrap.text-2xs.p-3.rounded-md.bg-gray-100.flex-1\"\n\n    return (\n        initialState\n        | onChange\n        | controls\n        | [\n            GRID,\n            jax_tangents_plot,\n            adev_tangents_plot,\n            [PRE, jax_code],\n            [PRE, adev_code],\n            comparison_plot,\n            optimization_plot,\n        ]\n        | frame_slider\n    )\n\n\nrender_plot(0.2, 0.05)\n</pre> def select_evenly_spaced(items, num_samples=5):     \"\"\"Select evenly spaced items from a list.\"\"\"     if num_samples &lt;= 1:         return [items[0]]      result = [items[0]]     step = (len(items) - 1) / (num_samples - 1)      for i in range(1, num_samples - 1):         index = int(i * step)         result.append(items[index])      result.append(items[-1])     return result   INITIAL_VAL = 0.2 SLIDER_STEP = 0.01 ANIMATION_STEP = 4   button_classes = (     \"px-3 py-1 bg-blue-500 text-white text-sm font-medium rounded-md hover:bg-blue-600\" )   def input_slider(label, value, min, max, step, on_change, default):     return [         \"label.flex.flex-col.gap-2\",         [\"div\", label, [\"span.font-bold.px-1\", value]],         [             \"input\",             {                 \"type\": \"range\",                 \"min\": min,                 \"max\": max,                 \"step\": step,                 \"defaultValue\": default,                 \"onChange\": on_change,                 \"class\": \"outline-none focus:outline-none\",             },         ],     ]   def input_checkbox(label, value, on_change):     return [         \"label.flex.items-center.gap-2\",         [             \"input\",             {                 \"type\": \"checkbox\",                 \"checked\": value,                 \"onChange\": on_change,                 \"class\": \"h-4 w-4 rounded border-gray-300 text-blue-600 focus:ring-blue-500\",             },         ],         label,         [\"span.font-bold.px-1\", value],     ]   def render_plot(initial_val, initial_sigma):     SLIDER_STEP = 0.01     ANIMATION_STEP = 4     COMPARISON_HEIGHT = 200     currentKey = key      def computeState(val, sigma):         jax_key, adev_key, samples_key = jax.random.split(currentKey, num=3)         return {             \"JAX_gradients\": compute_jax_vals(jax_key, val, sigma),             \"ADEV_gradients\": compute_adev_vals(adev_key, val, sigma),             \"samples\": make_samples(samples_key, thetas, sigma),             \"val\": val,             \"sigma\": sigma,             \"frame\": 0,         }      initialState = Plot.initialState(         computeState(initial_val, initial_sigma) | {\"show_expected_value\": True},         sync={\"sigma\", \"val\"},     )      def refresh(widget):         nonlocal currentKey         currentKey = jax.random.split(currentKey)[0]         widget.state.update(computeState(widget.state.val, widget.state.sigma))      onChange = Plot.onChange({         \"val\": lambda widget, e: widget.state.update(             computeState(float(e[\"value\"]), widget.state.sigma)         ),         \"sigma\": lambda widget, e: widget.state.update(             computeState(widget.state.val, float(e[\"value\"]))         ),     })      samples_plot = make_samples_plot(thetas, js(\"$state.samples\"))      def plot_tangents(gradients_id):         tangents_plots = Plot.new(Plot.aspectRatio(0.5))         color = \"blue\" if gradients_id == \"ADEV\" else \"orange\"          orange_to_red_plot = Plot.dot(             js(f\"$state.{gradients_id}_gradients\"),             x=\"0\",             y=\"1\",             fill=js(                 f\"\"\"(_, i) =&gt; d3.interpolateHsl('transparent', '{color}')(i/{EPOCHS})\"\"\"             ),             filter=(js(\"(d, i) =&gt; i &lt;= $state.frame\")),         )          tangents_plots += orange_to_red_plot          tangents_plots += Plot.line(             js(f\"\"\"$state.{gradients_id}_gradients.flatMap(([theta, expected_val, slope], i) =&gt; {{                         const y_intercept = expected_val - slope * theta                         return [[0, y_intercept, i], [1, slope + y_intercept, i]]                     }})                     \"\"\"),             z=\"2\",             stroke=Plot.constantly(f\"{gradients_id} Tangent\"),             opacity=js(\"(data) =&gt; data[2] === $state.frame ? 1 : 0.5\"),             strokeWidth=js(\"(data) =&gt; data[2] === $state.frame ? 3 : 1\"),             filter=js(f\"\"\"(data) =&gt; {{                 const index = data[2];                 if (index === $state.frame) return true;                 if (index &lt; $state.frame) {{                     const step = Math.floor({EPOCHS} / 10);                     return (index % step === 0);                 }}                 return false;             }}\"\"\"),         )          return Plot.new(             js(\"$state.show_expected_value ? %1 : null\", expected_value_plot),             Plot.domain([0, 1], [0, 0.4]),             tangents_plots,             Plot.title(f\"{gradients_id} Gradient Estimates\"),             Plot.color_map({\"JAX Tangent\": \"orange\", \"ADEV Tangent\": \"blue\"}),         )      comparison_plot = (         Plot.line(             js(\"$state.JAX_gradients.slice(0, $state.frame+1)\"),             x=Plot.index(),             y=\"2\",             stroke=Plot.constantly(\"Gradients from JAX\"),         )         + Plot.line(             js(\"$state.ADEV_gradients.slice(0, $state.frame+1)\"),             x=Plot.index(),             y=\"2\",             stroke=Plot.constantly(\"Gradients from ADEV\"),         )         + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}         + Plot.domainX([0, EPOCHS])         + Plot.title(\"Comparison of computed gradients JAX vs ADEV\")         + Plot.color_legend()         + {\"height\": COMPARISON_HEIGHT}     )      optimization_plot = Plot.new(         Plot.line(             js(\"$state.JAX_gradients\"),             x=Plot.index(),             y=\"1\",             stroke=Plot.constantly(\"Gradient ascent with JAX\"),             filter=js(\"(d, i) =&gt; i &lt;= $state.frame\"),         )         + Plot.line(             js(\"$state.ADEV_gradients\"),             x=Plot.index(),             y=\"1\",             stroke=Plot.constantly(\"Gradient ascent with ADEV\"),             filter=js(\"(d, i) =&gt; i &lt;= $state.frame\"),         )         + {             \"x\": {\"label\": \"Iteration\"},             \"y\": {\"label\": \"Expected Value\"},         }         + Plot.domainX([0, EPOCHS])         + Plot.title(\"Maximization of the expected value of a probabilistic function\")         + Plot.color_legend()         + {\"height\": COMPARISON_HEIGHT}     )      jax_tangents_plot = samples_plot + plot_tangents(\"JAX\")     adev_tangents_plot = samples_plot + plot_tangents(\"ADEV\")      frame_slider = Plot.Slider(         key=\"frame\",         init=0,         range=[0, EPOCHS],         step=ANIMATION_STEP,         fps=30,         label=\"Iteration:\",     )      controls = Plot.html([         \"div.flex.mb-3.gap-4.bg-gray-200.rounded-md.p-3\",         [             \"div.flex.flex-col.gap-1.w-32\",             input_slider(                 label=\"Initial Value:\",                 value=js(\"$state.val\"),                 min=0,                 max=1,                 step=SLIDER_STEP,                 on_change=js(\"(e) =&gt; $state.val = parseFloat(e.target.value)\"),                 default=initial_val,             ),             input_slider(                 label=\"Sigma:\",                 value=js(\"$state.sigma\"),                 min=0,                 max=0.2,                 step=0.01,                 on_change=js(\"(e) =&gt; $state.sigma = parseFloat(e.target.value)\"),                 default=initial_sigma,             ),         ],         [             \"div.flex.flex-col.gap-2.flex-auto\",             input_checkbox(                 label=\"Show expected value\",                 value=js(\"$state.show_expected_value\"),                 on_change=js(\"(e) =&gt; $state.show_expected_value = e.target.checked\"),             ),             Plot.katex(r\"\"\" y(\\theta) = \\mathbb{E}_{x\\sim P(\\theta)}[x] = \\int_{\\mathbb{R}}\\left[\\theta^2\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x}{\\sigma}\\right)^2} + (1-\\theta)\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{\\left(\\frac{x-0.5\\theta}{\\sigma}\\right)^2}\\right]dx =\\frac{\\theta-\\theta^2}{2}         \"\"\"),             [                 \"button.w-32\",                 {                     \"onClick\": lambda widget, e: refresh(widget),                     \"class\": button_classes,                 },                 \"Refresh\",             ],         ],     ])      jax_code = \"\"\"def noisy_jax_model(key, theta, sigma):     b = jax.random.bernoulli(key, theta)     return jax.lax.cond(         b,         lambda theta: jax.random.normal(key) * sigma * theta,         lambda theta: jax.random.normal(key) * sigma + theta / 2,         theta,     )\"\"\"      adev_code = \"\"\"@expectation def flip_approx_loss(theta, sigma):     b = flip_enum(theta)     return jax.lax.cond(         b,         lambda theta: normal_reparam(0.0, sigma) * theta,         lambda theta: normal_reparam(theta / 2, sigma),         theta,     )\"\"\"      GRID = \"div.grid.grid-cols-2.gap-4\"     PRE = \"pre.whitespace-pre-wrap.text-2xs.p-3.rounded-md.bg-gray-100.flex-1\"      return (         initialState         | onChange         | controls         | [             GRID,             jax_tangents_plot,             adev_tangents_plot,             [PRE, jax_code],             [PRE, adev_code],             comparison_plot,             optimization_plot,         ]         | frame_slider     )   render_plot(0.2, 0.05) Out[18]: <p>In the above example, by using <code>jvp_estimate</code> we used a forward-mode version of ADEV. GenJAX also supports a reverse-mode version which is also fully compatible with JAX and can be jitted.</p> In\u00a0[19]: Copied! <pre>@expectation\ndef flip_exact_loss(theta):\n    b = flip_enum(theta)\n    return jax.lax.cond(\n        b,\n        lambda _: 0.0,\n        lambda _: -theta / 2.0,\n        theta,\n    )\n\n\nrev_adev_grad = jax.jit(flip_exact_loss.grad_estimate)\n\narg = 0.2\nrev_adev_vals = []\nfor _ in range(EPOCHS):\n    key, subkey = jax.random.split(key)\n    (grad_val,) = rev_adev_grad(subkey, (arg,))\n    arg = arg - 0.01 * grad_val\n    rev_adev_vals.append(expected_val(arg))\n\n(\n    Plot.line(\n        {\"x\": list(range(EPOCHS)), \"y\": rev_adev_vals},\n        stroke=Plot.constantly(\"Reverse mode ADEV\"),\n    )\n    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n    + Plot.domainX([0, EPOCHS])\n    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n    + Plot.color_legend()\n)\n</pre> @expectation def flip_exact_loss(theta):     b = flip_enum(theta)     return jax.lax.cond(         b,         lambda _: 0.0,         lambda _: -theta / 2.0,         theta,     )   rev_adev_grad = jax.jit(flip_exact_loss.grad_estimate)  arg = 0.2 rev_adev_vals = [] for _ in range(EPOCHS):     key, subkey = jax.random.split(key)     (grad_val,) = rev_adev_grad(subkey, (arg,))     arg = arg - 0.01 * grad_val     rev_adev_vals.append(expected_val(arg))  (     Plot.line(         {\"x\": list(range(EPOCHS)), \"y\": rev_adev_vals},         stroke=Plot.constantly(\"Reverse mode ADEV\"),     )     + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}     + Plot.domainX([0, EPOCHS])     + Plot.title(\"Maximization of the expected value of a probabilistic function\")     + Plot.color_legend() ) Out[19]:"},{"location":"cookbook/inactive/differentiation/adev_example.html#differentiating-probabilistic-programs","title":"Differentiating probabilistic programs\u00b6","text":""},{"location":"cookbook/inactive/differentiation/adev_example.html#differentiating-probabilistic-programs","title":"Differentiating probabilistic programs\u00b6","text":""},{"location":"cookbook/inactive/expressivity/conditionals.html","title":"Conditionals","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\n\nfrom genjax import bernoulli, gen, normal, or_else, pretty, switch\n\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp  from genjax import bernoulli, gen, normal, or_else, pretty, switch  key = jax.random.key(0) pretty() <p>In pure Python, we can use usual conditionals</p> In\u00a0[3]: Copied! <pre>def simple_cond_python(p):\n    if p &gt; 0:\n        return 2 * p\n    else:\n        return -p\n\n\nsimple_cond_python(0.3), simple_cond_python(-0.4)\n</pre> def simple_cond_python(p):     if p &gt; 0:         return 2 * p     else:         return -p   simple_cond_python(0.3), simple_cond_python(-0.4) Out[3]: <p>In pure JAX, we write conditionals with <code>jax.lax.cond</code> as follows</p> In\u00a0[4]: Copied! <pre>def branch_1(p):\n    return 2 * p\n\n\ndef branch_2(p):\n    return -p\n\n\ndef simple_cond_jax(p):\n    pred = p &gt; 0\n    arg_of_cond = p\n    cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)\n    return cond_res\n\n\nsimple_cond_jax(0.3), simple_cond_jax(-0.4)\n</pre> def branch_1(p):     return 2 * p   def branch_2(p):     return -p   def simple_cond_jax(p):     pred = p &gt; 0     arg_of_cond = p     cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)     return cond_res   simple_cond_jax(0.3), simple_cond_jax(-0.4) Out[4]: <p>Compiled JAX code is usually quite faster than Python code</p> In\u00a0[5]: Copied! <pre>def python_loop(x):\n    for i in range(40000):\n        if x &lt; 100.0:\n            x = 2 * x\n        else:\n            x = x - 97.0\n    return x\n\n\n@jax.jit\ndef jax_loop(x):\n    return jax.lax.fori_loop(\n        0,\n        40000,\n        lambda _, x: jax.lax.cond(x &lt; 100.0, lambda x: 2 * x, lambda x: x - 97.0, x),\n        x,\n    )\n\n\n%timeit python_loop(1.0)\n# Get the JIT time out of the way\njax_loop(1.0)\n%timeit jax_loop(1.0)\n</pre> def python_loop(x):     for i in range(40000):         if x &lt; 100.0:             x = 2 * x         else:             x = x - 97.0     return x   @jax.jit def jax_loop(x):     return jax.lax.fori_loop(         0,         40000,         lambda _, x: jax.lax.cond(x &lt; 100.0, lambda x: 2 * x, lambda x: x - 97.0, x),         x,     )   %timeit python_loop(1.0) # Get the JIT time out of the way jax_loop(1.0) %timeit jax_loop(1.0) <pre>1.99 ms \u00b1 21.1 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n</pre> <pre>3.1 ms \u00b1 297 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>One restriction is that both branches should have the same pytree structure</p> In\u00a0[6]: Copied! <pre>def failing_simple_cond_1(p):\n    pred = p &gt; 0\n\n    def branch_1(p):\n        return (p, p)\n\n    def branch_2(p):\n        return -p\n\n    arg_of_cond = p\n    cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)\n    return cond_res\n\n\ntry:\n    print(failing_simple_cond_1(0.3))\nexcept TypeError as e:\n    print(e)\n</pre> def failing_simple_cond_1(p):     pred = p &gt; 0      def branch_1(p):         return (p, p)      def branch_2(p):         return -p      arg_of_cond = p     cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)     return cond_res   try:     print(failing_simple_cond_1(0.3)) except TypeError as e:     print(e) <pre>true_fun output must have same type structure as false_fun output, but there are differences: \n    * at output, true_fun output has &lt;class 'tuple'&gt; and false_fun output has pytree leaf, so their Python types differ\n</pre> <p>The other one is that the type of the output of the branches should be the same</p> In\u00a0[7]: Copied! <pre>def failing_simple_cond_2(p):\n    pred = p &gt; 0\n\n    def branch_1(p):\n        return 2 * p\n\n    def branch_2(p):\n        return 7\n\n    arg_of_cond = p\n    cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)\n    return cond_res\n\n\ntry:\n    print(failing_simple_cond_2(0.3))\nexcept TypeError as e:\n    print(e)\n</pre> def failing_simple_cond_2(p):     pred = p &gt; 0      def branch_1(p):         return 2 * p      def branch_2(p):         return 7      arg_of_cond = p     cond_res = jax.lax.cond(pred, branch_1, branch_2, arg_of_cond)     return cond_res   try:     print(failing_simple_cond_2(0.3)) except TypeError as e:     print(e) <pre>true_fun output and false_fun output must have identical types, got\nDIFFERENT ShapedArray(float32[], weak_type=True) vs. ShapedArray(int32[], weak_type=True).\n</pre> <p>In GenJAX, the syntax is a bit different still. Similarly to JAX having a custom primitive <code>jax.lax.cond</code> that creates a conditional by \"composing\" two functions seen as branches, GenJAX has a custom combinator that \"composes\" two generative functions, called <code>genjax.or_else</code>.</p> <p>We can first define the two branches as generative functions</p> In\u00a0[8]: Copied! <pre>@gen\ndef branch_1(p):\n    v = bernoulli(p) @ \"v1\"\n    return v\n\n\n@gen\ndef branch_2(p):\n    v = bernoulli(-p) @ \"v2\"\n    return v\n</pre> @gen def branch_1(p):     v = bernoulli(p) @ \"v1\"     return v   @gen def branch_2(p):     v = bernoulli(-p) @ \"v2\"     return v <p>Then we use the combinator to compose them</p> In\u00a0[9]: Copied! <pre>@gen\ndef cond_model(p):\n    pred = p &gt; 0\n    arg_1 = (p,)\n    arg_2 = (p,)\n    v = or_else(branch_1, branch_2)(pred, arg_1, arg_2) @ \"cond\"\n    return v\n</pre> @gen def cond_model(p):     pred = p &gt; 0     arg_1 = (p,)     arg_2 = (p,)     v = or_else(branch_1, branch_2)(pred, arg_1, arg_2) @ \"cond\"     return v In\u00a0[10]: Copied! <pre>jitted = jax.jit(cond_model.simulate)\nkey, subkey = jax.random.split(key)\ntr = jitted(subkey, (0.0,))\ntr.get_choices()\n</pre> jitted = jax.jit(cond_model.simulate) key, subkey = jax.random.split(key) tr = jitted(subkey, (0.0,)) tr.get_choices() Out[10]: <p>Alternatively, we can write <code>or_else</code> as follows:</p> In\u00a0[11]: Copied! <pre>@gen\ndef cond_model_v2(p):\n    pred = p &gt; 0\n    arg_1 = (p,)\n    arg_2 = (p,)\n    v = branch_1.or_else(branch_2)(pred, arg_1, arg_2) @ \"cond\"\n    return v\n</pre> @gen def cond_model_v2(p):     pred = p &gt; 0     arg_1 = (p,)     arg_2 = (p,)     v = branch_1.or_else(branch_2)(pred, arg_1, arg_2) @ \"cond\"     return v In\u00a0[12]: Copied! <pre>key, subkey = jax.random.split(key)\ncond_model_v2.simulate(subkey, (0.0,))\n</pre> key, subkey = jax.random.split(key) cond_model_v2.simulate(subkey, (0.0,)) Out[12]: <p>Note that it may be possible to write the following down, but this will not give you what you want in general!</p> In\u00a0[13]: Copied! <pre># TODO: find a way to make it fail to better show the point.\n@gen\ndef simple_cond_genjax(p):\n    def branch_1(p):\n        return bernoulli(p) @ \"v1\"\n\n    def branch_2(p):\n        return bernoulli(-p) @ \"v2\"\n\n    cond = jax.lax.cond(p &gt; 0, branch_1, branch_2, p)\n    return cond\n\n\nkey, subkey = jax.random.split(key)\ntr1 = simple_cond_genjax.simulate(subkey, (0.3,))\nkey, subkey = jax.random.split(key)\ntr2 = simple_cond_genjax.simulate(subkey, (-0.4,))\ntr1.get_retval(), tr2.get_retval()\n</pre> # TODO: find a way to make it fail to better show the point. @gen def simple_cond_genjax(p):     def branch_1(p):         return bernoulli(p) @ \"v1\"      def branch_2(p):         return bernoulli(-p) @ \"v2\"      cond = jax.lax.cond(p &gt; 0, branch_1, branch_2, p)     return cond   key, subkey = jax.random.split(key) tr1 = simple_cond_genjax.simulate(subkey, (0.3,)) key, subkey = jax.random.split(key) tr2 = simple_cond_genjax.simulate(subkey, (-0.4,)) tr1.get_retval(), tr2.get_retval() Out[13]: <p>Alternatively, if we have more than two branches, in JAX we can use the <code>jax.lax.switch</code> function.</p> In\u00a0[14]: Copied! <pre>def simple_switch_jax(p):\n    index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3\n    branches = [lambda p: 2 * p, lambda p: -p, lambda p: p]\n    switch_res = jax.lax.switch(index, branches, p)\n    return switch_res\n\n\nsimple_switch_jax(0.3), simple_switch_jax(1.1), simple_switch_jax(2.3)\n</pre> def simple_switch_jax(p):     index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3     branches = [lambda p: 2 * p, lambda p: -p, lambda p: p]     switch_res = jax.lax.switch(index, branches, p)     return switch_res   simple_switch_jax(0.3), simple_switch_jax(1.1), simple_switch_jax(2.3) Out[14]: <p>Likewise, in GenJAX we can use the <code>switch</code> combinator if we have more than two branches. We can first define three branches as generative functions</p> In\u00a0[15]: Copied! <pre>@gen\ndef branch_1(p):\n    v = normal(p, 1.0) @ \"v1\"\n    return v\n\n\n@gen\ndef branch_2(p):\n    v = normal(-p, 1.0) @ \"v2\"\n    return v\n\n\n@gen\ndef branch_3(p):\n    v = normal(p * p, 1.0) @ \"v3\"\n    return v\n</pre> @gen def branch_1(p):     v = normal(p, 1.0) @ \"v1\"     return v   @gen def branch_2(p):     v = normal(-p, 1.0) @ \"v2\"     return v   @gen def branch_3(p):     v = normal(p * p, 1.0) @ \"v3\"     return v <p>Then we use the combinator to compose them.</p> In\u00a0[16]: Copied! <pre>@gen\ndef switch_model(p):\n    index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3\n    v = switch(branch_1, branch_2, branch_3)(index, (p,), (p,), (p,)) @ \"s\"\n    return v\n\n\nkey, subkey = jax.random.split(key)\njitted = jax.jit(switch_model.simulate)\ntr1 = jitted(subkey, (0.0,))\nkey, subkey = jax.random.split(key)\ntr2 = jitted(subkey, (1.1,))\nkey, subkey = jax.random.split(key)\ntr3 = jitted(subkey, (2.2,))\n(\n    tr1.get_choices()[\"s\", \"v1\"],\n    tr2.get_choices()[\"s\", \"v2\"],\n    tr3.get_choices()[\"s\", \"v3\"],\n)\n</pre> @gen def switch_model(p):     index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3     v = switch(branch_1, branch_2, branch_3)(index, (p,), (p,), (p,)) @ \"s\"     return v   key, subkey = jax.random.split(key) jitted = jax.jit(switch_model.simulate) tr1 = jitted(subkey, (0.0,)) key, subkey = jax.random.split(key) tr2 = jitted(subkey, (1.1,)) key, subkey = jax.random.split(key) tr3 = jitted(subkey, (2.2,)) (     tr1.get_choices()[\"s\", \"v1\"],     tr2.get_choices()[\"s\", \"v2\"],     tr3.get_choices()[\"s\", \"v3\"], ) Out[16]: <p>We can rewrite the above a bit more elegantly using the *args syntax</p> In\u00a0[17]: Copied! <pre>@gen\ndef switch_model_v2(p):\n    index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3\n    branches = [branch_1, branch_2, branch_3]\n    args = [(p,), (p,), (p,)]\n    v = switch(*branches)(index, *args) @ \"switch\"\n    return v\n\n\njitted = switch_model_v2.simulate\nkey, subkey = jax.random.split(key)\ntr = jitted(subkey, (0.0,))\ntr.get_choices()[\"switch\", \"v1\"]\n</pre> @gen def switch_model_v2(p):     index = jnp.floor(jnp.abs(p)).astype(jnp.int32) % 3     branches = [branch_1, branch_2, branch_3]     args = [(p,), (p,), (p,)]     v = switch(*branches)(index, *args) @ \"switch\"     return v   jitted = switch_model_v2.simulate key, subkey = jax.random.split(key) tr = jitted(subkey, (0.0,)) tr.get_choices()[\"switch\", \"v1\"] Out[17]:"},{"location":"cookbook/inactive/expressivity/conditionals.html#how-do-i-use-conditionals-in-genjax","title":"How do I use conditionals in (Gen)JAX? \u00b6","text":""},{"location":"cookbook/inactive/expressivity/custom_distribution.html","title":"Custom distribution","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nfrom tensorflow_probability.substrates import jax as tfp\n\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import Distribution, ExactDensity, Pytree, Weight, gen, normal, pretty\nfrom genjax.typing import PRNGKey\n\ntfd = tfp.distributions\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp from tensorflow_probability.substrates import jax as tfp  from genjax import ChoiceMapBuilder as C from genjax import Distribution, ExactDensity, Pytree, Weight, gen, normal, pretty from genjax.typing import PRNGKey  tfd = tfp.distributions key = jax.random.key(0) pretty() <p>In GenJAX, there are two simple ways to extend the language by adding custom distributions which can be seamlessly used by the system.</p> <p>The first way is to add a distribution for which we can compute its density exactly. In this case the API follows what one expects: one method to sample and one method to compute logpdf.</p> In\u00a0[3]: Copied! <pre>@Pytree.dataclass\nclass NormalInverseGamma(ExactDensity):\n    def sample(self, key: PRNGKey, \u03bc, \u03c3, \u03b1, \u03b2):\n        key, subkey = jax.random.split(key)\n        x = tfd.Normal(\u03bc, \u03c3).sample(seed=key)\n        y = tfd.InverseGamma(\u03b1, \u03b2).sample(seed=subkey)\n        return (x, y)\n\n    def logpdf(self, v, \u03bc, \u03c3, \u03b1, \u03b2):\n        x, y = v\n        a = tfd.Normal(\u03bc, \u03c3).log_prob(x)\n        b = tfd.InverseGamma(\u03b1, \u03b2).log_prob(y)\n        return a + b\n</pre> @Pytree.dataclass class NormalInverseGamma(ExactDensity):     def sample(self, key: PRNGKey, \u03bc, \u03c3, \u03b1, \u03b2):         key, subkey = jax.random.split(key)         x = tfd.Normal(\u03bc, \u03c3).sample(seed=key)         y = tfd.InverseGamma(\u03b1, \u03b2).sample(seed=subkey)         return (x, y)      def logpdf(self, v, \u03bc, \u03c3, \u03b1, \u03b2):         x, y = v         a = tfd.Normal(\u03bc, \u03c3).log_prob(x)         b = tfd.InverseGamma(\u03b1, \u03b2).log_prob(y)         return a + b <p>Testing</p> In\u00a0[4]: Copied! <pre># Create a particular instance of the distribution\nnorm_inv_gamma = NormalInverseGamma()\n\n\n@gen\ndef model():\n    (x, y) = norm_inv_gamma(0.0, 1.0, 1.0, 1.0) @ \"xy\"\n    z = normal(x, y) @ \"z\"\n    return z\n\n\n# Sampling from the model\nkey, subkey = jax.random.split(key)\njax.jit(model.simulate)(key, ())\n\n# Computing density of joint\njax.jit(model.assess)(C[\"xy\"].set((2.0, 2.0)) | C[\"z\"].set(2.0), ())\n</pre> # Create a particular instance of the distribution norm_inv_gamma = NormalInverseGamma()   @gen def model():     (x, y) = norm_inv_gamma(0.0, 1.0, 1.0, 1.0) @ \"xy\"     z = normal(x, y) @ \"z\"     return z   # Sampling from the model key, subkey = jax.random.split(key) jax.jit(model.simulate)(key, ())  # Computing density of joint jax.jit(model.assess)(C[\"xy\"].set((2.0, 2.0)) | C[\"z\"].set(2.0), ()) Out[4]: <p>The second way is to create a distribution via the <code>Distribution</code> class. Here, the <code>logpdf</code> method is replace by the more general <code>estimate_logpdf</code> method. The distribution is asked to return an unbiased density estimate of its logpdf at the provided value. The <code>sample</code> method is replaced by <code>random_weighted</code>. It returns a sample from the distribution as well as an unbiased estimate of the reciprocal density, i.e. an estimate of $\\frac{1}{p(x)}$. Here we'll create a simple mixture of Gaussians.</p> In\u00a0[5]: Copied! <pre>@Pytree.dataclass\nclass GaussianMixture(Distribution):\n    # It can have static args\n    bias: float = Pytree.static(default=0.0)\n\n    # For distributions that can compute their densities exactly, `random_weighted` should return a sample x and the reciprocal density 1/p(x).\n    def random_weighted(self, key: PRNGKey, probs, means, vars) -&gt; tuple[Weight, any]:\n        # making sure that the inputs are jnp arrays for jax compatibility\n        probs = jnp.asarray(probs)\n        means = jnp.asarray(means)\n        vars = jnp.asarray(vars)\n\n        # sampling from the categorical distribution and then sampling from the normal distribution\n        cat = tfd.Categorical(probs=probs)\n        cat_index = jnp.asarray(cat.sample(seed=key))\n        normal = tfd.Normal(\n            loc=means[cat_index] + jnp.asarray(self.bias), scale=vars[cat_index]\n        )\n        key, subkey = jax.random.split(key)\n        normal_sample = normal.sample(seed=subkey)\n\n        # calculating the reciprocal density\n        zipped = jnp.stack([probs, means, vars], axis=1)\n        weight_recip = -jnp.log(\n            sum(\n                jax.vmap(\n                    lambda z: tfd.Normal(\n                        loc=z[1] + jnp.asarray(self.bias), scale=z[2]\n                    ).prob(normal_sample)\n                    * tfd.Categorical(probs=probs).prob(z[0])\n                )(zipped)\n            )\n        )\n\n        return weight_recip, normal_sample\n\n    # For distributions that can compute their densities exactly, `estimate_logpdf` should return the log density at x.\n    def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:\n        zipped = jnp.stack([probs, means, vars], axis=1)\n        return jnp.log(\n            sum(\n                jax.vmap(\n                    lambda z: tfd.Normal(\n                        loc=z[1] + jnp.asarray(self.bias), scale=z[2]\n                    ).prob(x)\n                    * tfd.Categorical(probs=probs).prob(z[0])\n                )(zipped)\n            )\n        )\n</pre> @Pytree.dataclass class GaussianMixture(Distribution):     # It can have static args     bias: float = Pytree.static(default=0.0)      # For distributions that can compute their densities exactly, `random_weighted` should return a sample x and the reciprocal density 1/p(x).     def random_weighted(self, key: PRNGKey, probs, means, vars) -&gt; tuple[Weight, any]:         # making sure that the inputs are jnp arrays for jax compatibility         probs = jnp.asarray(probs)         means = jnp.asarray(means)         vars = jnp.asarray(vars)          # sampling from the categorical distribution and then sampling from the normal distribution         cat = tfd.Categorical(probs=probs)         cat_index = jnp.asarray(cat.sample(seed=key))         normal = tfd.Normal(             loc=means[cat_index] + jnp.asarray(self.bias), scale=vars[cat_index]         )         key, subkey = jax.random.split(key)         normal_sample = normal.sample(seed=subkey)          # calculating the reciprocal density         zipped = jnp.stack([probs, means, vars], axis=1)         weight_recip = -jnp.log(             sum(                 jax.vmap(                     lambda z: tfd.Normal(                         loc=z[1] + jnp.asarray(self.bias), scale=z[2]                     ).prob(normal_sample)                     * tfd.Categorical(probs=probs).prob(z[0])                 )(zipped)             )         )          return weight_recip, normal_sample      # For distributions that can compute their densities exactly, `estimate_logpdf` should return the log density at x.     def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:         zipped = jnp.stack([probs, means, vars], axis=1)         return jnp.log(             sum(                 jax.vmap(                     lambda z: tfd.Normal(                         loc=z[1] + jnp.asarray(self.bias), scale=z[2]                     ).prob(x)                     * tfd.Categorical(probs=probs).prob(z[0])                 )(zipped)             )         ) <p>Testing:</p> In\u00a0[6]: Copied! <pre>gauss_mix = GaussianMixture(0.0)\n\n\n@gen\ndef model(probs):\n    mix1 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix1\"\n    mix2 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix2\"\n    return mix1, mix2\n\n\nprobs = jnp.array([0.5, 0.5])\n\n# Sampling from the model\nkey, subkey = jax.random.split(key)\njax.jit(model.simulate)(subkey, (probs,))\n\n# Computing density of joint\nkey, subkey = jax.random.split(key)\njax.jit(model.importance)(subkey, C[\"mix1\"].set(3.0) | C[\"mix2\"].set(4.0), (probs,))\n</pre> gauss_mix = GaussianMixture(0.0)   @gen def model(probs):     mix1 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix1\"     mix2 = gauss_mix(probs, jnp.array([0.0, 1.0]), jnp.array([1.0, 1.0])) @ \"mix2\"     return mix1, mix2   probs = jnp.array([0.5, 0.5])  # Sampling from the model key, subkey = jax.random.split(key) jax.jit(model.simulate)(subkey, (probs,))  # Computing density of joint key, subkey = jax.random.split(key) jax.jit(model.importance)(subkey, C[\"mix1\"].set(3.0) | C[\"mix2\"].set(4.0), (probs,)) Out[6]:"},{"location":"cookbook/inactive/expressivity/custom_distribution.html#how-do-i-create-a-custom-distribution-in-genjax","title":"How do I create a custom distribution in GenJAX? \u00b6","text":""},{"location":"cookbook/inactive/expressivity/masking.html","title":"Masking","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nfrom PIL import Image\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import bernoulli, categorical, gen, normal, or_else, pretty\n\npretty()\nkey = jax.random.key(0)\n</pre> import jax import jax.numpy as jnp from PIL import Image  import genjax from genjax import ChoiceMapBuilder as C from genjax import bernoulli, categorical, gen, normal, or_else, pretty  pretty() key = jax.random.key(0) <p>One classic trick is to encode all the options as an array and pick the desired value from the array with a dynamic one.</p> <p>Here's a first example:</p> In\u00a0[3]: Copied! <pre>@gen\ndef model(\n    i, means, vars\n):  # provide all the possible values and the dynamic index to pick from them\n    x = normal(means[i], vars[i]) @ \"x\"\n    return x\n\n\nkey, subkey = jax.random.split(key)\nmodel.simulate(subkey, (7, jnp.arange(10, dtype=jnp.float32), jnp.ones(10)))\n</pre> @gen def model(     i, means, vars ):  # provide all the possible values and the dynamic index to pick from them     x = normal(means[i], vars[i]) @ \"x\"     return x   key, subkey = jax.random.split(key) model.simulate(subkey, (7, jnp.arange(10, dtype=jnp.float32), jnp.ones(10))) Out[3]: <p>Now, what if there's a value we may or may not want to get depending on a dynamic value?</p> <p>In this case, we can use masking. Let's look at an example in JAX.</p> In\u00a0[4]: Copied! <pre>non_masked = jnp.arange(9).reshape(3, 3)\n\nnon_masked\n</pre> non_masked = jnp.arange(9).reshape(3, 3)  non_masked Out[4]: In\u00a0[5]: Copied! <pre># mask the upper triangular part of the matrix\nmask = jnp.mask_indices(3, jnp.triu)\n\nnon_masked[mask]\n</pre> # mask the upper triangular part of the matrix mask = jnp.mask_indices(3, jnp.triu)  non_masked[mask] Out[5]: <p>We can use similar logic for generative functions in GenJAX.</p> <p>Let's create an HMM using the scan combinator.</p> In\u00a0[6]: Copied! <pre>state_size = 10\nlength = 10\nvariance = jnp.eye(state_size)\nkey, subkey = jax.random.split(key)\ninitial_state = jax.random.normal(subkey, (state_size,))\n\n\n@genjax.gen\ndef hmm_step(x):\n    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n    return new_x\n\n\nhmm = hmm_step.iterate_final(n=length)\n</pre> state_size = 10 length = 10 variance = jnp.eye(state_size) key, subkey = jax.random.split(key) initial_state = jax.random.normal(subkey, (state_size,))   @genjax.gen def hmm_step(x):     new_x = genjax.mv_normal(x, variance) @ \"new_x\"     return new_x   hmm = hmm_step.iterate_final(n=length) <p>When we run it, we get a full trace.</p> In\u00a0[7]: Copied! <pre>jitted = jax.jit(hmm.simulate)\nkey, subkey = jax.random.split(key)\ntrace = jitted(subkey, (initial_state,))\ntrace.get_choices()\n</pre> jitted = jax.jit(hmm.simulate) key, subkey = jax.random.split(key) trace = jitted(subkey, (initial_state,)) trace.get_choices() Out[7]: <p>To get the partial results in the HMM instead, we can use the masked version of <code>iterate_final</code> as follows:</p> In\u00a0[8]: Copied! <pre>stop_at_index = 5\npairs = jnp.arange(state_size) &lt; stop_at_index\nmasked_hmm = hmm_step.masked_iterate_final()\nkey, subkey = jax.random.split(key)\nchoices = masked_hmm.simulate(subkey, (initial_state, pairs)).get_choices()\nchoices\n</pre> stop_at_index = 5 pairs = jnp.arange(state_size) &lt; stop_at_index masked_hmm = hmm_step.masked_iterate_final() key, subkey = jax.random.split(key) choices = masked_hmm.simulate(subkey, (initial_state, pairs)).get_choices() choices Out[8]: <p>We see that we obtain a filtered choice map, with a selection representing the boolean mask array. Within the filtered choice map, we have a static choice map where all the results are computed, without the mask applied to them. This is generally what will happen behind the scene in GenJAX: results will tend to be computed and then ignored, which is often more efficient on the GPU rather than being too eager in trying to avoid to do computations in the first place.</p> <p>Let's now use it in a bigger computation where the masking index is dynamic and comes from a sampled value.</p> In\u00a0[9]: Copied! <pre>@gen\ndef larger_model(init, probs):\n    i = categorical(probs) @ \"i\"\n    mask = jnp.arange(10) &lt; i\n    x = masked_hmm(init, mask) @ \"x\"\n    return x\n\n\nkey, subkey = jax.random.split(key)\ninit = jax.random.normal(subkey, (state_size,))\nprobs = jnp.arange(state_size) / sum(jnp.arange(state_size))\nkey, subkey = jax.random.split(key)\nchoices = larger_model.simulate(subkey, (init, probs)).get_choices()\nchoices\n</pre> @gen def larger_model(init, probs):     i = categorical(probs) @ \"i\"     mask = jnp.arange(10) &lt; i     x = masked_hmm(init, mask) @ \"x\"     return x   key, subkey = jax.random.split(key) init = jax.random.normal(subkey, (state_size,)) probs = jnp.arange(state_size) / sum(jnp.arange(state_size)) key, subkey = jax.random.split(key) choices = larger_model.simulate(subkey, (init, probs)).get_choices() choices Out[9]: <p>We have already seen how to use conditionals in GenJAX models in the <code>conditionals</code> notebook. Behind the scene, it's using the same logic with masks.</p> In\u00a0[10]: Copied! <pre>@gen\ndef cond_model(p):\n    pred = p &gt; 0\n    arg_1 = (p,)\n    arg_2 = (p,)\n    v = (\n        or_else(\n            gen(lambda p: bernoulli(p) @ \"v1\"), gen(lambda p: bernoulli(-p) @ \"v1\")\n        )(pred, arg_1, arg_2)\n        @ \"cond\"\n    )\n    return v\n\n\nkey, subkey = jax.random.split(key)\nchoices = cond_model.simulate(subkey, (0.5,)).get_choices()\nchoices\n</pre> @gen def cond_model(p):     pred = p &gt; 0     arg_1 = (p,)     arg_2 = (p,)     v = (         or_else(             gen(lambda p: bernoulli(p) @ \"v1\"), gen(lambda p: bernoulli(-p) @ \"v1\")         )(pred, arg_1, arg_2)         @ \"cond\"     )     return v   key, subkey = jax.random.split(key) choices = cond_model.simulate(subkey, (0.5,)).get_choices() choices Out[10]: <p>We see that both branches will get evaluated and a mask will be applied to each branch, whose value depends on the evaluation of the boolean predicate <code>pred</code>.</p> <p>What's happening behind the scene for masked values in the trace? Simply put, even though the system computes values, they are ignored w.r.t. the math of inference.</p> <p>We can check it on a simple example, with two versions of a model, where one has an extra masked variable <code>y</code>. Let's first define the two versions of the model.</p> In\u00a0[11]: Copied! <pre>@gen\ndef simple_model():\n    x = normal(0.0, 1.0) @ \"x\"\n    return x\n\n\n@gen\ndef submodel():\n    y = normal(0.0, 1.0) @ \"y\"\n    return y\n\n\n@gen\ndef model_with_mask():\n    x = normal(0.0, 1.0) @ \"x\"\n    _ = submodel.mask()(False) @ \"y\"\n    return x\n\n\n@gen\ndef proposal(_: genjax.Target):\n    x = normal(3.0, 1.0) @ \"x\"\n    return x\n</pre> @gen def simple_model():     x = normal(0.0, 1.0) @ \"x\"     return x   @gen def submodel():     y = normal(0.0, 1.0) @ \"y\"     return y   @gen def model_with_mask():     x = normal(0.0, 1.0) @ \"x\"     _ = submodel.mask()(False) @ \"y\"     return x   @gen def proposal(_: genjax.Target):     x = normal(3.0, 1.0) @ \"x\"     return x <p>Let's now test that on the same key, they return the exact same score:</p> In\u00a0[12]: Copied! <pre>key, subkey = jax.random.split(key)\n\nsimple_target = genjax.Target(simple_model, (), C.n())\nmasked_target = genjax.Target(model_with_mask, (), C.n())\nsimple_alg = genjax.smc.Importance(simple_target, q=proposal.marginal())\nmasked_alg = genjax.smc.Importance(masked_target, q=proposal.marginal())\n\n# TODO: something's fishy here with the math. Get the same whether I mask or not.\nsimple_alg.simulate(subkey, (simple_target,)).get_score() == masked_alg.simulate(\n    subkey, (masked_target,)\n).get_score()\n\nmasked_alg.simulate(subkey, (masked_target,))\n</pre> key, subkey = jax.random.split(key)  simple_target = genjax.Target(simple_model, (), C.n()) masked_target = genjax.Target(model_with_mask, (), C.n()) simple_alg = genjax.smc.Importance(simple_target, q=proposal.marginal()) masked_alg = genjax.smc.Importance(masked_target, q=proposal.marginal())  # TODO: something's fishy here with the math. Get the same whether I mask or not. simple_alg.simulate(subkey, (simple_target,)).get_score() == masked_alg.simulate(     subkey, (masked_target,) ).get_score()  masked_alg.simulate(subkey, (masked_target,)) Out[12]: <p>Let's see a final example for an unknown number of objects that may evolve over time. For this, we can use <code>vmap</code> over a masked object andd we get to choose which ones are masked or not.</p> <p>Let's create a model consisting of a 2D image where each pixel is traced.</p> In\u00a0[13]: Copied! <pre>@gen\ndef single_pixel():\n    pixel = normal(0.0, 1.0) @ \"pixel\"\n    return pixel\n\n\nimage_model = single_pixel.mask().vmap(in_axes=(0,)).vmap(in_axes=(0,))\n</pre> @gen def single_pixel():     pixel = normal(0.0, 1.0) @ \"pixel\"     return pixel   image_model = single_pixel.mask().vmap(in_axes=(0,)).vmap(in_axes=(0,)) <p>Let's create a circular mask around the image.</p> In\u00a0[14]: Copied! <pre>import matplotlib.animation as animation\nimport matplotlib.pyplot as plt\n\n\ndef create_circle_mask(size=200, center=None, radius=80):\n    if center is None:\n        center = (size // 2, size // 2)\n\n    y, x = jnp.ogrid[:size, :size]\n    dist_from_center = jnp.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)\n\n    mask = dist_from_center &lt;= radius\n    return mask\n\n\ncircle_mask = create_circle_mask()\n\nplt.imshow(circle_mask, cmap=\"gray\")\nplt.show()\n</pre> import matplotlib.animation as animation import matplotlib.pyplot as plt   def create_circle_mask(size=200, center=None, radius=80):     if center is None:         center = (size // 2, size // 2)      y, x = jnp.ogrid[:size, :size]     dist_from_center = jnp.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)      mask = dist_from_center &lt;= radius     return mask   circle_mask = create_circle_mask()  plt.imshow(circle_mask, cmap=\"gray\") plt.show() <p>Let's now sample from the masked image and play with the mask and inference.</p> In\u00a0[15]: Copied! <pre>key, subkey = jax.random.split(key)\n\ntr = image_model.simulate(subkey, (circle_mask,))\nflag = tr.get_choices()[:, :, \"pixel\"].flag\nim = flag * tr.get_choices()[:, :, \"pixel\"].value\n\nplt.imshow(im, cmap=\"gray\", vmin=0, vmax=1)\nplt.show()\n</pre> key, subkey = jax.random.split(key)  tr = image_model.simulate(subkey, (circle_mask,)) flag = tr.get_choices()[:, :, \"pixel\"].flag im = flag * tr.get_choices()[:, :, \"pixel\"].value  plt.imshow(im, cmap=\"gray\", vmin=0, vmax=1) plt.show() <p>We can create a small animation by updating the mask over time using the GenJAX <code>update</code> function to ensure that the probabilistic parts are taken properly into account.</p> In\u00a0[16]: Copied! <pre>number_iter = 10\nfig, ax = plt.subplots()\n\n# Load the image\nimage_path = \"./ending_dynamic_computation.png\"  # Update with your image path\nimage = Image.open(image_path)\n\n# Convert to grayscale if needed and resize to match new_im dimensions\nimage = image.convert(\"L\")  # Convert to grayscale\nimage = image.resize(im.shape[1::-1])  # Resize to match (height, width)\n\n# Convert to NumPy array\nimage_array = jnp.array(image) / 255.0\n\nimages = []\nfor i in range(number_iter):\n    key, subkey = jax.random.split(key)\n    new_circle_mask = create_circle_mask(radius=10 * i)\n    arg_diff = (genjax.Diff(new_circle_mask, genjax.UnknownChange),)\n    constraints = C.n()\n    update_problem = genjax.Update(constraints)\n    tr, _, _, _ = tr.edit(key, update_problem, arg_diff)\n    flag = tr.get_choices()[:, :, \"pixel\"].flag\n    new_im = flag * (tr.get_choices()[:, :, \"pixel\"].value / 5.0 + image_array)\n    images.append([ax.imshow(new_im, cmap=\"gray\", vmin=0, vmax=1, animated=True)])\n\nani = animation.ArtistAnimation(fig, images, interval=200, blit=True, repeat_delay=1000)\n\n# Save the animation as a GIF\nani.save(\"masked_image_animation.gif\", writer=\"pillow\")\n\n# Display the animation in the notebook\nfrom IPython.display import HTML\n\nHTML(ani.to_jshtml())\n</pre> number_iter = 10 fig, ax = plt.subplots()  # Load the image image_path = \"./ending_dynamic_computation.png\"  # Update with your image path image = Image.open(image_path)  # Convert to grayscale if needed and resize to match new_im dimensions image = image.convert(\"L\")  # Convert to grayscale image = image.resize(im.shape[1::-1])  # Resize to match (height, width)  # Convert to NumPy array image_array = jnp.array(image) / 255.0  images = [] for i in range(number_iter):     key, subkey = jax.random.split(key)     new_circle_mask = create_circle_mask(radius=10 * i)     arg_diff = (genjax.Diff(new_circle_mask, genjax.UnknownChange),)     constraints = C.n()     update_problem = genjax.Update(constraints)     tr, _, _, _ = tr.edit(key, update_problem, arg_diff)     flag = tr.get_choices()[:, :, \"pixel\"].flag     new_im = flag * (tr.get_choices()[:, :, \"pixel\"].value / 5.0 + image_array)     images.append([ax.imshow(new_im, cmap=\"gray\", vmin=0, vmax=1, animated=True)])  ani = animation.ArtistAnimation(fig, images, interval=200, blit=True, repeat_delay=1000)  # Save the animation as a GIF ani.save(\"masked_image_animation.gif\", writer=\"pillow\")  # Display the animation in the notebook from IPython.display import HTML  HTML(ani.to_jshtml()) Out[16]: Once Loop Reflect"},{"location":"cookbook/inactive/expressivity/masking.html#i-want-more-dynamic-features-but-jax-only-accepts-arrays-with-statically-known-sizes-what-do-i-do","title":"I want more dynamic features but JAX only accepts arrays with statically known sizes, what do I do? \u00b6","text":""},{"location":"cookbook/inactive/expressivity/mixture.html","title":"Mixture","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>from jax import random\n\nfrom genjax import flip, gen, inverse_gamma, mix, normal\n\nkey = random.key(0)\n</pre> from jax import random  from genjax import flip, gen, inverse_gamma, mix, normal  key = random.key(0) <p>We simply use the <code>mix</code> combinator. Note that the trace is the join of the traces of the different components.</p> <p>We first define the three components of the mixture model as generative functions.</p> In\u00a0[3]: Copied! <pre>@gen\ndef mixture_component_1(p):\n    x = normal(p, 1.0) @ \"x\"\n    return x\n\n\n@gen\ndef mixture_component_2(p):\n    b = flip(p) @ \"b\"\n    return b\n\n\n@gen\ndef mixture_component_3(p):\n    y = inverse_gamma(p, 0.1) @ \"y\"\n    return y\n</pre> @gen def mixture_component_1(p):     x = normal(p, 1.0) @ \"x\"     return x   @gen def mixture_component_2(p):     b = flip(p) @ \"b\"     return b   @gen def mixture_component_3(p):     y = inverse_gamma(p, 0.1) @ \"y\"     return y <p>The mix combinator take as input the logits of the mixture components, and args for each component of the mixture.</p> In\u00a0[4]: Copied! <pre>@gen\ndef mixture_model(p):\n    z = normal(p, 1.0) @ \"z\"\n    logits = (0.3, 0.5, 0.2)\n    arg_1 = (p,)\n    arg_2 = (p,)\n    arg_3 = (p,)\n    a = (\n        mix(mixture_component_1, mixture_component_2, mixture_component_3)(\n            logits, arg_1, arg_2, arg_3\n        )\n        @ \"a\"\n    )\n    return a + z\n\n\nkey, subkey = random.split(key)\ntr = mixture_model.simulate(subkey, (0.4,))\nprint(\"return value:\", tr.get_retval())\nprint(\"value for z:\", tr.get_choices()[\"z\"])\n</pre> @gen def mixture_model(p):     z = normal(p, 1.0) @ \"z\"     logits = (0.3, 0.5, 0.2)     arg_1 = (p,)     arg_2 = (p,)     arg_3 = (p,)     a = (         mix(mixture_component_1, mixture_component_2, mixture_component_3)(             logits, arg_1, arg_2, arg_3         )         @ \"a\"     )     return a + z   key, subkey = random.split(key) tr = mixture_model.simulate(subkey, (0.4,)) print(\"return value:\", tr.get_retval()) print(\"value for z:\", tr.get_choices()[\"z\"]) <pre>return value: 0.06437492\nvalue for z: -0.9356251\n</pre> <p>The combinator uses a fix address \"mixture_component\" for the components of the mixture model.</p> In\u00a0[5]: Copied! <pre>print(\"value for the mixture_component:\", tr.get_choices()[\"a\", \"mixture_component\"])\n</pre> print(\"value for the mixture_component:\", tr.get_choices()[\"a\", \"mixture_component\"]) <pre>value for the mixture_component: 1\n</pre>"},{"location":"cookbook/inactive/expressivity/mixture.html#how-can-i-write-a-mixture-of-models-in-genjax","title":"How can I write a mixture of models in GenJAX? \u00b6","text":""},{"location":"cookbook/inactive/expressivity/stochastic_probabilities.html","title":"Stochastic probabilities","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>This notebook builds on top of the <code>custom_distribution</code> one.</p> In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nfrom tensorflow_probability.substrates import jax as tfp\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import Pytree, Weight, pretty\nfrom genjax._src.generative_functions.distributions.distribution import Distribution\nfrom genjax.typing import Any\n\ntfd = tfp.distributions\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp from tensorflow_probability.substrates import jax as tfp  import genjax from genjax import ChoiceMapBuilder as C from genjax import Pytree, Weight, pretty from genjax._src.generative_functions.distributions.distribution import Distribution from genjax.typing import Any  tfd = tfp.distributions key = jax.random.key(0) pretty() <p>Recall how we defined a distribution for a Gaussian mixture, using the <code>Distribution</code> class.</p> In\u00a0[3]: Copied! <pre>@Pytree.dataclass\nclass GaussianMixture(Distribution):\n    def random_weighted(\n        self, key: jax.random.key, probs, means, vars\n    ) -&gt; tuple[Weight, Any]:\n        probs = jnp.asarray(probs)\n        means = jnp.asarray(means)\n        vars = jnp.asarray(vars)\n        cat = tfd.Categorical(probs=probs)\n        cat_index = jnp.asarray(cat.sample(seed=key))\n        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n        key, subkey = jax.random.split(key)\n        normal_sample = normal.sample(seed=subkey)\n        zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)\n        weight_recip = -jax.scipy.special.logsumexp(\n            jax.vmap(\n                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(normal_sample)\n                + tfd.Categorical(probs=probs).log_prob(z[0])\n            )(zipped)\n        )\n\n        return weight_recip, normal_sample\n\n    def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:\n        zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)\n        return jax.scipy.special.logsumexp(\n            jax.vmap(\n                lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(x)\n                + tfd.Categorical(probs=probs).log_prob(z[0])\n            )(zipped)\n        )\n</pre> @Pytree.dataclass class GaussianMixture(Distribution):     def random_weighted(         self, key: jax.random.key, probs, means, vars     ) -&gt; tuple[Weight, Any]:         probs = jnp.asarray(probs)         means = jnp.asarray(means)         vars = jnp.asarray(vars)         cat = tfd.Categorical(probs=probs)         cat_index = jnp.asarray(cat.sample(seed=key))         normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])         key, subkey = jax.random.split(key)         normal_sample = normal.sample(seed=subkey)         zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)         weight_recip = -jax.scipy.special.logsumexp(             jax.vmap(                 lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(normal_sample)                 + tfd.Categorical(probs=probs).log_prob(z[0])             )(zipped)         )          return weight_recip, normal_sample      def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:         zipped = jnp.stack([jnp.arange(0, len(probs)), means, vars], axis=1)         return jax.scipy.special.logsumexp(             jax.vmap(                 lambda z: tfd.Normal(loc=z[1], scale=z[2]).log_prob(x)                 + tfd.Categorical(probs=probs).log_prob(z[0])             )(zipped)         ) <p>In the class above, note in <code>estimate_logpdf</code> how we computed the density as a sum over all possible paths in the that could lead to a particular outcome <code>x</code>.</p> <p>In fact, the same occurs in <code>random_weighted</code>: even though we know exactly the path we took to get to the sample <code>normal_sample</code>, when evaluating the reciprocal density, we also sum over all possible paths that could lead to that <code>value</code>.</p> <p>Precisely, this required to sum over all the possible values of the categorical distribution <code>cat</code>. We technically sampled two random values <code>cat_index</code> and <code>normal_sample</code>, but we are only interested in the distribution on <code>normal_sample</code>: we marginalized out the intermediate random variable <code>cat_index</code>.</p> <p>Mathematically, we have <code>p(normal_sample) = sum_{cat_index} p(normal_sample, cat_index)</code>.</p> <p>GenJAX supports a more general kind of distribution, that only need to be able to estimate their densities. The correctness criterion for this to be valid are that the estimation should be unbiased, i.e. the correct value on average.</p> <p>More precisely,  <code>estimate_logpdf</code> should return an unbiased density estimate, while <code>random_weighted</code> should return an unbiased estimate for the reciprocal density. In general you can't get one from the other, as the following example shows.</p> <p>Flip a coin and with $50%$ chance return $1$, otherwise $3$. This gives an unbiased estimator of $2$. If we now return $\\frac{1}{1}$ with 50%, and $\\frac{1}{3}$ otherwise, the average value is $\\frac{2}{3}$, which is not $\\frac{1}{2}$.</p> <p>Let's now define a Gaussian mixture distribution that only estimates its density.</p> In\u00a0[4]: Copied! <pre>@Pytree.dataclass\nclass StochasticGaussianMixture(Distribution):\n    def random_weighted(\n        self, key: jax.random.key, probs, means, vars\n    ) -&gt; tuple[Weight, Any]:\n        probs = jnp.asarray(probs)\n        means = jnp.asarray(means)\n        vars = jnp.asarray(vars)\n        cat = tfd.Categorical(probs=probs)\n        cat_index = jnp.asarray(cat.sample(seed=key))\n        normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])\n        key, subkey = jax.random.split(key)\n        normal_sample = normal.sample(seed=subkey)\n        # We can estimate the reciprocal (marginal) density in constant time. Math magic explained at the end!\n        weight_recip = -tfd.Normal(\n            loc=means[cat_index], scale=vars[cat_index]\n        ).log_prob(normal_sample)\n        return weight_recip, normal_sample\n\n    # Given a sample `x`, we can also estimate the density in constant time\n    # Math again explained at the end.\n    # TODO: we could probably improve further with a better proposal\n    def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:\n        cat = tfd.Categorical(probs=probs)\n        cat_index = jnp.asarray(cat.sample(seed=key))\n        return tfd.Normal(loc=means[cat_index], scale=vars[cat_index]).log_prob(x)\n</pre> @Pytree.dataclass class StochasticGaussianMixture(Distribution):     def random_weighted(         self, key: jax.random.key, probs, means, vars     ) -&gt; tuple[Weight, Any]:         probs = jnp.asarray(probs)         means = jnp.asarray(means)         vars = jnp.asarray(vars)         cat = tfd.Categorical(probs=probs)         cat_index = jnp.asarray(cat.sample(seed=key))         normal = tfd.Normal(loc=means[cat_index], scale=vars[cat_index])         key, subkey = jax.random.split(key)         normal_sample = normal.sample(seed=subkey)         # We can estimate the reciprocal (marginal) density in constant time. Math magic explained at the end!         weight_recip = -tfd.Normal(             loc=means[cat_index], scale=vars[cat_index]         ).log_prob(normal_sample)         return weight_recip, normal_sample      # Given a sample `x`, we can also estimate the density in constant time     # Math again explained at the end.     # TODO: we could probably improve further with a better proposal     def estimate_logpdf(self, key: jax.random.key, x, probs, means, vars) -&gt; Weight:         cat = tfd.Categorical(probs=probs)         cat_index = jnp.asarray(cat.sample(seed=key))         return tfd.Normal(loc=means[cat_index], scale=vars[cat_index]).log_prob(x) <p>To test, we start by creating a generative function using our new distribution.</p> In\u00a0[5]: Copied! <pre>sgm = StochasticGaussianMixture()\n\n\n@genjax.gen\ndef model(cat_probs, means, vars):\n    x = sgm(cat_probs, means, vars) @ \"x\"\n    y_means = jnp.repeat(x, len(means))\n    y = sgm(cat_probs, y_means, vars) @ \"y\"\n    return (x, y)\n</pre> sgm = StochasticGaussianMixture()   @genjax.gen def model(cat_probs, means, vars):     x = sgm(cat_probs, means, vars) @ \"x\"     y_means = jnp.repeat(x, len(means))     y = sgm(cat_probs, y_means, vars) @ \"y\"     return (x, y) <p>We can then simulate from the model, assess a trace, or use importance sampling with the default proposal, seemlessly.</p> In\u00a0[6]: Copied! <pre>cat_probs = jnp.array([0.1, 0.4, 0.2, 0.3])\nmeans = jnp.array([0.0, 1.0, 2.0, 3.0])\nvars = jnp.array([1.0, 1.0, 1.0, 1.0])\n\nkey, subkey = jax.random.split(key)\ntr = model.simulate(subkey, (cat_probs, means, vars))\ntr\n</pre> cat_probs = jnp.array([0.1, 0.4, 0.2, 0.3]) means = jnp.array([0.0, 1.0, 2.0, 3.0]) vars = jnp.array([1.0, 1.0, 1.0, 1.0])  key, subkey = jax.random.split(key) tr = model.simulate(subkey, (cat_probs, means, vars)) tr Out[6]: In\u00a0[7]: Copied! <pre># TODO: assess currently raises a not implemented error, but we can use importance with a full trace instead\n# model.assess(tr.get_choices(), (cat_probs, means, vars))\nkey, subkey = jax.random.split(key)\n_, w = model.importance(subkey, tr.get_choices(), (cat_probs, means, vars))\nw\n</pre> # TODO: assess currently raises a not implemented error, but we can use importance with a full trace instead # model.assess(tr.get_choices(), (cat_probs, means, vars)) key, subkey = jax.random.split(key) _, w = model.importance(subkey, tr.get_choices(), (cat_probs, means, vars)) w Out[7]: In\u00a0[8]: Copied! <pre>y = 2.0\nkey, subkey = jax.random.split(key)\nmodel.importance(subkey, C[\"y\"].set(y), (cat_probs, means, vars))\n</pre> y = 2.0 key, subkey = jax.random.split(key) model.importance(subkey, C[\"y\"].set(y), (cat_probs, means, vars)) Out[8]: <p>Let's also check that <code>estimate_logpdf</code> from our distribution <code>sgm</code> indeed correctly estimates the density.</p> In\u00a0[9]: Copied! <pre>gm = GaussianMixture()\nx = 2.0\nN = 42\nn_estimates = 2000000\ncat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\ncat_probs = cat_probs / jnp.sum(cat_probs)\nmeans = jnp.arange(0.0, N * 1.0, 1.0)\nvars = jnp.ones(N) / N\nkey, subkey = jax.random.split(key)\nlog_density = gm.estimate_logpdf(subkey, x, cat_probs, means, vars)  # exact value\nlog_density\njitted = jax.jit(jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None)))\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, n_estimates)\nestimates = jitted(keys, x, cat_probs, means, vars)\nlog_mean_estimates = jax.scipy.special.logsumexp(estimates) - jnp.log(len(estimates))\nlog_density, log_mean_estimates\n</pre> gm = GaussianMixture() x = 2.0 N = 42 n_estimates = 2000000 cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N)) cat_probs = cat_probs / jnp.sum(cat_probs) means = jnp.arange(0.0, N * 1.0, 1.0) vars = jnp.ones(N) / N key, subkey = jax.random.split(key) log_density = gm.estimate_logpdf(subkey, x, cat_probs, means, vars)  # exact value log_density jitted = jax.jit(jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None))) key, subkey = jax.random.split(key) keys = jax.random.split(subkey, n_estimates) estimates = jitted(keys, x, cat_probs, means, vars) log_mean_estimates = jax.scipy.special.logsumexp(estimates) - jnp.log(len(estimates)) log_density, log_mean_estimates Out[9]: <p>One benefit of using density estimates instead of exact ones is that it can be much faster to compute. Here's a way to test it, though it will not shine on this example as it is too simple. We will explore examples in different notebooks where this shines more brightly.</p> In\u00a0[10]: Copied! <pre>N = 30000\nn_estimates = 10\ncat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N))\ncat_probs = cat_probs / jnp.sum(cat_probs)\nmeans = jnp.arange(0.0, N * 1.0, 1.0)\nvars = jnp.ones(N) / N\n\njitted_exact = jax.jit(gm.estimate_logpdf)\njitted_approx = jax.jit(\n    lambda key, x, cat_probs, means, vars: jax.scipy.special.logsumexp(\n        jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None))(\n            key, x, cat_probs, means, vars\n        )\n    )\n    - jnp.log(n_estimates)\n)\n\n# warmup the jit\nkey, subkey = jax.random.split(key)\njitted_exact(subkey, x, cat_probs, means, vars)\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, n_estimates)\njitted_approx(keys, x, cat_probs, means, vars)\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, n_estimates)\n%timeit jitted(keys, x, cat_probs, means, vars)\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, n_estimates)\n%timeit jitted_approx(keys, x, cat_probs, means, vars)\n</pre> N = 30000 n_estimates = 10 cat_probs = jnp.array(jnp.arange(1.0 / N, 1.0 + 1.0 / N, 1.0 / N)) cat_probs = cat_probs / jnp.sum(cat_probs) means = jnp.arange(0.0, N * 1.0, 1.0) vars = jnp.ones(N) / N  jitted_exact = jax.jit(gm.estimate_logpdf) jitted_approx = jax.jit(     lambda key, x, cat_probs, means, vars: jax.scipy.special.logsumexp(         jax.vmap(sgm.estimate_logpdf, in_axes=(0, None, None, None, None))(             key, x, cat_probs, means, vars         )     )     - jnp.log(n_estimates) )  # warmup the jit key, subkey = jax.random.split(key) jitted_exact(subkey, x, cat_probs, means, vars) key, subkey = jax.random.split(key) keys = jax.random.split(subkey, n_estimates) jitted_approx(keys, x, cat_probs, means, vars) key, subkey = jax.random.split(key) keys = jax.random.split(subkey, n_estimates) %timeit jitted(keys, x, cat_probs, means, vars) key, subkey = jax.random.split(key) keys = jax.random.split(subkey, n_estimates) %timeit jitted_approx(keys, x, cat_probs, means, vars) <pre>2.94 ms \u00b1 36.5 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <pre>2.95 ms \u00b1 43.6 \u03bcs per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>Now, the reason we need both methods <code>random_weighted</code> and <code>estimate_logpdf</code> is that both methods will be used at different times, notably depending on whether we use the distribution in a proposal or in a model, as we show next.</p> <p>Let's define a simple model and a proposal which both use our <code>sgm</code> distribution.</p> In\u00a0[11]: Copied! <pre>@genjax.gen\ndef model(cat_probs, means, vars):\n    x = sgm(cat_probs, means, vars) @ \"x\"\n    y_means = jnp.repeat(x, len(means))\n    y = sgm(cat_probs, y_means, vars) @ \"y\"\n    return (x, y)\n\n\n@genjax.gen\ndef proposal(obs, cat_probs, means, vars):\n    y = obs[\"y\"]\n    # simple logic to propose a new x: its mean was presumably closer to y\n    new_means = jax.vmap(lambda m: (m + y) / 2)(means)\n    x = sgm(cat_probs, new_means, vars) @ \"x\"\n    return (x, y)\n</pre> @genjax.gen def model(cat_probs, means, vars):     x = sgm(cat_probs, means, vars) @ \"x\"     y_means = jnp.repeat(x, len(means))     y = sgm(cat_probs, y_means, vars) @ \"y\"     return (x, y)   @genjax.gen def proposal(obs, cat_probs, means, vars):     y = obs[\"y\"]     # simple logic to propose a new x: its mean was presumably closer to y     new_means = jax.vmap(lambda m: (m + y) / 2)(means)     x = sgm(cat_probs, new_means, vars) @ \"x\"     return (x, y) <p>Let's define importance sampling once again. Note that it is exactly the same as the usual one!</p> <p>This is because behind the scenes GenJAX implements <code>simulate</code> using <code>random_weighted</code> and <code>assess</code> using <code>estimate_logpdf</code>.</p> In\u00a0[12]: Copied! <pre>def gensp_importance_sampling(target, obs, proposal):\n    def _inner(key, target_args, proposal_args):\n        key, subkey = jax.random.split(key)\n        trace = proposal.simulate(key, *proposal_args)\n        chm = obs ^ trace.get_choices()\n        proposal_logpdf = trace.get_score()\n        # TODO: using importance instead of assess, as assess is not implemented\n        _, target_logpdf = target.importance(subkey, chm, *target_args)\n        importance_weight = target_logpdf - proposal_logpdf\n        return (trace, importance_weight)\n\n    return _inner\n</pre> def gensp_importance_sampling(target, obs, proposal):     def _inner(key, target_args, proposal_args):         key, subkey = jax.random.split(key)         trace = proposal.simulate(key, *proposal_args)         chm = obs ^ trace.get_choices()         proposal_logpdf = trace.get_score()         # TODO: using importance instead of assess, as assess is not implemented         _, target_logpdf = target.importance(subkey, chm, *target_args)         importance_weight = target_logpdf - proposal_logpdf         return (trace, importance_weight)      return _inner <p>Testing</p> In\u00a0[13]: Copied! <pre>obs = C[\"y\"].set(2.0)\n\nkey, subkey = jax.random.split(key)\ngensp_importance_sampling(model, obs, proposal)(\n    subkey, ((cat_probs, means, vars),), ((obs, cat_probs, means, vars),)\n)\n</pre> obs = C[\"y\"].set(2.0)  key, subkey = jax.random.split(key) gensp_importance_sampling(model, obs, proposal)(     subkey, ((cat_probs, means, vars),), ((obs, cat_probs, means, vars),) ) <pre>/tmp/ipykernel_7749/1815923397.py:5: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = obs ^ trace.get_choices()\n</pre> Out[13]: <p>Finally, for those curious about the math magic that enabled to correctly (meaning unbiasedly) estimate the pdf and its reciprocal, there's a follow up cookbook on this!</p>"},{"location":"cookbook/inactive/expressivity/stochastic_probabilities.html#how-to-create-and-use-distributions-with-inexact-likelihood-evaluations","title":"How to create and use distributions with inexact likelihood evaluations \u00b6","text":""},{"location":"cookbook/inactive/expressivity/stochastic_probabilities_math.html","title":"Stochastic probabilities math","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>Let's start with <code>estimate_logpdf</code>. We have that the marginal distribution over the returned value <code>x</code> (the sample from the normal distribution) is given by $$p(x) = \\sum_i p(x\\mid z=i) p(z=i)$$ where the sum is over the possible values of the categorical distribution, $p(x|z=i)$  is the density of the $i$-th normal at $x$, and $p(z=i)$ is the density of the categorical at the value $i$.</p> <p>This sum can be rewritten as the expectation under the categorical distribution $p(z)$: $$\\sum_i p(x\\mid z=i)p(z=i) = \\mathbb{E}_{z\\sim p(z)}[p(x\\mid z)]$$ This means we can get an unbiased estimate of the expectation by simply sampling a <code>z</code> and returning <code>p(x|z)</code>: the average value of this process is obviously its expectation (it's the definition on the expectation). In other words, we proved that the estimation strategy used in <code>estimate_logpdf</code> indeed returns an unbiased estimate of the exact marginal.</p> <p>Lastly, as we discussed above we cannot in general invert an unbiased estimate to get an unbiased estimate of the reciprocal, so one may be suspicious that the returned weight in <code>random_weighted</code> looks like the negation (in logspace) of the one returned in <code>estimate_logpdf</code>. Here the argument is different, based on the following identity: $$\\frac{1}{p(x)} = \\mathbb{E}_{z\\sim p(z\\mid x)}[\\frac{1}{p(x\\mid z)}]$$ The idea is that we can get an unbiased estimate if we can sample from the posterior $p(z|x)$. Given an $x$, this is an intractable sampling problem in general. However, in <code>random_weighted</code>, we sample a $z$ together with the $x$, and this $z$ is an exact posterior sample of $z$ that we get \"for free\". Now to finish the explanation, the compact way to prove the identity is as follows.</p> <p>$$ \\begin{matrix} \\frac{1}{p(x)} &amp;\\\\ = \\frac{1}{p(x)} \\mathbb{E}_{z \\sim B}[p(z)] &amp; \\text{$p(z)$ density w.r.t. base measure $B$ and of total mass 1}\\\\ = \\frac{1}{p(x)} \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)}]   &amp;\\text{seeing $p(z|x)$ as an importance sampler for $B$}\\\\ = \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z\\mid x)p(x)}]  &amp; \\text{$p(x)$ doesn't depend on $z$ moved within the expectation}\\\\ = \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z,x)}]   &amp; \\text{ definition of joint distribution}\\\\ = \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{p(z)}{p(z)p(x|z)}] &amp; \\text{definition of conditional distribution}\\\\ =  \\mathbb{E}_{z \\sim p(z\\mid x)}[\\frac{1}{p(x|z)}]   &amp; \\text{simplification} \\end{matrix} $$</p>"},{"location":"cookbook/inactive/expressivity/stochastic_probabilities_math.html#details-on-random_weighted-and-estimate_logpdf","title":"Details on random_weighted and estimate_logpdf \u00b6","text":""},{"location":"cookbook/inactive/inference/custom_proposal.html","title":"Custom proposal","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>One thing one can do is write a custom proposal for importance sampling. The idea is to sample from this one instead of the default one used by genjax when using <code>model.importance</code>. The default one is only informed by the structure of the model, and not by the posterior defined by both the model and the observations.</p> In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nfrom jax import jit, vmap\nfrom jax.scipy.special import logsumexp\n\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import Target, gen, normal, pretty, smc\n\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp from jax import jit, vmap from jax.scipy.special import logsumexp  from genjax import ChoiceMapBuilder as C from genjax import Target, gen, normal, pretty, smc  key = jax.random.key(0) pretty() <p>Let's first define a simple model with a broad normal prior and some observations</p> In\u00a0[3]: Copied! <pre>@gen\ndef model():\n    # Initially, the prior is a pretty broad normal distribution centred at 0\n    x = normal(0.0, 100.0) @ \"x\"\n    # We add some observations, which will shift the posterior towards these values\n    _ = normal(x, 1.0) @ \"obs1\"\n    _ = normal(x, 1.0) @ \"obs2\"\n    _ = normal(x, 1.0) @ \"obs3\"\n    return x\n\n\n# We create some data, 3 observed values at 234\nobs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)\n</pre> @gen def model():     # Initially, the prior is a pretty broad normal distribution centred at 0     x = normal(0.0, 100.0) @ \"x\"     # We add some observations, which will shift the posterior towards these values     _ = normal(x, 1.0) @ \"obs1\"     _ = normal(x, 1.0) @ \"obs2\"     _ = normal(x, 1.0) @ \"obs3\"     return x   # We create some data, 3 observed values at 234 obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0) <pre>/tmp/ipykernel_7867/323051816.py:13: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)\n/tmp/ipykernel_7867/323051816.py:13: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)\n</pre> <p>We then run importance sampling with a default proposal, snd print the average weight of the samples, to give us a sense of how well the proposal is doing.</p> In\u00a0[4]: Copied! <pre>key, *sub_keys = jax.random.split(key, 1000 + 1)\nsub_keys = jnp.array(sub_keys)\nargs = ()\njitted = jit(vmap(model.importance, in_axes=(0, None, None)))\ntrace, weight = jitted(sub_keys, obs, args)\nprint(\"The average weight is\", logsumexp(weight) - jnp.log(len(weight)))\nprint(\"The maximum weight is\", weight.max())\n</pre> key, *sub_keys = jax.random.split(key, 1000 + 1) sub_keys = jnp.array(sub_keys) args = () jitted = jit(vmap(model.importance, in_axes=(0, None, None))) trace, weight = jitted(sub_keys, obs, args) print(\"The average weight is\", logsumexp(weight) - jnp.log(len(weight))) print(\"The maximum weight is\", weight.max()) <pre>The average weight is -9.859367\nThe maximum weight is -2.951612\n</pre> <p>We can see that both the average and even maximum weight are quite low, which means that the proposal is not doing a great job.</p> <p>If there is no observations, ideally, the weight should center around 1 and be quite concentrated around that value. A weight much higher than 1 means that the proposal is too narrow and is missing modes. Indeed, for that to happen, one has to sample a very unlikely value under the proposal which is very likely under the target. A weight much lower than 1 means that the proposal is too broad and is wasting samples. This happens in this case as the default proposal uses the broad prior <code>normal(0.0, 100.0)</code> as a proposal, which is far from the observed values centred around $234.0$.</p> <p>If there are observations, as is the case above, the weight should center around the marginal on the observations. More precisely, if the model has density $p(x,y)$ where $y$ are the observations and the proposal has density $q(x)$, then a weight is given by $w = \\frac{p(x,y)}{q(x)}$ whose average value over many runs (expectations under the proposal) is $p(y)$.</p> <p>We now define a custom proposal, which will be a normal distribution centred around the observed values</p> In\u00a0[5]: Copied! <pre>@gen\ndef proposal(obs):\n    avg_val = jnp.array(obs).mean()\n    std = jnp.array(obs).std()\n    x = (\n        normal(avg_val, 0.1 + std) @ \"x\"\n    )  # To avoid a degenerate proposal, we add a small value to the standard deviation\n    return x\n</pre> @gen def proposal(obs):     avg_val = jnp.array(obs).mean()     std = jnp.array(obs).std()     x = (         normal(avg_val, 0.1 + std) @ \"x\"     )  # To avoid a degenerate proposal, we add a small value to the standard deviation     return x <p>To do things by hand first, let's reimplement the importance function. It samples from the proposal and then computes the importance weight</p> In\u00a0[6]: Copied! <pre>def importance_sample(target, obs, proposal):\n    def _inner(key, target_args, proposal_args):\n        trace = proposal.simulate(key, *proposal_args)\n        # the full choice map under which we evaluate the model\n        # has the sampled values from the proposal and the observed values\n        chm = obs ^ trace.get_choices()\n        proposal_logpdf = trace.get_score()\n        target_logpdf, _ = target.assess(chm, *target_args)\n        importance_weight = target_logpdf - proposal_logpdf\n        return (trace, importance_weight)\n\n    return _inner\n</pre> def importance_sample(target, obs, proposal):     def _inner(key, target_args, proposal_args):         trace = proposal.simulate(key, *proposal_args)         # the full choice map under which we evaluate the model         # has the sampled values from the proposal and the observed values         chm = obs ^ trace.get_choices()         proposal_logpdf = trace.get_score()         target_logpdf, _ = target.assess(chm, *target_args)         importance_weight = target_logpdf - proposal_logpdf         return (trace, importance_weight)      return _inner <p>We then run importance sampling with the custom proposal</p> In\u00a0[7]: Copied! <pre>key, *sub_keys = jax.random.split(key, 1000 + 1)\nsub_keys = jnp.array(sub_keys)\nargs_for_model = ()\nargs_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\njitted = jit(vmap(importance_sample(model, obs, proposal), in_axes=(0, None, None)))\ntrace, new_weight = jitted(sub_keys, (args_for_model,), (args_for_proposal,))\n</pre> key, *sub_keys = jax.random.split(key, 1000 + 1) sub_keys = jnp.array(sub_keys) args_for_model = () args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),) jitted = jit(vmap(importance_sample(model, obs, proposal), in_axes=(0, None, None))) trace, new_weight = jitted(sub_keys, (args_for_model,), (args_for_proposal,)) <pre>/tmp/ipykernel_7867/2378245163.py:6: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  chm = obs ^ trace.get_choices()\n</pre> <p>We see that the new values, both average and maximum, are much higher than before, which means that the custom proposal is doing a much better job</p> In\u00a0[8]: Copied! <pre>print(\"The new average weight is\", logsumexp(new_weight) - jnp.log(len(new_weight)))\nprint(\"The new maximum weight is\", new_weight.max())\n</pre> print(\"The new average weight is\", logsumexp(new_weight) - jnp.log(len(new_weight))) print(\"The new maximum weight is\", new_weight.max()) <pre>The new average weight is -11.4855\nThe new maximum weight is -7.8311706\n</pre> <p>We can also do the same using the library functions.</p> <p>To do this, let's first create a target posterior distribution. It consists of the model, arguments for it, and observations.</p> In\u00a0[9]: Copied! <pre>target_posterior = Target(model, args_for_model, obs)\n</pre> target_posterior = Target(model, args_for_model, obs) <p>Next, we redefine the proposal slightly to take the target as argument. This way, it can extract the observations fro the target as we previously used. But the target can for instance also depend on the arguments passed to the model.</p> In\u00a0[10]: Copied! <pre>@gen\ndef proposal(target: Target):\n    model_obs = target.constraint\n    used_obs = jnp.array([model_obs[\"obs1\"], model_obs[\"obs2\"], model_obs[\"obs3\"]])\n    avg_val = jnp.array(used_obs).mean()\n    std = jnp.array(used_obs).std()\n    x = normal(avg_val, 0.1 + std) @ \"x\"\n    return x\n</pre> @gen def proposal(target: Target):     model_obs = target.constraint     used_obs = jnp.array([model_obs[\"obs1\"], model_obs[\"obs2\"], model_obs[\"obs3\"]])     avg_val = jnp.array(used_obs).mean()     std = jnp.array(used_obs).std()     x = normal(avg_val, 0.1 + std) @ \"x\"     return x <p>Now, similarly to the importance_sampling notebook, we create an instance algorithm: it specifies a strategy to approximate our posterior of interest, <code>target_posterior</code>, using importance sampling with <code>k_particles</code>, and our custom proposal.</p> <p>To specify that we use all the traced variables from <code>proposal</code> in importance sampling (we will revisit why that may not be the case in the ravi_stack notebook) are to be used, we will use <code>proposal.marginal()</code>. This indicates that no traced variable from <code>proposal</code> is marginalized out.</p> In\u00a0[11]: Copied! <pre>k_particles = 1000\nalg = smc.ImportanceK(target_posterior, q=proposal.marginal(), k_particles=k_particles)\n</pre> k_particles = 1000 alg = smc.ImportanceK(target_posterior, q=proposal.marginal(), k_particles=k_particles) <p>This will perform sampling importance resampling (SIR) with a $1000$ intermediate particles and one resampled and returned at the end which is returned. Testing</p> In\u00a0[12]: Copied! <pre>jitted = jit(alg.simulate)\nkey, subkey = jax.random.split(key)\nposterior_samples = jitted(subkey, (target_posterior,))\nposterior_samples\n</pre> jitted = jit(alg.simulate) key, subkey = jax.random.split(key) posterior_samples = jitted(subkey, (target_posterior,)) posterior_samples Out[12]:"},{"location":"cookbook/inactive/inference/custom_proposal.html#im-doing-importance-sampling-as-advised-but-its-bad-what-can-i-do","title":"I'm doing importance sampling as advised but it's bad, what can I do? \u00b6","text":""},{"location":"cookbook/inactive/inference/importance_sampling.html","title":"Importance sampling","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>We will do it with importance sampling, which works as follows. We choose a distribution $q$ called a proposal that you we will sample from, and we need a distribution $p$ of interest, typically representing a posterior from a model having received observations.</p> In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport jax.tree_util as jtu\nfrom jax import jit, vmap\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import Target, bernoulli, beta, gen, pretty, smc\n\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp import jax.tree_util as jtu from jax import jit, vmap  import genjax from genjax import ChoiceMapBuilder as C from genjax import Target, bernoulli, beta, gen, pretty, smc  key = jax.random.key(0) pretty() <p>Let's first look at a simple python version of the algorithm to get the idea.</p> In\u00a0[3]: Copied! <pre>def importance_sample(model, proposal):\n    def _inner(key, model_args, proposal_args):\n        # we sample from the easy distribution, the proposal `q`\n        trace = proposal.simulate(key, *proposal_args)\n        chm = trace.get_choices()\n        # we evaluate the score of the easy distribution q(x)\n        proposal_logpdf = trace.get_score()\n        # we evaluate the score of the hard distribution p(x)\n        model_logpdf, _ = model.assess(chm, *model_args)\n        # the importance weight is p(x)/q(x), which corrects for the bias from sampling from q instead of p\n        importance_weight = model_logpdf - proposal_logpdf\n        return (trace, importance_weight)\n        # we return the trace and the importance weight\n\n    return _inner\n</pre> def importance_sample(model, proposal):     def _inner(key, model_args, proposal_args):         # we sample from the easy distribution, the proposal `q`         trace = proposal.simulate(key, *proposal_args)         chm = trace.get_choices()         # we evaluate the score of the easy distribution q(x)         proposal_logpdf = trace.get_score()         # we evaluate the score of the hard distribution p(x)         model_logpdf, _ = model.assess(chm, *model_args)         # the importance weight is p(x)/q(x), which corrects for the bias from sampling from q instead of p         importance_weight = model_logpdf - proposal_logpdf         return (trace, importance_weight)         # we return the trace and the importance weight      return _inner <p>We can test this on a very simple example.</p> In\u00a0[4]: Copied! <pre>model = genjax.normal\nproposal = genjax.normal\n\nmodel_args = (0.0, 1.0)\nproposal_args = (3.0, 4.0)\n\nkey, subkey = jax.random.split(key)\nsample, importance_weight = jit(importance_sample(model, proposal))(\n    subkey, (model_args,), (proposal_args,)\n)\nprint(importance_weight, sample.get_choices())\n</pre> model = genjax.normal proposal = genjax.normal  model_args = (0.0, 1.0) proposal_args = (3.0, 4.0)  key, subkey = jax.random.split(key) sample, importance_weight = jit(importance_sample(model, proposal))(     subkey, (model_args,), (proposal_args,) ) print(importance_weight, sample.get_choices()) <pre>-18.546162 Choice(v=&lt;jax.Array(-6.769823, dtype=float32)&gt;)\n</pre> <p>We can also run it in parallel!</p> In\u00a0[5]: Copied! <pre>jitted = jit(\n    vmap(\n        importance_sample(model, proposal),\n        in_axes=(0, None, None),\n    )\n)\nkey, *sub_keys = jax.random.split(key, 100 + 1)\nsub_keys = jnp.array(sub_keys)\n(sample, importance_weight) = jitted(sub_keys, (model_args,), (proposal_args,))\nsample.get_choices(), importance_weight\n</pre> jitted = jit(     vmap(         importance_sample(model, proposal),         in_axes=(0, None, None),     ) ) key, *sub_keys = jax.random.split(key, 100 + 1) sub_keys = jnp.array(sub_keys) (sample, importance_weight) = jitted(sub_keys, (model_args,), (proposal_args,)) sample.get_choices(), importance_weight Out[5]: <p>In GenJAX, every generative function comes equipped with a default proposal which we can use for importance sampling.</p> <p>Let's define a generative function.</p> In\u00a0[6]: Copied! <pre>@gen\ndef beta_bernoulli_process(u):\n    p = beta(1.0, u) @ \"p\"\n    v = bernoulli(p) @ \"v\"\n    return v\n</pre> @gen def beta_bernoulli_process(u):     p = beta(1.0, u) @ \"p\"     v = bernoulli(p) @ \"v\"     return v <p>By giving constraints to some of the random samples, which we call observations, we obtain a posterior inference problem where the goal is to infer the distribution of the random variables which are not observed.</p> In\u00a0[7]: Copied! <pre>obs = C[\"v\"].set(1)\nargs = (0.5,)\n</pre> obs = C[\"v\"].set(1) args = (0.5,) <p>The method <code>.importance</code> defines a default proposal based on the generative function which targets the posterior distribution we just defined. It returns a pair containing a trace and the log incremental weight. This weight corrects for the bias from sampling from the proposal instead of the intractable posterior distribution.</p> In\u00a0[8]: Copied! <pre>key, subkey = jax.random.split(key)\ntrace, weight = beta_bernoulli_process.importance(subkey, obs, args)\n\ntrace, weight\n</pre> key, subkey = jax.random.split(key) trace, weight = beta_bernoulli_process.importance(subkey, obs, args)  trace, weight Out[8]: In\u00a0[9]: Copied! <pre>N = 1000\nK = 100\n\n\ndef SIR(N, K, model, chm):\n    @jit\n    def _inner(key, args):\n        key, subkey = jax.random.split(key)\n        traces, weights = vmap(model.importance, in_axes=(0, None, None))(\n            jax.random.split(key, N), chm, args\n        )\n        idxs = vmap(jax.jit(genjax.categorical.simulate), in_axes=(0, None))(\n            jax.random.split(subkey, K), (weights,)\n        ).get_retval()\n        samples = traces.get_choices()\n        resampled_samples = vmap(lambda idx: jtu.tree_map(lambda v: v[idx], samples))(\n            idxs\n        )\n        return resampled_samples\n\n    return _inner\n</pre> N = 1000 K = 100   def SIR(N, K, model, chm):     @jit     def _inner(key, args):         key, subkey = jax.random.split(key)         traces, weights = vmap(model.importance, in_axes=(0, None, None))(             jax.random.split(key, N), chm, args         )         idxs = vmap(jax.jit(genjax.categorical.simulate), in_axes=(0, None))(             jax.random.split(subkey, K), (weights,)         ).get_retval()         samples = traces.get_choices()         resampled_samples = vmap(lambda idx: jtu.tree_map(lambda v: v[idx], samples))(             idxs         )         return resampled_samples      return _inner <p>Testing</p> In\u00a0[10]: Copied! <pre>chm = C[\"v\"].set(1)\nargs = (0.5,)\nkey, subkey = jax.random.split(key)\nsamples = jit(SIR(N, K, beta_bernoulli_process, chm))(subkey, args)\nsamples\n</pre> chm = C[\"v\"].set(1) args = (0.5,) key, subkey = jax.random.split(key) samples = jit(SIR(N, K, beta_bernoulli_process, chm))(subkey, args) samples Out[10]: <p>Another way to do the basically the same thing using library functions.</p> <p>To do this, we first define a Target for importance sampling, i.e. the posterior inference problem we're targetting. It consists of a generative function, arguments to the generative function, and observations.</p> In\u00a0[11]: Copied! <pre>target_posterior = Target(beta_bernoulli_process, (args,), chm)\n</pre> target_posterior = Target(beta_bernoulli_process, (args,), chm) <p>Next, we define an inference strategy algorithm (Algorithm class) to use to approximate the target distribution.</p> <p>It's importance sampling with $N$ particles in our case.</p> In\u00a0[12]: Copied! <pre>alg = smc.ImportanceK(target_posterior, k_particles=N)\n</pre> alg = smc.ImportanceK(target_posterior, k_particles=N) <p>To get a different sense of what's going on, the hierarchy of classes is as follows:</p> <p><code>ImportanceK &lt;: SMCAlgorithm &lt;: Algorithm &lt;: SampleDistribution &lt;: Distribution &lt;: GenerativeFunction &lt;: Pytree</code></p> <p>In words, importance sampling (<code>ImportanceK</code>) is a particular instance of Sequential Monte Carlo ( <code>SMCAlgorithm</code>). The latter is one instance of approximate inference strategy (<code>Algorithm</code>). An inference strategy in particular produces samples for a distribution (<code>SampleDistribution</code>), which is a distribution (<code>Distribution</code>) whose return value is the sample. A distribution here is the definition from GenSP (Lew et al 2023) which has two methods <code>random_weighted</code> and <code>estimate_logpdf</code>. See the appropriate cookbook for details on these. Finally, a distribution is a particular case of generative function (<code>GenerativeFunction</code>), which are all pytrees (<code>Pytree</code>) to be JAX-compatible and in particular jittable.</p> <p>To get K independent samples from the approximate posterior distribution, we can for instance use <code>vmap</code>.</p> In\u00a0[13]: Copied! <pre># It's a bit different from the previous example, because each of the final\n# K samples is obtained by running a different set of N-particles.\n# This can of course be optimized but we keep it simple here.\njitted = jit(vmap(alg.simulate, in_axes=(0, None)))\n</pre> # It's a bit different from the previous example, because each of the final # K samples is obtained by running a different set of N-particles. # This can of course be optimized but we keep it simple here. jitted = jit(vmap(alg.simulate, in_axes=(0, None))) <p>Testing</p> In\u00a0[14]: Copied! <pre>key, *sub_keys = jax.random.split(key, K + 1)\nsub_keys = jnp.array(sub_keys)\nposterior_samples = jitted(sub_keys, (target_posterior,)).get_retval()\n\n# This only does the importance sampling step, not the resampling step\n# Therefore the shape is (K, N, 1)\nposterior_samples[\"p\"]\n</pre> key, *sub_keys = jax.random.split(key, K + 1) sub_keys = jnp.array(sub_keys) posterior_samples = jitted(sub_keys, (target_posterior,)).get_retval()  # This only does the importance sampling step, not the resampling step # Therefore the shape is (K, N, 1) posterior_samples[\"p\"] Out[14]: <p>We can check the mean value estimate for <code>\"p\"</code>.</p> In\u00a0[15]: Copied! <pre>posterior_samples[\"p\"].mean(axis=(0, 1))\n</pre> posterior_samples[\"p\"].mean(axis=(0, 1)) Out[15]: <p>And we compare the relative difference with the one obtained using the previous method.</p> In\u00a0[16]: Copied! <pre>100.0 * jnp.abs(\n    samples[\"p\"].mean() - posterior_samples[\"p\"].mean(axis=(0, 1))\n) / posterior_samples[\"p\"].mean(axis=(0, 1))  # about 2% difference\n</pre> 100.0 * jnp.abs(     samples[\"p\"].mean() - posterior_samples[\"p\"].mean(axis=(0, 1)) ) / posterior_samples[\"p\"].mean(axis=(0, 1))  # about 2% difference Out[16]:"},{"location":"cookbook/inactive/inference/importance_sampling.html#i-want-to-do-my-first-inference-task-how-do-i-do-it","title":"I want to do my first inference task, how do I do it? \u00b6","text":""},{"location":"cookbook/inactive/inference/mapping_tutorial.html","title":"Mapping tutorial","text":"<p>The goal of this tutorial is \u2013 in a sense \u2013 the opposite of the localization tutorial. We consider the following scenario:</p> <ul> <li>We have a two-dimensional map consisting of walls and free space.</li> <li>A robot is located at the center of the map (so it knows its position).</li> <li>The robot is equipped with a sensor that can (noisily) measure the distance to the nearest wall in several directions.</li> <li>We want to infer the locations of the walls in the map.</li> </ul> <p>As a simplifying assumption, we assume that</p> <ul> <li>the map is a $21 \\times 21$ grid of unit blocks (\"pixels\") and</li> <li>each pixel is either a wall or a free space.</li> </ul> <p>This tutorial will teach you how to:</p> <ul> <li>model this scenario in GenJAX,</li> <li>perform Bayesian inference to infer the walls on the map from noisy sensor measurements:<ul> <li>first, via importance sampling (which turns out to perform poorly)</li> <li>then, via Gibbs sampling (which turns out to perform well),</li> </ul> </li> <li>using visualizations and interactivity in GenStudio.</li> </ul> <p>We will explore various variants of Gibbs sampling, so this can also be seen as a tutorial for Gibbs sampling.</p> <p>A fully interactive version of this tutorial can be found at the very end of this notebook.</p> <p>Potential future extensions:</p> <ul> <li>include observations from several points of view</li> <li>include uncertainty in the robot's position</li> <li>allow the robot to move around (including uncertainty in the motion)</li> </ul> In\u00a0[1]: Copied! <pre># Global setup code\n\nimport genstudio.plot as Plot\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrand\nfrom penzai import pz\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import pretty\nfrom genjax.typing import IntArray\n\npretty()\n\nPlot.configure({\"display_as\": \"html\", \"dev\": False})\n</pre> # Global setup code  import genstudio.plot as Plot import jax import jax.numpy as jnp import jax.random as jrand from penzai import pz  import genjax from genjax import ChoiceMapBuilder as C from genjax import pretty from genjax.typing import IntArray  pretty()  Plot.configure({\"display_as\": \"html\", \"dev\": False}) In\u00a0[2]: Copied! <pre>GRID = r\"\"\"\n.....................\n.....................\n.XXX..........X......\n.X.X..........X......\n.X............XXX....\n.X..............X....\n.X..............X....\n.XX............XXX...\n..X..............X...\n..X..............X...\n..X..............X...\n..X..............X...\n..XXXXX......XXXXX...\n......X......X.......\n......X....XXX.......\n......X....X.........\n......X....X.........\n......XXXXXX.........\n.....................\n.....................\n.....................\n\"\"\"\n\nGRID_SIZE = 21\nCENTER = jnp.array([GRID_SIZE // 2, GRID_SIZE // 2])\n\n\ndef extract_points_from_grid_string(grid_string):\n    \"\"\"Extracts the points from the grid string.\"\"\"\n    lines = grid_string.strip().split(\"\\n\")\n    height = len(lines)\n    width = len(lines[0])\n    walls = jnp.zeros((height, width))\n    for i, line in enumerate(lines):\n        for j, char in enumerate(line):\n            if char == \"X\":\n                walls = walls.at[i, j].set(1)\n    return walls\n\n\ntrue_walls = extract_points_from_grid_string(GRID)\ntrue_walls\n</pre> GRID = r\"\"\" ..................... ..................... .XXX..........X...... .X.X..........X...... .X............XXX.... .X..............X.... .X..............X.... .XX............XXX... ..X..............X... ..X..............X... ..X..............X... ..X..............X... ..XXXXX......XXXXX... ......X......X....... ......X....XXX....... ......X....X......... ......X....X......... ......XXXXXX......... ..................... ..................... ..................... \"\"\"  GRID_SIZE = 21 CENTER = jnp.array([GRID_SIZE // 2, GRID_SIZE // 2])   def extract_points_from_grid_string(grid_string):     \"\"\"Extracts the points from the grid string.\"\"\"     lines = grid_string.strip().split(\"\\n\")     height = len(lines)     width = len(lines[0])     walls = jnp.zeros((height, width))     for i, line in enumerate(lines):         for j, char in enumerate(line):             if char == \"X\":                 walls = walls.at[i, j].set(1)     return walls   true_walls = extract_points_from_grid_string(GRID) true_walls Out[2]: <p>Now, <code>true_walls</code> is a 21 x 21 array of 0s and 1s, where 1s indicate the presence of a wall.</p> In\u00a0[3]: Copied! <pre>js_code = Plot.Import(\n    source=\"\"\"\nexport const wallsToCenters = (walls) =&gt; {\n   // walls is a 2d array of 0s and 1s \n   // we want to return a list of [x1, y1, x2, y2] for each 1 in the array\n   // we can do this by iterating over the array and collecting the coordinates\n   // of each 1\n   const centers = [];\n   for (let i = 0; i &lt; walls.length; i++) {\n      for (let j = 0; j &lt; walls[i].length; j++) {\n        if (walls[i][j] &gt; 0) {\n            centers.push([i, j, walls[i][j]]);\n        }\n      }\n   }\n   return centers;\n}\n\"\"\",\n    refer=[\"wallsToCenters\"],\n)\n\nplot_walls = (\n    Plot.rect(\n        Plot.js(\"wallsToCenters($state.true_walls)\"),\n        x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),\n        x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),\n        y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),\n        y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),\n        stroke=Plot.constantly(\"ground truth\"),\n        strokeWidth=2,\n        fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"),\n    )\n    + Plot.domain([0, GRID_SIZE], [0, GRID_SIZE])\n    + Plot.aspectRatio(1)\n    + Plot.width(500)\n)\n\n\ndef make_plot(true_walls):\n    return (\n        js_code\n        &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True)\n        &amp; Plot.new(plot_walls, Plot.color_legend())\n    )\n</pre> js_code = Plot.Import(     source=\"\"\" export const wallsToCenters = (walls) =&gt; {    // walls is a 2d array of 0s and 1s     // we want to return a list of [x1, y1, x2, y2] for each 1 in the array    // we can do this by iterating over the array and collecting the coordinates    // of each 1    const centers = [];    for (let i = 0; i &lt; walls.length; i++) {       for (let j = 0; j &lt; walls[i].length; j++) {         if (walls[i][j] &gt; 0) {             centers.push([i, j, walls[i][j]]);         }       }    }    return centers; } \"\"\",     refer=[\"wallsToCenters\"], )  plot_walls = (     Plot.rect(         Plot.js(\"wallsToCenters($state.true_walls)\"),         x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),         x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),         y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),         y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),         stroke=Plot.constantly(\"ground truth\"),         strokeWidth=2,         fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"),     )     + Plot.domain([0, GRID_SIZE], [0, GRID_SIZE])     + Plot.aspectRatio(1)     + Plot.width(500) )   def make_plot(true_walls):     return (         js_code         &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True)         &amp; Plot.new(plot_walls, Plot.color_legend())     ) In\u00a0[4]: Copied! <pre>make_plot(true_walls)\n</pre> make_plot(true_walls) Out[4]: In\u00a0[5]: Copied! <pre>def on_click(widget, event):\n    x, y = round(event[\"x\"]), round(event[\"y\"])\n    true_walls = jnp.array(widget.state.true_walls)\n    true_walls = true_walls.at[x, y].set(1)\n    widget.state.update({\"true_walls\": true_walls})\n\n\nwalls = jnp.zeros((GRID_SIZE, GRID_SIZE))\n\ninteractive_walls = Plot.events(onClick=on_click, onDraw=on_click)\n\n\ndef make_plot(true_walls, extra_plot=None, interactive=False):\n    true_walls = true_walls.astype(jnp.float32)\n    map_plot = plot_walls + extra_plot\n    if interactive:\n        map_plot = map_plot + interactive_walls\n    plot = (\n        js_code &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True) &amp; map_plot\n    )\n    if interactive:\n        plot = plot | [\n            \"div.bg-blue-500.text-white.p-3.rounded-sm\",\n            {\n                \"onClick\": lambda widget, _event: widget.state.update({\n                    \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))\n                })\n            },\n            \"Clear walls\",\n        ]\n        plot = plot.display_as(\"widget\")\n    return plot\n\n\nmake_plot(true_walls, interactive=True)\n</pre> def on_click(widget, event):     x, y = round(event[\"x\"]), round(event[\"y\"])     true_walls = jnp.array(widget.state.true_walls)     true_walls = true_walls.at[x, y].set(1)     widget.state.update({\"true_walls\": true_walls})   walls = jnp.zeros((GRID_SIZE, GRID_SIZE))  interactive_walls = Plot.events(onClick=on_click, onDraw=on_click)   def make_plot(true_walls, extra_plot=None, interactive=False):     true_walls = true_walls.astype(jnp.float32)     map_plot = plot_walls + extra_plot     if interactive:         map_plot = map_plot + interactive_walls     plot = (         js_code &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True) &amp; map_plot     )     if interactive:         plot = plot | [             \"div.bg-blue-500.text-white.p-3.rounded-sm\",             {                 \"onClick\": lambda widget, _event: widget.state.update({                     \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))                 })             },             \"Clear walls\",         ]         plot = plot.display_as(\"widget\")     return plot   make_plot(true_walls, interactive=True) Out[5]: In\u00a0[6]: Copied! <pre>def walls_prior(prior_wall_prob):\n    return genjax.flip.repeat(n=GRID_SIZE).repeat(n=GRID_SIZE)(prior_wall_prob)\n</pre> def walls_prior(prior_wall_prob):     return genjax.flip.repeat(n=GRID_SIZE).repeat(n=GRID_SIZE)(prior_wall_prob) <p>In order to get a better understanding, let us sample from the prior. To do this, JAX requires a random seed, called \"key\", which we pass to the <code>.simulate</code> method of the generative function. The result is a trace, which contains the return value of the function, the score, and the choice (\"sampled values during execution\"). The score is the log probability of the choices.</p> In\u00a0[7]: Copied! <pre>key = jrand.key(0)\nsample_prior_jitted = jax.jit(walls_prior(prior_wall_prob=0.5).simulate)\ntr = sample_prior_jitted(key, ())\ndisplay(tr.get_score())\ndisplay(tr.get_choices())\ndisplay(tr.get_retval())\n</pre> key = jrand.key(0) sample_prior_jitted = jax.jit(walls_prior(prior_wall_prob=0.5).simulate) tr = sample_prior_jitted(key, ()) display(tr.get_score()) display(tr.get_choices()) display(tr.get_retval()) <p>Let's visualize the map of pixels that was sampled from the prior.</p> In\u00a0[8]: Copied! <pre>walls = tr.get_retval()\nmake_plot(walls)\n</pre> walls = tr.get_retval() make_plot(walls) Out[8]: <p>We can see that, in fact, about half the pixels are walls. So the prior seems to work as intended.</p> <p>POSSIBLE EXTENSION: These maps don't look like real-world maps. The prior could be refined in various ways, for instance \"Wall pixels\" are likely connected, so we could generate line segments in the prior rather than individual pixels.</p> In\u00a0[9]: Copied! <pre>OFFSETS = jnp.array([\n    [-0.5, -0.5],\n    [0.5, -0.5],\n    [0.5, 0.5],\n    [-0.5, 0.5],\n])\n\n\ndef line_segments_of_pixel(pixel: IntArray):\n    vertices = pixel[None, :] + OFFSETS\n    return jnp.stack([vertices, jnp.roll(vertices, 1, axis=0)], axis=1)\n\n\nline_segments = jax.vmap(line_segments_of_pixel, in_axes=0)\n\n\ndef solve_lines(p, u, q, v, PARALLEL_TOL=1.0e-10):\n    \"\"\"\n    Solves for the intersection of two lines defined by points and direction vectors.\n\n    Args:\n    - p, u: Point and direction vector defining the first line.\n    - q, v: Point and direction vector defining the second line.\n    - PARALLEL_TOL: Tolerance for determining if lines are parallel.\n\n    Returns:\n    - s, t: Parameters for the line equations at the intersection point.\n            Returns [-inf, -inf] if lines are parallel.\n    \"\"\"\n    det = u[0] * v[1] - u[1] * v[0]\n    return jnp.where(\n        jnp.abs(det) &lt; PARALLEL_TOL,\n        jnp.array([-jnp.inf, -jnp.inf]),\n        jnp.array([\n            (v[0] * (p[1] - q[1]) - v[1] * (p[0] - q[0])) / det,\n            (u[1] * (q[0] - p[0]) - u[0] * (q[1] - p[1])) / det,\n        ]),\n    )\n\n\ndef distance(pos, dir, seg):\n    \"\"\"\n    Computes the distance from `pos` to a segment `seg`, in a given direction `dir`.\n\n    Args:\n    - pos: The position: `[pos_x, pos_y]`.\n    - dir: The direction: `[dir_x, dir_y]`.\n    - seg: The Segment object: `[[start_x, start_y], [end_x, end_y]]`.\n\n    Returns:\n    - float: The distance to the segment. Returns infinity if no valid intersection is found.\n    \"\"\"\n    a = solve_lines(pos, dir, seg[0], seg[1] - seg[0])\n    return jnp.where(\n        (a[0] &gt;= 0.0) &amp; (a[1] &gt;= 0.0) &amp; (a[1] &lt;= 1.0),\n        a[0],\n        jnp.inf,\n    )\n\n\ndef unit_dir(angle):\n    \"\"\"Unit vector in the direction of `angle`.\"\"\"\n    return jnp.array([jnp.cos(angle), jnp.sin(angle)])\n</pre> OFFSETS = jnp.array([     [-0.5, -0.5],     [0.5, -0.5],     [0.5, 0.5],     [-0.5, 0.5], ])   def line_segments_of_pixel(pixel: IntArray):     vertices = pixel[None, :] + OFFSETS     return jnp.stack([vertices, jnp.roll(vertices, 1, axis=0)], axis=1)   line_segments = jax.vmap(line_segments_of_pixel, in_axes=0)   def solve_lines(p, u, q, v, PARALLEL_TOL=1.0e-10):     \"\"\"     Solves for the intersection of two lines defined by points and direction vectors.      Args:     - p, u: Point and direction vector defining the first line.     - q, v: Point and direction vector defining the second line.     - PARALLEL_TOL: Tolerance for determining if lines are parallel.      Returns:     - s, t: Parameters for the line equations at the intersection point.             Returns [-inf, -inf] if lines are parallel.     \"\"\"     det = u[0] * v[1] - u[1] * v[0]     return jnp.where(         jnp.abs(det) &lt; PARALLEL_TOL,         jnp.array([-jnp.inf, -jnp.inf]),         jnp.array([             (v[0] * (p[1] - q[1]) - v[1] * (p[0] - q[0])) / det,             (u[1] * (q[0] - p[0]) - u[0] * (q[1] - p[1])) / det,         ]),     )   def distance(pos, dir, seg):     \"\"\"     Computes the distance from `pos` to a segment `seg`, in a given direction `dir`.      Args:     - pos: The position: `[pos_x, pos_y]`.     - dir: The direction: `[dir_x, dir_y]`.     - seg: The Segment object: `[[start_x, start_y], [end_x, end_y]]`.      Returns:     - float: The distance to the segment. Returns infinity if no valid intersection is found.     \"\"\"     a = solve_lines(pos, dir, seg[0], seg[1] - seg[0])     return jnp.where(         (a[0] &gt;= 0.0) &amp; (a[1] &gt;= 0.0) &amp; (a[1] &lt;= 1.0),         a[0],         jnp.inf,     )   def unit_dir(angle):     \"\"\"Unit vector in the direction of `angle`.\"\"\"     return jnp.array([jnp.cos(angle), jnp.sin(angle)]) <p>The robot's sensor can measure the distance in <code>NUM_DIRECTIONS</code> equidistant directions.</p> <p>The following code computes the sensor readings. It makes use of the JAX functions <code>jax.lax.cond</code> (instead of <code>if</code>) and <code>jax.vmap</code> (instead of a for-loop) because we want JAX to JIT-compile and vectorize these operations.</p> In\u00a0[10]: Copied! <pre>def angles(num_angles):\n    return jnp.arange(0, 1, 1 / num_angles) * 2 * jnp.pi\n\n\nNUM_DIRECTIONS = 100\nMAX_DISTANCE = GRID_SIZE * 2  # cap on distances to avoid infinities\n\n\ndef distance_to_pixel(pos, dir, coord, wall):\n    \"\"\"Distance from the origin to the pixel at `coord` if the ray in direction `dir` hits the pixel and it is a wall.\"\"\"\n    segs = line_segments_of_pixel(coord)\n    dists = jax.vmap(lambda seg: distance(pos, dir, seg))(segs)\n    return jax.lax.cond(\n        wall &gt; 0,  # is there a wall?\n        lambda: jnp.min(dists, axis=0),\n        lambda: jnp.inf,\n    )\n\n\ndef distance_to_pixels(pos, dir, walls):\n    \"\"\"Distance from the origin to the nearest wall among `pixels` in direction `dir`.\n\n    The distance is capped at `MAX_DISTANCE` to avoid infinities in the calculations.\"\"\"\n    return jnp.minimum(\n        jnp.min(\n            jax.vmap(\n                lambda i, row: jax.vmap(\n                    lambda j, is_wall: distance_to_pixel(\n                        pos, dir, jnp.array([i, j]), is_wall\n                    )\n                )(jnp.arange(walls.shape[1]), row)\n            )(jnp.arange(walls.shape[0]), walls)\n        ),\n        MAX_DISTANCE,\n    )\n\n\ndef sensor_distances(pos, pixels, angles):\n    \"\"\"Sensor distances in all directions (as specified by `angles`) for the map given by `pixels`.\"\"\"\n    return jax.vmap(lambda angle: distance_to_pixels(pos, unit_dir(angle), pixels))(\n        angles\n    )\n</pre> def angles(num_angles):     return jnp.arange(0, 1, 1 / num_angles) * 2 * jnp.pi   NUM_DIRECTIONS = 100 MAX_DISTANCE = GRID_SIZE * 2  # cap on distances to avoid infinities   def distance_to_pixel(pos, dir, coord, wall):     \"\"\"Distance from the origin to the pixel at `coord` if the ray in direction `dir` hits the pixel and it is a wall.\"\"\"     segs = line_segments_of_pixel(coord)     dists = jax.vmap(lambda seg: distance(pos, dir, seg))(segs)     return jax.lax.cond(         wall &gt; 0,  # is there a wall?         lambda: jnp.min(dists, axis=0),         lambda: jnp.inf,     )   def distance_to_pixels(pos, dir, walls):     \"\"\"Distance from the origin to the nearest wall among `pixels` in direction `dir`.      The distance is capped at `MAX_DISTANCE` to avoid infinities in the calculations.\"\"\"     return jnp.minimum(         jnp.min(             jax.vmap(                 lambda i, row: jax.vmap(                     lambda j, is_wall: distance_to_pixel(                         pos, dir, jnp.array([i, j]), is_wall                     )                 )(jnp.arange(walls.shape[1]), row)             )(jnp.arange(walls.shape[0]), walls)         ),         MAX_DISTANCE,     )   def sensor_distances(pos, pixels, angles):     \"\"\"Sensor distances in all directions (as specified by `angles`) for the map given by `pixels`.\"\"\"     return jax.vmap(lambda angle: distance_to_pixels(pos, unit_dir(angle), pixels))(         angles     ) <p>As before, it is useful to have visualizations.</p> In\u00a0[11]: Copied! <pre>def plot_sensors(pos, readings, angles):\n    \"\"\"Plot the sensor readings.\"\"\"\n    unit_vecs = jax.vmap(unit_dir, in_axes=0)(angles)\n    ray_endpoints = unit_vecs * readings[:, None]\n    return (\n        [\n            Plot.line([pos, pos + endpoint], stroke=Plot.constantly(\"sensor rays\"))\n            for endpoint in ray_endpoints\n        ]\n        + [\n            Plot.ellipse(\n                [pos + endpoint], r=0.1, fill=Plot.constantly(\"sensor readings\")\n            )\n            for endpoint in ray_endpoints\n        ]\n        + Plot.ellipse([pos], r=0.2, fill=\"red\")\n    )\n\n\ntrue_readings = sensor_distances(CENTER, true_walls, angles(NUM_DIRECTIONS))\n\nmake_plot(true_walls, plot_sensors(CENTER, true_readings, angles(NUM_DIRECTIONS)))\n</pre> def plot_sensors(pos, readings, angles):     \"\"\"Plot the sensor readings.\"\"\"     unit_vecs = jax.vmap(unit_dir, in_axes=0)(angles)     ray_endpoints = unit_vecs * readings[:, None]     return (         [             Plot.line([pos, pos + endpoint], stroke=Plot.constantly(\"sensor rays\"))             for endpoint in ray_endpoints         ]         + [             Plot.ellipse(                 [pos + endpoint], r=0.1, fill=Plot.constantly(\"sensor readings\")             )             for endpoint in ray_endpoints         ]         + Plot.ellipse([pos], r=0.2, fill=\"red\")     )   true_readings = sensor_distances(CENTER, true_walls, angles(NUM_DIRECTIONS))  make_plot(true_walls, plot_sensors(CENTER, true_readings, angles(NUM_DIRECTIONS))) Out[11]: In\u00a0[12]: Copied! <pre>@genjax.gen\ndef sensor_model_single(pos, pixels, sensor_noise, angle):\n    exact_distance = distance_to_pixels(pos, unit_dir(angle), pixels)\n    return genjax.normal(exact_distance, sensor_noise) @ ()\n\n\nsensor_model = sensor_model_single.vmap(in_axes=(None, None, None, 0))\n</pre> @genjax.gen def sensor_model_single(pos, pixels, sensor_noise, angle):     exact_distance = distance_to_pixels(pos, unit_dir(angle), pixels)     return genjax.normal(exact_distance, sensor_noise) @ ()   sensor_model = sensor_model_single.vmap(in_axes=(None, None, None, 0)) <p>Let's sample from the sensor model to see what data the robot receives. We can see that the sensor readings are no longer exact, but contain quite a bit of noise. The noise level can be controlled by changing the <code>NOISE</code> variable above.</p> In\u00a0[13]: Copied! <pre>@pz.pytree_dataclass\nclass ModelParams(genjax.Pytree):\n    prior_wall_prob: float = genjax.Pytree.static()\n    sensor_noise: float = genjax.Pytree.static()\n    num_angles: int = genjax.Pytree.static()\n\n\nDEFAULT_PARAMS = ModelParams(\n    prior_wall_prob=0.5, sensor_noise=0.2, num_angles=NUM_DIRECTIONS\n)\n</pre> @pz.pytree_dataclass class ModelParams(genjax.Pytree):     prior_wall_prob: float = genjax.Pytree.static()     sensor_noise: float = genjax.Pytree.static()     num_angles: int = genjax.Pytree.static()   DEFAULT_PARAMS = ModelParams(     prior_wall_prob=0.5, sensor_noise=0.2, num_angles=NUM_DIRECTIONS ) In\u00a0[14]: Copied! <pre>key = jrand.key(0)\ntrace = sensor_model.simulate(\n    key, (CENTER, true_walls, DEFAULT_PARAMS.sensor_noise, angles(NUM_DIRECTIONS))\n)\nobserved_readings = trace.get_retval()\nmake_plot(true_walls, plot_sensors(CENTER, observed_readings, angles(NUM_DIRECTIONS)))\n</pre> key = jrand.key(0) trace = sensor_model.simulate(     key, (CENTER, true_walls, DEFAULT_PARAMS.sensor_noise, angles(NUM_DIRECTIONS)) ) observed_readings = trace.get_retval() make_plot(true_walls, plot_sensors(CENTER, observed_readings, angles(NUM_DIRECTIONS))) Out[14]: <p>To get an idea of what the robot sees, let's remove the walls. How can we infer the walls?</p> In\u00a0[15]: Copied! <pre>make_plot(\n    jnp.zeros((GRID_SIZE, GRID_SIZE)),\n    plot_sensors(CENTER, observed_readings, angles(NUM_DIRECTIONS)),\n)\n</pre> make_plot(     jnp.zeros((GRID_SIZE, GRID_SIZE)),     plot_sensors(CENTER, observed_readings, angles(NUM_DIRECTIONS)), ) Out[15]: In\u00a0[16]: Copied! <pre>@genjax.gen\ndef full_model(pos, model_params):\n    walls = walls_prior(model_params.prior_wall_prob) @ \"walls\"\n    readings = (\n        sensor_model(\n            pos, walls, model_params.sensor_noise, angles(model_params.num_angles)\n        )\n        @ \"readings\"\n    )\n    return (walls, readings)\n</pre> @genjax.gen def full_model(pos, model_params):     walls = walls_prior(model_params.prior_wall_prob) @ \"walls\"     readings = (         sensor_model(             pos, walls, model_params.sensor_noise, angles(model_params.num_angles)         )         @ \"readings\"     )     return (walls, readings) <p>This model samples a map from the prior and then samples sensor readings from the sensor model. The samples are given the addresses <code>\"walls\"</code> and <code>\"readings\"</code>. When inspecting the trace, we can see that the sampled values are stored under these addresses.</p> In\u00a0[17]: Copied! <pre>key = jrand.key(1)\ntrace = full_model.simulate(key, (CENTER, DEFAULT_PARAMS))\nwalls, readings = trace.get_retval()\nmake_plot(walls, plot_sensors(CENTER, readings, angles(NUM_DIRECTIONS)))\ntrace.get_choices()\n</pre> key = jrand.key(1) trace = full_model.simulate(key, (CENTER, DEFAULT_PARAMS)) walls, readings = trace.get_retval() make_plot(walls, plot_sensors(CENTER, readings, angles(NUM_DIRECTIONS))) trace.get_choices() Out[17]: <p>How can we infer the walls given some observed readings?</p> In\u00a0[18]: Copied! <pre>def importance_sampling(key, args, observed_readings, N=100):\n    \"\"\"Very naive MAP estimation. Just try N random samples and take the one with the highest weight.\"\"\"\n    model_importance = jax.jit(full_model.importance)\n    keys = jrand.split(key, N)\n    constraints = C[\"readings\"].set(observed_readings)\n    traces, log_weights = jax.vmap(\n        lambda key: model_importance(key, constraints, args)\n    )(keys)\n    log_weights = log_weights - jax.scipy.special.logsumexp(log_weights)\n    return traces, log_weights\n\n\nkey, subkey = jrand.split(jrand.key(0))\ntraces, log_weights = importance_sampling(\n    subkey, (CENTER, DEFAULT_PARAMS), observed_readings, N=100000\n)\nsubkeys = jrand.split(subkey, 100)\nresampled_indices = jax.vmap(lambda i: jrand.categorical(subkeys[i], log_weights))(\n    jnp.arange(100)\n)\nresampled_indices\n</pre> def importance_sampling(key, args, observed_readings, N=100):     \"\"\"Very naive MAP estimation. Just try N random samples and take the one with the highest weight.\"\"\"     model_importance = jax.jit(full_model.importance)     keys = jrand.split(key, N)     constraints = C[\"readings\"].set(observed_readings)     traces, log_weights = jax.vmap(         lambda key: model_importance(key, constraints, args)     )(keys)     log_weights = log_weights - jax.scipy.special.logsumexp(log_weights)     return traces, log_weights   key, subkey = jrand.split(jrand.key(0)) traces, log_weights = importance_sampling(     subkey, (CENTER, DEFAULT_PARAMS), observed_readings, N=100000 ) subkeys = jrand.split(subkey, 100) resampled_indices = jax.vmap(lambda i: jrand.categorical(subkeys[i], log_weights))(     jnp.arange(100) ) resampled_indices Out[18]: <p>If we look at the resampled indices, we see that they are all the same. What this means is that one trace had such a high weight that none of the other traces had a chance to be resampled. This makes sense, because it is extremely unlikely to guess a \"good\" map just by sampling from the prior. So one (bad) sample will get lucky and have a high weight by chance and be selected every time. We can confirm this by looking at a histogram of the weights.</p> In\u00a0[19]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.hist(log_weights, bins=100, range=(-10000, 0))\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.hist(log_weights, bins=100, range=(-10000, 0)) plt.show() <p>As we can see, the next best trace has a log weight that is orders of magnitude worse than the best sample.</p> <p>This is clearly bad. We need better inference methods.</p> In\u00a0[20]: Copied! <pre>full_model_assess = jax.jit(full_model.assess)\n\n\ndef gibbs_update_pixel(key, args, readings, walls, i, j):\n    is_wall_false = walls.at[i, j].set(0)\n    is_wall_true = walls.at[i, j].set(1)\n    chm_false = C[\"readings\"].set(readings) | C[\"walls\"].set(is_wall_false)\n    (false_weight, _) = full_model_assess(chm_false, args)\n    chm_true = C[\"readings\"].set(readings) | C[\"walls\"].set(is_wall_true)\n    (true_weight, _) = full_model_assess(chm_true, args)\n    # categorical automatically normalizes the weights\n    pixel_is_wall = genjax.categorical.sample(\n        key, jnp.array([false_weight, true_weight])\n    ).astype(jnp.float32)\n    return pixel_is_wall\n\n\ndef simple_gibbs_sweep(key, args, readings, walls):\n    subkeys = jrand.split(key, walls.shape)\n    walls = jax.lax.fori_loop(\n        0,\n        walls.shape[0],\n        lambda i, walls: jax.lax.fori_loop(\n            0,\n            walls.shape[1],\n            lambda j, walls: walls.at[i, j].set(\n                gibbs_update_pixel(subkeys[i, j], args, readings, walls, i, j)\n            ),\n            walls,\n        ),\n        walls,\n    )\n    return walls\n</pre> full_model_assess = jax.jit(full_model.assess)   def gibbs_update_pixel(key, args, readings, walls, i, j):     is_wall_false = walls.at[i, j].set(0)     is_wall_true = walls.at[i, j].set(1)     chm_false = C[\"readings\"].set(readings) | C[\"walls\"].set(is_wall_false)     (false_weight, _) = full_model_assess(chm_false, args)     chm_true = C[\"readings\"].set(readings) | C[\"walls\"].set(is_wall_true)     (true_weight, _) = full_model_assess(chm_true, args)     # categorical automatically normalizes the weights     pixel_is_wall = genjax.categorical.sample(         key, jnp.array([false_weight, true_weight])     ).astype(jnp.float32)     return pixel_is_wall   def simple_gibbs_sweep(key, args, readings, walls):     subkeys = jrand.split(key, walls.shape)     walls = jax.lax.fori_loop(         0,         walls.shape[0],         lambda i, walls: jax.lax.fori_loop(             0,             walls.shape[1],             lambda j, walls: walls.at[i, j].set(                 gibbs_update_pixel(subkeys[i, j], args, readings, walls, i, j)             ),             walls,         ),         walls,     )     return walls <p>Starting from any initial sample, applying Gibbs sweeps will (under certain conditions) yield approximate samples from the posterior distribution. Let's try this out.</p> In\u00a0[21]: Copied! <pre>def run_gibbs_chain(key, gibbs_update, args, readings, num_samples=100):\n    walls = jnp.zeros((GRID_SIZE, GRID_SIZE))\n    key, *subkeys = jrand.split(key, num_samples + 1)\n    subkeys = jnp.array(subkeys)\n\n    def inner():\n        _, gibbs_chain = jax.lax.scan(\n            lambda walls, key: (gibbs_update(key, args, readings, walls), walls),\n            walls,\n            subkeys,\n        )\n        return gibbs_chain\n\n    return jax.jit(inner)()\n\n\ngibbs_chain = run_gibbs_chain(\n    jrand.key(0), simple_gibbs_sweep, (CENTER, DEFAULT_PARAMS), observed_readings\n)\ngibbs_chain[:10]\n</pre> def run_gibbs_chain(key, gibbs_update, args, readings, num_samples=100):     walls = jnp.zeros((GRID_SIZE, GRID_SIZE))     key, *subkeys = jrand.split(key, num_samples + 1)     subkeys = jnp.array(subkeys)      def inner():         _, gibbs_chain = jax.lax.scan(             lambda walls, key: (gibbs_update(key, args, readings, walls), walls),             walls,             subkeys,         )         return gibbs_chain      return jax.jit(inner)()   gibbs_chain = run_gibbs_chain(     jrand.key(0), simple_gibbs_sweep, (CENTER, DEFAULT_PARAMS), observed_readings ) gibbs_chain[:10] Out[21]: In\u00a0[22]: Copied! <pre>def gibbs_update_pixel_incremental(key, trace: genjax.Trace, i, j):\n    walls = trace.get_choices()[\"walls\"]\n    # IndexRequest should be faster but isn't. Too much overhead?\n    # request = genjax.StaticRequest({\"walls\": genjax.IndexRequest(jnp.array(i), genjax.IndexRequest(jnp.array(j), genjax.Update(C.v(1.0 - walls[i, j]))))})\n    request = genjax.StaticRequest({\n        \"walls\": genjax.Update(C.v(walls.at[i, j].set(1.0 - walls[i, j])))\n    })\n    new_tr, inc_weight, _retdiff, _bwd_request = trace.edit(key, request, None)\n    return jax.lax.cond(\n        genjax.bernoulli.sample(\n            key, logits=inc_weight\n        ),  # i.e. with probability e^inc_weight / (1 + e^inc_weight)\n        lambda: new_tr,  # use the updated trace\n        lambda: trace,  # otherwise keep the old trace\n    )\n\n\ndef simple_gibbs_sweep_incremental(key, trace):\n    shape = trace.get_choices()[\"walls\"].shape\n    subkeys = jrand.split(key, shape)\n    trace = jax.lax.fori_loop(\n        0,\n        shape[0],\n        lambda i, trace: jax.lax.fori_loop(\n            0,\n            shape[1],\n            lambda j, trace: gibbs_update_pixel_incremental(subkeys[i][j], trace, i, j),\n            trace,\n        ),\n        trace,\n    )\n    return trace\n\n\ndef run_gibbs_chain_incremental(key, gibbs_update, args, readings, num_samples=100):\n    walls = jnp.zeros((GRID_SIZE, GRID_SIZE))\n    constraints = C[\"walls\"].set(walls) | C[\"readings\"].set(readings)\n    trace, _ = full_model.importance(key, constraints, args)\n    key, *subkeys = jrand.split(key, num_samples + 1)\n    subkeys = jnp.array(subkeys)\n\n    def inner():\n        _, gibbs_chain = jax.lax.scan(\n            lambda trace, key: (gibbs_update(key, trace), trace.get_choices()[\"walls\"]),\n            trace,\n            subkeys,\n        )\n        return gibbs_chain\n\n    return jax.jit(inner)()\n\n\ngibbs_chain_incremental = jax.jit(\n    lambda: run_gibbs_chain_incremental(\n        jrand.key(0),\n        simple_gibbs_sweep_incremental,\n        (CENTER, DEFAULT_PARAMS),\n        observed_readings,\n    )\n)()\ngibbs_chain_incremental[:10]\n</pre> def gibbs_update_pixel_incremental(key, trace: genjax.Trace, i, j):     walls = trace.get_choices()[\"walls\"]     # IndexRequest should be faster but isn't. Too much overhead?     # request = genjax.StaticRequest({\"walls\": genjax.IndexRequest(jnp.array(i), genjax.IndexRequest(jnp.array(j), genjax.Update(C.v(1.0 - walls[i, j]))))})     request = genjax.StaticRequest({         \"walls\": genjax.Update(C.v(walls.at[i, j].set(1.0 - walls[i, j])))     })     new_tr, inc_weight, _retdiff, _bwd_request = trace.edit(key, request, None)     return jax.lax.cond(         genjax.bernoulli.sample(             key, logits=inc_weight         ),  # i.e. with probability e^inc_weight / (1 + e^inc_weight)         lambda: new_tr,  # use the updated trace         lambda: trace,  # otherwise keep the old trace     )   def simple_gibbs_sweep_incremental(key, trace):     shape = trace.get_choices()[\"walls\"].shape     subkeys = jrand.split(key, shape)     trace = jax.lax.fori_loop(         0,         shape[0],         lambda i, trace: jax.lax.fori_loop(             0,             shape[1],             lambda j, trace: gibbs_update_pixel_incremental(subkeys[i][j], trace, i, j),             trace,         ),         trace,     )     return trace   def run_gibbs_chain_incremental(key, gibbs_update, args, readings, num_samples=100):     walls = jnp.zeros((GRID_SIZE, GRID_SIZE))     constraints = C[\"walls\"].set(walls) | C[\"readings\"].set(readings)     trace, _ = full_model.importance(key, constraints, args)     key, *subkeys = jrand.split(key, num_samples + 1)     subkeys = jnp.array(subkeys)      def inner():         _, gibbs_chain = jax.lax.scan(             lambda trace, key: (gibbs_update(key, trace), trace.get_choices()[\"walls\"]),             trace,             subkeys,         )         return gibbs_chain      return jax.jit(inner)()   gibbs_chain_incremental = jax.jit(     lambda: run_gibbs_chain_incremental(         jrand.key(0),         simple_gibbs_sweep_incremental,         (CENTER, DEFAULT_PARAMS),         observed_readings,     ) )() gibbs_chain_incremental[:10] Out[22]: <p>Due to the performance decrease, we will stick with the first version.</p> In\u00a0[23]: Copied! <pre>def plot_inferred_walls(walls):\n    return Plot.rect(\n        Plot.js(\"wallsToCenters(%1)\", walls),\n        x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),\n        x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),\n        y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),\n        y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),\n        fill=Plot.constantly(\"inferred walls\"),\n        fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"),\n    )\n\n\ndef make_plot(\n    true_walls,\n    pos=None,\n    sensor_readings=None,\n    angles=None,\n    inferred_walls=None,\n    interactive=False,\n):\n    true_walls = true_walls.astype(jnp.float32)\n    map_plot = Plot.new()\n    if inferred_walls is not None:\n        inferred_walls = inferred_walls.astype(jnp.float32)\n        map_plot += plot_inferred_walls(inferred_walls)\n    map_plot += plot_walls\n    if sensor_readings is not None and angles is not None:\n        map_plot += plot_sensors(pos, sensor_readings, angles)\n    if interactive:\n        map_plot += interactive_walls\n    plot = (\n        js_code\n        &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True)\n        &amp; Plot.new(map_plot, Plot.color_legend())\n    )\n    if interactive:\n        plot = plot | [\n            \"div.bg-blue-500.text-white.p-3.rounded-sm\",\n            {\n                \"onClick\": lambda widget, _event: widget.state.update({\n                    \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))\n                })\n            },\n            \"Clear walls\",\n        ]\n        plot = plot.display_as(\"widget\")\n    return plot\n\n\nmake_plot(\n    true_walls,\n    pos=CENTER,\n    sensor_readings=observed_readings,\n    angles=angles(NUM_DIRECTIONS),\n    inferred_walls=true_walls,\n)\n</pre> def plot_inferred_walls(walls):     return Plot.rect(         Plot.js(\"wallsToCenters(%1)\", walls),         x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),         x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),         y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),         y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),         fill=Plot.constantly(\"inferred walls\"),         fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"),     )   def make_plot(     true_walls,     pos=None,     sensor_readings=None,     angles=None,     inferred_walls=None,     interactive=False, ):     true_walls = true_walls.astype(jnp.float32)     map_plot = Plot.new()     if inferred_walls is not None:         inferred_walls = inferred_walls.astype(jnp.float32)         map_plot += plot_inferred_walls(inferred_walls)     map_plot += plot_walls     if sensor_readings is not None and angles is not None:         map_plot += plot_sensors(pos, sensor_readings, angles)     if interactive:         map_plot += interactive_walls     plot = (         js_code         &amp; Plot.initial_state({\"true_walls\": true_walls}, sync=True)         &amp; Plot.new(map_plot, Plot.color_legend())     )     if interactive:         plot = plot | [             \"div.bg-blue-500.text-white.p-3.rounded-sm\",             {                 \"onClick\": lambda widget, _event: widget.state.update({                     \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))                 })             },             \"Clear walls\",         ]         plot = plot.display_as(\"widget\")     return plot   make_plot(     true_walls,     pos=CENTER,     sensor_readings=observed_readings,     angles=angles(NUM_DIRECTIONS),     inferred_walls=true_walls, ) Out[23]: <p>Use the slider at the bottom of the first plot to visualize the iterations of the Gibbs sampler. The second plot averages over the chain and gives an idea of how likely each pixel is to be a wall.</p> In\u00a0[24]: Copied! <pre>num_frames = 100\nthinning = gibbs_chain.shape[0] // num_frames\nanimation = Plot.Frames([\n    make_plot(\n        true_walls,\n        pos=CENTER,\n        sensor_readings=observed_readings,\n        angles=angles(NUM_DIRECTIONS),\n        inferred_walls=sample,\n    )\n    for sample in gibbs_chain[::thinning]\n])\ndisplay(animation)\ngibbs_mean = jnp.mean(gibbs_chain, axis=0)\nplot = make_plot(\n    true_walls,\n    pos=CENTER,\n    sensor_readings=observed_readings,\n    angles=angles(NUM_DIRECTIONS),\n    inferred_walls=gibbs_mean,\n)\ndisplay(plot)\n</pre> num_frames = 100 thinning = gibbs_chain.shape[0] // num_frames animation = Plot.Frames([     make_plot(         true_walls,         pos=CENTER,         sensor_readings=observed_readings,         angles=angles(NUM_DIRECTIONS),         inferred_walls=sample,     )     for sample in gibbs_chain[::thinning] ]) display(animation) gibbs_mean = jnp.mean(gibbs_chain, axis=0) plot = make_plot(     true_walls,     pos=CENTER,     sensor_readings=observed_readings,     angles=angles(NUM_DIRECTIONS),     inferred_walls=gibbs_mean, ) display(plot) <p>We can see that it takes about 10 iterations before all the walls at the bottom are removed.</p> In\u00a0[25]: Copied! <pre>jax.vmap(\n    lambda dist: jax.vmap(\n        lambda i: jax.vmap(lambda j: jnp.abs(i - 5) + jnp.abs(j - 5) == dist)(\n            jnp.arange(11)\n        )\n    )(jnp.arange(11))\n)(jnp.arange(10))\n</pre> jax.vmap(     lambda dist: jax.vmap(         lambda i: jax.vmap(lambda j: jnp.abs(i - 5) + jnp.abs(j - 5) == dist)(             jnp.arange(11)         )     )(jnp.arange(11)) )(jnp.arange(10)) Out[25]: In\u00a0[26]: Copied! <pre>def smarter_gibbs_update_distance(key, args, readings, walls, distance):\n    pos, _params = args\n    subkeys = jrand.split(key, (GRID_SIZE, GRID_SIZE))\n    updated_walls = jax.lax.fori_loop(\n        0,\n        GRID_SIZE,\n        lambda i, walls: jax.lax.fori_loop(\n            0,\n            GRID_SIZE,\n            lambda j, walls: walls.at[i, j].set(\n                jax.lax.cond(\n                    jnp.sum(jnp.floor(jnp.abs(jnp.array([i, j]) - pos))) == distance,\n                    lambda: gibbs_update_pixel(\n                        subkeys[i, j], args, readings, walls, i, j\n                    ),\n                    lambda: walls[i, j],\n                )\n            ),\n            walls,\n        ),\n        walls,\n    )\n    return updated_walls\n\n\ndef smarter_gibbs_update(key, args, readings, walls):\n    subkeys = jrand.split(key, 2 * GRID_SIZE)\n    walls = jax.lax.fori_loop(\n        0,\n        2 * GRID_SIZE,\n        lambda distance, walls: smarter_gibbs_update_distance(\n            subkeys[distance], args, readings, walls, distance\n        ),\n        walls,\n    )\n    return walls\n</pre> def smarter_gibbs_update_distance(key, args, readings, walls, distance):     pos, _params = args     subkeys = jrand.split(key, (GRID_SIZE, GRID_SIZE))     updated_walls = jax.lax.fori_loop(         0,         GRID_SIZE,         lambda i, walls: jax.lax.fori_loop(             0,             GRID_SIZE,             lambda j, walls: walls.at[i, j].set(                 jax.lax.cond(                     jnp.sum(jnp.floor(jnp.abs(jnp.array([i, j]) - pos))) == distance,                     lambda: gibbs_update_pixel(                         subkeys[i, j], args, readings, walls, i, j                     ),                     lambda: walls[i, j],                 )             ),             walls,         ),         walls,     )     return updated_walls   def smarter_gibbs_update(key, args, readings, walls):     subkeys = jrand.split(key, 2 * GRID_SIZE)     walls = jax.lax.fori_loop(         0,         2 * GRID_SIZE,         lambda distance, walls: smarter_gibbs_update_distance(             subkeys[distance], args, readings, walls, distance         ),         walls,     )     return walls In\u00a0[27]: Copied! <pre>gibbs_chain = run_gibbs_chain(\n    jrand.key(0), smarter_gibbs_update, (CENTER, DEFAULT_PARAMS), observed_readings\n)\ngibbs_chain[:10]\n</pre> gibbs_chain = run_gibbs_chain(     jrand.key(0), smarter_gibbs_update, (CENTER, DEFAULT_PARAMS), observed_readings ) gibbs_chain[:10] Out[27]: In\u00a0[28]: Copied! <pre>animation = Plot.Frames([\n    make_plot(\n        true_walls,\n        pos=CENTER,\n        sensor_readings=observed_readings,\n        angles=angles(NUM_DIRECTIONS),\n        inferred_walls=sample,\n    )\n    for sample in gibbs_chain\n])\ndisplay(animation)\ngibbs_mean = jnp.mean(gibbs_chain, axis=0)\nplot = make_plot(\n    true_walls,\n    pos=CENTER,\n    sensor_readings=observed_readings,\n    angles=angles(NUM_DIRECTIONS),\n    inferred_walls=gibbs_mean,\n)\ndisplay(plot)\n</pre> animation = Plot.Frames([     make_plot(         true_walls,         pos=CENTER,         sensor_readings=observed_readings,         angles=angles(NUM_DIRECTIONS),         inferred_walls=sample,     )     for sample in gibbs_chain ]) display(animation) gibbs_mean = jnp.mean(gibbs_chain, axis=0) plot = make_plot(     true_walls,     pos=CENTER,     sensor_readings=observed_readings,     angles=angles(NUM_DIRECTIONS),     inferred_walls=gibbs_mean, ) display(plot) <p>Here it only takes 3 iterations before all the walls at the bottom are (correctly) removed.</p> <p>Problem: This approach only works well if the prior wall probability was around 0.5. Let's see what happens for a much smaller value.</p> In\u00a0[29]: Copied! <pre>modified_params = ModelParams(\n    prior_wall_prob=0.05,\n    sensor_noise=DEFAULT_PARAMS.sensor_noise,\n    num_angles=DEFAULT_PARAMS.num_angles,\n)\ngibbs_chain = run_gibbs_chain(\n    jrand.key(0), smarter_gibbs_update, (CENTER, modified_params), observed_readings\n)\ngibbs_chain[:10]\n</pre> modified_params = ModelParams(     prior_wall_prob=0.05,     sensor_noise=DEFAULT_PARAMS.sensor_noise,     num_angles=DEFAULT_PARAMS.num_angles, ) gibbs_chain = run_gibbs_chain(     jrand.key(0), smarter_gibbs_update, (CENTER, modified_params), observed_readings ) gibbs_chain[:10] Out[29]: In\u00a0[30]: Copied! <pre>animation = Plot.Frames([\n    make_plot(\n        true_walls,\n        pos=CENTER,\n        sensor_readings=observed_readings,\n        angles=angles(NUM_DIRECTIONS),\n        inferred_walls=sample,\n    )\n    for sample in gibbs_chain\n])\ndisplay(animation)\ngibbs_mean = jnp.mean(gibbs_chain, axis=0)\nplot = make_plot(\n    true_walls,\n    pos=CENTER,\n    sensor_readings=observed_readings,\n    angles=angles(NUM_DIRECTIONS),\n    inferred_walls=gibbs_mean,\n)\ndisplay(plot)\n</pre> animation = Plot.Frames([     make_plot(         true_walls,         pos=CENTER,         sensor_readings=observed_readings,         angles=angles(NUM_DIRECTIONS),         inferred_walls=sample,     )     for sample in gibbs_chain ]) display(animation) gibbs_mean = jnp.mean(gibbs_chain, axis=0) plot = make_plot(     true_walls,     pos=CENTER,     sensor_readings=observed_readings,     angles=angles(NUM_DIRECTIONS),     inferred_walls=gibbs_mean, ) display(plot) <p>We see that Gibbs does not seem to converge at all. Why is that?</p> <p>Due to the low prior probability, there are very few walls on the map. Let's say we do a Gibbs update on a pixel that's currently a wall. If we remove the wall, that means the affected sensor rays will often not hit anything else and the sensor reading will change drastically. This means a sudden drop in likelihood, so the Gibbs update will leave the pixel unchanged. In other words, the pixel and its neighborhood are highly correlated, and Gibbs struggles in such situation.</p> In\u00a0[31]: Copied! <pre>def generate_all_possible_blocks(block_size):\n    \"\"\"Returns all possible squares of size `block_size` x `block_size`, filled with 0s and 1s.\n\n    Returned shape: 2^(block_size^2) x block_size x block_size\"\"\"\n    values = jnp.meshgrid(*(jnp.arange(2) for _ in range(block_size * block_size)))\n    values = [value.flatten() for value in values]\n    values = jnp.stack(values, axis=1)\n    return values.reshape(-1, block_size, block_size)\n\n\ngenerate_all_possible_blocks(2)\n</pre> def generate_all_possible_blocks(block_size):     \"\"\"Returns all possible squares of size `block_size` x `block_size`, filled with 0s and 1s.      Returned shape: 2^(block_size^2) x block_size x block_size\"\"\"     values = jnp.meshgrid(*(jnp.arange(2) for _ in range(block_size * block_size)))     values = [value.flatten() for value in values]     values = jnp.stack(values, axis=1)     return values.reshape(-1, block_size, block_size)   generate_all_possible_blocks(2) Out[31]: <p>The Gibbs update now works similar to the first version, but updates 2x2 blocks at once. Note that we don't use the clever ordering of the updates in Gibbs sweep. Instead, we go back to the first version of Gibbs that updates the pixels in the order of their indices. This still works well enough.</p> <p>Note that we consider all 2x2 blocks on the map, which may overlap (e.g. the one at (0, 0) and the one at (1, 1)). This does not a problem \u2013 the Gibbs update is still valid.</p> In\u00a0[32]: Copied! <pre>BLOCK_SIZE = 2\n\n\ndef gibbs_update_block(key, args, readings, walls, i, j):\n    blocks = generate_all_possible_blocks(\n        BLOCK_SIZE\n    )  # shape = (2^(BLOCK_SIZE^2), BLOCK_SIZE, BLOCK_SIZE)\n    chm = C[\"readings\"].set(readings) | C[\"walls\"].set(walls)\n    (weights, (walls_changed, _)) = jax.vmap(\n        lambda block: full_model_assess(\n            chm.at[\"walls\"].set(\n                jax.lax.dynamic_update_slice(walls, block.astype(jnp.float32), (i, j))\n            ),\n            args,\n        )\n    )(blocks)\n    # categorical automatically normalizes the weights\n    idx = genjax.categorical.sample(key, weights)\n    return walls_changed[idx]\n\n\ndef block_gibbs_sweep(key, args, readings, walls):\n    subkeys = jrand.split(key, walls.shape)\n    walls = jax.lax.fori_loop(\n        0,\n        walls.shape[0] - BLOCK_SIZE + 1,\n        lambda i, walls: jax.lax.fori_loop(\n            0,\n            walls.shape[1] - BLOCK_SIZE + 1,\n            lambda j, walls: gibbs_update_block(\n                subkeys[i, j], args, readings, walls, i, j\n            ),\n            walls,\n        ),\n        walls,\n    )\n    return walls\n</pre> BLOCK_SIZE = 2   def gibbs_update_block(key, args, readings, walls, i, j):     blocks = generate_all_possible_blocks(         BLOCK_SIZE     )  # shape = (2^(BLOCK_SIZE^2), BLOCK_SIZE, BLOCK_SIZE)     chm = C[\"readings\"].set(readings) | C[\"walls\"].set(walls)     (weights, (walls_changed, _)) = jax.vmap(         lambda block: full_model_assess(             chm.at[\"walls\"].set(                 jax.lax.dynamic_update_slice(walls, block.astype(jnp.float32), (i, j))             ),             args,         )     )(blocks)     # categorical automatically normalizes the weights     idx = genjax.categorical.sample(key, weights)     return walls_changed[idx]   def block_gibbs_sweep(key, args, readings, walls):     subkeys = jrand.split(key, walls.shape)     walls = jax.lax.fori_loop(         0,         walls.shape[0] - BLOCK_SIZE + 1,         lambda i, walls: jax.lax.fori_loop(             0,             walls.shape[1] - BLOCK_SIZE + 1,             lambda j, walls: gibbs_update_block(                 subkeys[i, j], args, readings, walls, i, j             ),             walls,         ),         walls,     )     return walls In\u00a0[33]: Copied! <pre>gibbs_chain = run_gibbs_chain(\n    jrand.key(0), block_gibbs_sweep, (CENTER, modified_params), observed_readings\n)\ngibbs_chain\n</pre> gibbs_chain = run_gibbs_chain(     jrand.key(0), block_gibbs_sweep, (CENTER, modified_params), observed_readings ) gibbs_chain Out[33]: In\u00a0[34]: Copied! <pre>animation = Plot.Frames([\n    make_plot(\n        true_walls,\n        pos=CENTER,\n        sensor_readings=observed_readings,\n        angles=angles(NUM_DIRECTIONS),\n        inferred_walls=sample,\n    )\n    for sample in gibbs_chain\n])\ndisplay(animation)\ngibbs_mean = jnp.mean(gibbs_chain, axis=0)\nplot = make_plot(\n    true_walls,\n    pos=CENTER,\n    sensor_readings=observed_readings,\n    angles=angles(NUM_DIRECTIONS),\n    inferred_walls=gibbs_mean,\n)\ndisplay(plot)\n</pre> animation = Plot.Frames([     make_plot(         true_walls,         pos=CENTER,         sensor_readings=observed_readings,         angles=angles(NUM_DIRECTIONS),         inferred_walls=sample,     )     for sample in gibbs_chain ]) display(animation) gibbs_mean = jnp.mean(gibbs_chain, axis=0) plot = make_plot(     true_walls,     pos=CENTER,     sensor_readings=observed_readings,     angles=angles(NUM_DIRECTIONS),     inferred_walls=gibbs_mean, ) display(plot) <p>One can see that each Gibbs sweep \"pushes\" the walls that are too close to the center outwards, until they match the observations better.</p> In\u00a0[35]: Copied! <pre>def on_click(widget, event):\n    x, y = round(event[\"x\"]), round(event[\"y\"])\n    pos = widget.state.position\n    if (event[\"x\"] - pos[0]) ** 2 + (event[\"y\"] - pos[1]) ** 2 &lt; 0.25:\n        # don't draw a wall on the sensor\n        widget.state.update({\"wall_mode\": False})\n    else:\n        widget.state.update({\"wall_mode\": True})\n        x, y = round(event[\"x\"]), round(event[\"y\"])\n        pos = widget.state.position\n        true_walls = jnp.array(widget.state.true_walls)\n        true_walls = true_walls.at[x, y].set(1 - true_walls[x, y])\n        widget.state.update({\"true_walls\": true_walls})\n\n\ndef on_drag(widget, event):\n    if widget.state.wall_mode:\n        x, y = round(event[\"x\"]), round(event[\"y\"])\n        true_walls = jnp.array(widget.state.true_walls)\n        true_walls = true_walls.at[x, y].set(1)\n        widget.state.update({\"true_walls\": true_walls})\n\n\ninteractive_walls = Plot.events(onClick=on_click, onDraw=on_drag)\n\nplot_inferred_walls_interactive = Plot.rect(\n    Plot.js(\"wallsToCenters($state.inferred_walls)\", walls),\n    x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),\n    x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),\n    y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),\n    y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),\n    fill=Plot.constantly(\"inferred walls\"),\n    fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"),\n)\n\nsensor_js_code = Plot.Import(\n    \"\"\"\nexport function rayEndpoints(pos, sensor_readings, num_angles) {\n    let endpoints = [];\n    let angleStep = (2 * Math.PI) / num_angles; // Divide full circle into equal parts\n\n    for (let i = 0; i &lt; num_angles; i++) {\n        let angle = i * angleStep;\n        let distance = sensor_readings[i];\n        let x = pos[0] + distance * Math.cos(angle);\n        let y = pos[1] + distance * Math.sin(angle);\n        endpoints.push([x, y]);\n    }\n    return endpoints;\n}\n\"\"\",\n    refer=[\"rayEndpoints\"],\n)\n\n\ndef on_position_drag(widget, event):\n    widget.state.update({\n        \"wall_mode\": False,\n        \"position\": jnp.array([event[\"x\"], event[\"y\"]]),\n    })\n\n\nplot_interactive_sensors = sensor_js_code &amp; (\n    Plot.line(\n        Plot.js(\n            \"rayEndpoints($state.position, $state.sensor_readings, $state.num_angles).flatMap(p =&gt; [p, $state.position])\"\n        ),\n        stroke=Plot.constantly(\"sensor rays\"),\n    )\n    + Plot.ellipse(\n        Plot.js(\n            \"rayEndpoints($state.position, $state.sensor_readings, $state.num_angles)\"\n        ),\n        r=0.1,\n        fill=Plot.constantly(\"sensor readings\"),\n    )\n    + Plot.ellipse(\n        [Plot.js(\"$state.position\")],\n        r=0.3,\n        fill=Plot.constantly(\"sensor position\"),\n        render=Plot.renderChildEvents(onDrag=on_position_drag),\n    )\n)\n\n\ndef on_change(widget, _event):\n    update_state(widget.state)\n\n\ndef update_state(state):\n    true_walls = state.true_walls\n    params = ModelParams(\n        prior_wall_prob=float(state.prior_wall_prob),\n        sensor_noise=float(state.sensor_noise),\n        num_angles=int(state.num_angles),\n    )\n    pos = state.position\n    key = jrand.key(0)\n    readings = sensor_model.simulate(\n        key, (pos, true_walls, params.sensor_noise, angles(params.num_angles))\n    ).get_retval()\n    state.sensor_readings = readings\n    update_inferred_walls(state)\n\n\ndef update_inferred_walls(state):\n    if state.show_mean:\n        state.inferred_walls = (\n            state.mean_chain\n            if state.mean_chain is not None\n            else jnp.zeros(true_walls.shape)\n        )\n    else:\n        state.inferred_walls = (\n            state.chain[state.chain_idx]\n            if state.chain is not None\n            else jnp.zeros(true_walls.shape)\n        )\n\n\ndef do_inference(state):\n    update_state(state)\n    pos = state.position\n    readings = state.sensor_readings\n    params = ModelParams(\n        prior_wall_prob=float(state.prior_wall_prob),\n        sensor_noise=float(state.sensor_noise),\n        num_angles=int(state.num_angles),\n    )\n    key = jrand.key(0)\n    state.chain = run_gibbs_chain(key, block_gibbs_sweep, (pos, params), readings)\n    state.mean_chain = jnp.mean(state.chain, axis=0)\n    update_inferred_walls(state)\n\n\ndef make_interactive_plot(true_walls):\n    true_walls = true_walls.astype(jnp.float32)\n    map_plot = Plot.new()\n    map_plot += plot_inferred_walls_interactive\n    map_plot += plot_walls\n    map_plot += plot_interactive_sensors\n    map_plot += interactive_walls\n    buttons = (\n        Plot.html([\n            \"div.bg-blue-500.text-white.p-3.rounded-sm\",\n            {\"onClick\": lambda widget, _event: do_inference(widget.state)},\n            \"Run inference\",\n        ])\n        &amp; Plot.html([\n            \"div.bg-blue-500.text-white.p-3.rounded-sm\",\n            {\n                \"onClick\": lambda widget, _event: widget.state.update({\n                    \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))\n                })\n            },\n            \"Clear walls\",\n        ])\n        &amp; Plot.html([\n            \"div.bg-blue-500.text-white.p-3.rounded-sm\",\n            {\n                \"onClick\": lambda widget, _event: widget.state.update({\n                    \"inferred_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE)),\n                    \"chain\": None,\n                    \"mean_chain\": None,\n                })\n            },\n            \"Clear inferred walls\",\n        ])\n    )\n    sliders = (\n        Plot.html([\n            \"label\",\n            {\"class\": \"flex items-center gap-2 cursor-pointer\"},\n            [\n                \"input\",\n                {\n                    \"type\": \"checkbox\",\n                    \"checked\": Plot.js(\"$state.show_mean\"),\n                    \"onChange\": Plot.js(\"(e) =&gt; $state.show_mean = e.target.checked\"),\n                },\n            ],\n            \"show average over chain\",\n        ])\n        &amp; Plot.Slider(\n            key=\"chain_idx\",\n            range=(0, 100),\n            step=1,\n            label=Plot.js(\n                \"$state.show_mean ? `Deselect 'show average' to view individual samples` : `Sample no.: ${$state.chain_idx}`\"\n            ),\n        )\n    ) | (\n        Plot.Slider(\n            key=\"prior_wall_prob\",\n            range=(0, 1),\n            step=0.005,\n            label=Plot.js(\"`Prior wall probability: ${$state.prior_wall_prob}`\"),\n        )\n    ) &amp; (\n        Plot.Slider(\n            key=\"sensor_noise\",\n            range=(0, 5),\n            step=0.05,\n            label=Plot.js(\"`Sensor noise: ${$state.sensor_noise}`\"),\n        )\n    ) &amp; (\n        Plot.Slider(\n            key=\"num_angles\",\n            range=(10, 1000),\n            step=10,\n            label=Plot.js(\"`Number of angles: ${$state.num_angles}`\"),\n        )\n    )\n    initial_pos = CENTER\n    initial_noise = 0.2\n    plot = (\n        (\n            js_code\n            &amp; sensor_js_code\n            &amp; Plot.initial_state(\n                {\n                    \"true_walls\": true_walls,\n                    \"position\": initial_pos,\n                    \"sensor_noise\": initial_noise,\n                    \"prior_wall_prob\": 0.5,\n                    \"num_angles\": NUM_DIRECTIONS,\n                    \"sensor_readings\": sensor_model.simulate(\n                        key,\n                        (\n                            initial_pos,\n                            true_walls,\n                            initial_noise,\n                            angles(NUM_DIRECTIONS),\n                        ),\n                    ).get_retval(),\n                    \"inferred_walls\": jnp.zeros(true_walls.shape),\n                    \"show_mean\": True,\n                    \"chain_idx\": 0,\n                    \"chain\": None,\n                    \"mean_chain\": None,\n                    \"wall_mode\": False,\n                },\n                sync=True,\n            )\n            &amp; Plot.new(map_plot, Plot.color_legend())\n        )\n        | buttons\n        | sliders\n        | Plot.onChange({\n            \"prior_wall_prob\": on_change,\n            \"sensor_noise\": on_change,\n            \"num_angles\": on_change,\n            \"true_walls\": on_change,\n            \"position\": on_change,\n            \"chain_idx\": on_change,\n            \"show_mean\": on_change,\n        })\n    )\n    plot = plot.display_as(\"widget\")\n    return plot\n\n\nmake_interactive_plot(true_walls)\n</pre> def on_click(widget, event):     x, y = round(event[\"x\"]), round(event[\"y\"])     pos = widget.state.position     if (event[\"x\"] - pos[0]) ** 2 + (event[\"y\"] - pos[1]) ** 2 &lt; 0.25:         # don't draw a wall on the sensor         widget.state.update({\"wall_mode\": False})     else:         widget.state.update({\"wall_mode\": True})         x, y = round(event[\"x\"]), round(event[\"y\"])         pos = widget.state.position         true_walls = jnp.array(widget.state.true_walls)         true_walls = true_walls.at[x, y].set(1 - true_walls[x, y])         widget.state.update({\"true_walls\": true_walls})   def on_drag(widget, event):     if widget.state.wall_mode:         x, y = round(event[\"x\"]), round(event[\"y\"])         true_walls = jnp.array(widget.state.true_walls)         true_walls = true_walls.at[x, y].set(1)         widget.state.update({\"true_walls\": true_walls})   interactive_walls = Plot.events(onClick=on_click, onDraw=on_drag)  plot_inferred_walls_interactive = Plot.rect(     Plot.js(\"wallsToCenters($state.inferred_walls)\", walls),     x1=Plot.js(\"([x, y, value]) =&gt; x - 0.5\"),     x2=Plot.js(\"([x, y, value]) =&gt; x + 0.5\"),     y1=Plot.js(\"([x, y, value]) =&gt; y - 0.5\"),     y2=Plot.js(\"([x, y, value]) =&gt; y + 0.5\"),     fill=Plot.constantly(\"inferred walls\"),     fillOpacity=Plot.js(\"([x, y, value]) =&gt; value\"), )  sensor_js_code = Plot.Import(     \"\"\" export function rayEndpoints(pos, sensor_readings, num_angles) {     let endpoints = [];     let angleStep = (2 * Math.PI) / num_angles; // Divide full circle into equal parts      for (let i = 0; i &lt; num_angles; i++) {         let angle = i * angleStep;         let distance = sensor_readings[i];         let x = pos[0] + distance * Math.cos(angle);         let y = pos[1] + distance * Math.sin(angle);         endpoints.push([x, y]);     }     return endpoints; } \"\"\",     refer=[\"rayEndpoints\"], )   def on_position_drag(widget, event):     widget.state.update({         \"wall_mode\": False,         \"position\": jnp.array([event[\"x\"], event[\"y\"]]),     })   plot_interactive_sensors = sensor_js_code &amp; (     Plot.line(         Plot.js(             \"rayEndpoints($state.position, $state.sensor_readings, $state.num_angles).flatMap(p =&gt; [p, $state.position])\"         ),         stroke=Plot.constantly(\"sensor rays\"),     )     + Plot.ellipse(         Plot.js(             \"rayEndpoints($state.position, $state.sensor_readings, $state.num_angles)\"         ),         r=0.1,         fill=Plot.constantly(\"sensor readings\"),     )     + Plot.ellipse(         [Plot.js(\"$state.position\")],         r=0.3,         fill=Plot.constantly(\"sensor position\"),         render=Plot.renderChildEvents(onDrag=on_position_drag),     ) )   def on_change(widget, _event):     update_state(widget.state)   def update_state(state):     true_walls = state.true_walls     params = ModelParams(         prior_wall_prob=float(state.prior_wall_prob),         sensor_noise=float(state.sensor_noise),         num_angles=int(state.num_angles),     )     pos = state.position     key = jrand.key(0)     readings = sensor_model.simulate(         key, (pos, true_walls, params.sensor_noise, angles(params.num_angles))     ).get_retval()     state.sensor_readings = readings     update_inferred_walls(state)   def update_inferred_walls(state):     if state.show_mean:         state.inferred_walls = (             state.mean_chain             if state.mean_chain is not None             else jnp.zeros(true_walls.shape)         )     else:         state.inferred_walls = (             state.chain[state.chain_idx]             if state.chain is not None             else jnp.zeros(true_walls.shape)         )   def do_inference(state):     update_state(state)     pos = state.position     readings = state.sensor_readings     params = ModelParams(         prior_wall_prob=float(state.prior_wall_prob),         sensor_noise=float(state.sensor_noise),         num_angles=int(state.num_angles),     )     key = jrand.key(0)     state.chain = run_gibbs_chain(key, block_gibbs_sweep, (pos, params), readings)     state.mean_chain = jnp.mean(state.chain, axis=0)     update_inferred_walls(state)   def make_interactive_plot(true_walls):     true_walls = true_walls.astype(jnp.float32)     map_plot = Plot.new()     map_plot += plot_inferred_walls_interactive     map_plot += plot_walls     map_plot += plot_interactive_sensors     map_plot += interactive_walls     buttons = (         Plot.html([             \"div.bg-blue-500.text-white.p-3.rounded-sm\",             {\"onClick\": lambda widget, _event: do_inference(widget.state)},             \"Run inference\",         ])         &amp; Plot.html([             \"div.bg-blue-500.text-white.p-3.rounded-sm\",             {                 \"onClick\": lambda widget, _event: widget.state.update({                     \"true_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE))                 })             },             \"Clear walls\",         ])         &amp; Plot.html([             \"div.bg-blue-500.text-white.p-3.rounded-sm\",             {                 \"onClick\": lambda widget, _event: widget.state.update({                     \"inferred_walls\": jnp.zeros((GRID_SIZE, GRID_SIZE)),                     \"chain\": None,                     \"mean_chain\": None,                 })             },             \"Clear inferred walls\",         ])     )     sliders = (         Plot.html([             \"label\",             {\"class\": \"flex items-center gap-2 cursor-pointer\"},             [                 \"input\",                 {                     \"type\": \"checkbox\",                     \"checked\": Plot.js(\"$state.show_mean\"),                     \"onChange\": Plot.js(\"(e) =&gt; $state.show_mean = e.target.checked\"),                 },             ],             \"show average over chain\",         ])         &amp; Plot.Slider(             key=\"chain_idx\",             range=(0, 100),             step=1,             label=Plot.js(                 \"$state.show_mean ? `Deselect 'show average' to view individual samples` : `Sample no.: ${$state.chain_idx}`\"             ),         )     ) | (         Plot.Slider(             key=\"prior_wall_prob\",             range=(0, 1),             step=0.005,             label=Plot.js(\"`Prior wall probability: ${$state.prior_wall_prob}`\"),         )     ) &amp; (         Plot.Slider(             key=\"sensor_noise\",             range=(0, 5),             step=0.05,             label=Plot.js(\"`Sensor noise: ${$state.sensor_noise}`\"),         )     ) &amp; (         Plot.Slider(             key=\"num_angles\",             range=(10, 1000),             step=10,             label=Plot.js(\"`Number of angles: ${$state.num_angles}`\"),         )     )     initial_pos = CENTER     initial_noise = 0.2     plot = (         (             js_code             &amp; sensor_js_code             &amp; Plot.initial_state(                 {                     \"true_walls\": true_walls,                     \"position\": initial_pos,                     \"sensor_noise\": initial_noise,                     \"prior_wall_prob\": 0.5,                     \"num_angles\": NUM_DIRECTIONS,                     \"sensor_readings\": sensor_model.simulate(                         key,                         (                             initial_pos,                             true_walls,                             initial_noise,                             angles(NUM_DIRECTIONS),                         ),                     ).get_retval(),                     \"inferred_walls\": jnp.zeros(true_walls.shape),                     \"show_mean\": True,                     \"chain_idx\": 0,                     \"chain\": None,                     \"mean_chain\": None,                     \"wall_mode\": False,                 },                 sync=True,             )             &amp; Plot.new(map_plot, Plot.color_legend())         )         | buttons         | sliders         | Plot.onChange({             \"prior_wall_prob\": on_change,             \"sensor_noise\": on_change,             \"num_angles\": on_change,             \"true_walls\": on_change,             \"position\": on_change,             \"chain_idx\": on_change,             \"show_mean\": on_change,         })     )     plot = plot.display_as(\"widget\")     return plot   make_interactive_plot(true_walls) Out[35]:"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#mapping-tutorial","title":"Mapping tutorial\u00b6","text":""},{"location":"cookbook/inactive/inference/mapping_tutorial.html#loading-the-map","title":"Loading the map\u00b6","text":"<p>The following function is useful to load the ground truth map from a string (which makes editing the walls by hand easier than the array representation).</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#plotting","title":"Plotting\u00b6","text":"<p>In order to understand the problem, our code, and for debugging, it is useful to have visualizations. Hence we define the following functions to plot the map.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#interactive-map","title":"Interactive map\u00b6","text":""},{"location":"cookbook/inactive/inference/mapping_tutorial.html#prior","title":"Prior\u00b6","text":"<p>For a Bayesian model, we first need to specify our prior belief about the map. We will keep it very simple here and assume that each pixel is a wall with probability 0.5.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#exact-sensor-model","title":"Exact sensor model\u00b6","text":"<p>As mentioned at the start, the robot located at the origin has a sensor to measure the distance to the nearest wall in several directions. We can model this as follows. The math in the next cell is standard geometry, but not relevant for the overall understanding of modeling and inference in GenJAX, so feel free to skip the details.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#noisy-sensor-model","title":"Noisy sensor model\u00b6","text":"<p>A real sensor is going to be noisy, which we model with a normal distribution. As before, we specify the model for a single direction/angle first, and then <code>vmap</code> it over all directions.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#full-model","title":"Full model\u00b6","text":"<p>Now, we can put the pieces together by combining the prior and the sensor model.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#importance-sampling-self-normalized-importance-sampling","title":"Importance sampling (self-normalized importance sampling)\u00b6","text":"<p>One of the simplest inference methods is importance sampling. To do this, we constrain our model to the observed data. For samples corresponding to observations (i.e. the sensor readings), we don't actually sample, but instead use the observed value and record the likelihood of seeing that observation. GenJAX provides a method for this: <code>model.importance(key, constraints, args)</code> runs the model with the random seed <code>key</code> and arguments <code>args</code>, but constrains some sampled values according to the <code>constraints</code> (the observations, i.e. the readings). It returns a trace with the sampled values ($walls$) and a log weight $\\log p(observations \\mid walls)$.</p> <p>If we run <code>.importance</code> several times, some traces (with a better prior sample) will have a higher weight and others (with a worse prior choice) will have lower weight. Suppose these weighted samples are $(walls_i, \\log(w_i))$ for $i = 1 \\ldots N$. We can normalize them as follows: $w'_i := \\frac{w_i}{\\sum_{i} w_i}$. Then posterior expected values can be approximated using the weighted samples: $\\mathbb{E}_{walls \\sim p(walls \\mid observations)} [f(walls)] \\approx \\sum_{i=1}^N w'_i f(walls_i)$.</p> <p>We can also obtain unweighted samples approximating the posterior by resampling: sampling from a categorical distribution where each category $walls_i$ has $w'_i$.</p> <p>Let's try this.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#basic-gibbs-sampling","title":"Basic Gibbs sampling\u00b6","text":"<p>As a better inference algorithm, we turn to Gibbs sampling. Gibbs sampling is a simple algorithm to sample from a joint distribution $p(x_1, \\dots, x_n)$, assuming one can sample from the conditional distributions $p(x_i \\mid x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n)$. A Gibbs starts with initial values $x_1^{(0)}, \\dots, x_n^{(0)}$ and then iteratively updates each $x_i$ from the conditional distribution $p(x_i \\mid x_1^{(t)}, \\dots, x_{i-1}^{(t)}, x_{i+1}^{(t - 1)}, \\dots, x_n^{(t - 1)})$ until convergence.</p> <p>In our case, the $x_i$'s are the pixels. We can sample from $p(x_i \\mid x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = \\frac{p(x_1, \\dots, x_n)}{p(x_1, \\dots, x_{i-1}, 0, x_{i+1}, \\dots, x_n) + p(x_1, \\dots, x_{i-1}, 1, x_{i+1}, \\dots, x_n)}$ by enumeration: evaluating the joint density at $x_i = 0$ and $x_i = 1$ (free space or wall). To evaluate the density, GenJAX provides the method <code>model.assess(choice_map, args)</code>, which runs the model with all samples constrained to <code>choice_map</code> and returns the likelihood.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#incremental-trace-updates","title":"Incremental trace updates\u00b6","text":"<p>We can also write this more concisely in GenJAX by incrementally updating the current trace. Usually this should be faster, but it turns out to be slower in our case. This may be due to the overhead of the trace infrastructure involved.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#plotting-the-inferred-walls","title":"Plotting the inferred walls\u00b6","text":""},{"location":"cookbook/inactive/inference/mapping_tutorial.html#gibbs-sampling-part-2-better-variable-ordering","title":"Gibbs sampling, part 2: better variable ordering\u00b6","text":"<p>Gibbs sampling does not require the variables to be updated in a specific order. They order can even be random (and that sometimes improves convergence).</p> <p>We can make Gibbs sampling converge faster by updating pixels in a different order. Note that if a pixel near the origin is a wall, it casts a \"shadow\" and all the pixels in its shadow are irrelevant (and thus sampled from the prior as fair coin flips). If the pixel near the origin flips and becomes free space, all the pixels in its shadow are suddenly \"visible\", but will have been randomly initialized before.</p> <p>For this reason it is better to update the pixels from the sensor position outwards. This way, the pixels near the sensor are updated before the pixels in their shadow, so the pixels in the shadow can be updated in the same sweep. Specifically, we first update the pixels according to their distance to the sensor position. First, the pixel at which the sensor is placed itself, then the \"diamond\" around it, then the next layer etc., like this:</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#gibbs-sampling-part-3-block-gibbs","title":"Gibbs sampling, part 3: block Gibbs\u00b6","text":"<p>One way of dealing with correlated variables in Gibbs sampling is to update not a single variable at once, but a set of variables. In particular, we will update 2x2 blocks of pixels at once. For the Gibbs update of each 2x2 block, we will consider all 16 possible options for the placement of walls in that block. Intuitively, this helps because often we want to do something like move a wall \"back\" by one pixel, which is less likely to happen in independent Gibbs updates.</p> <p>Let's first generate all possible values for an <code>n x n</code> block</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#fully-interactive-version","title":"Fully interactive version\u00b6","text":"<p>You can play around with the setup and the block Gibbs sampler using the interactive widget below. Usage instructions are at the bottom.</p>"},{"location":"cookbook/inactive/inference/mapping_tutorial.html#usage-instructions","title":"Usage instructions\u00b6","text":"<ul> <li>draw walls (ground truth) by clicking pixels or clicking and dragging</li> <li>remove individual walls by clicking the pixel again</li> <li>remove all walls by clicking \"Clear walls\"</li> <li>clear the inference result by clicking \"Clear inferred walls\"</li> <li>use the sliders to update the parameters of the model</li> <li>click and drag the little circle to change the position of the sensor</li> <li>run block Gibbs with the current parameter settings by clicking \"Run inference\" (This may take a few seconds, please be patient.)</li> <li>inspect the estimated posterior probability of a pixel being a wall by selecting \"show average over chain\"</li> <li>inspect individual Gibbs samples by deselecting \"show average over chain\" and using the slider to its right</li> </ul> <p>Note: inference does not update automatically due to the time it takes. You have to click \"Run inference\" to update the inference results.</p>"},{"location":"cookbook/inactive/inference/mcmc.html","title":"MCMC","text":"### What is MCMC? How do I use it? How do I write one? [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChiSym/genjax/blob/main/docs/cookbook/inactive/inference/mcmc.ipynb) In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import gen, normal, pretty\nfrom genjax._src.core.compiler.interpreters.incremental import Diff\n\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp import matplotlib.pyplot as plt  from genjax import ChoiceMapBuilder as C from genjax import gen, normal, pretty from genjax._src.core.compiler.interpreters.incremental import Diff  key = jax.random.key(0) pretty() <p>We can first define a simple model using GenJAX.</p> In\u00a0[3]: Copied! <pre>@gen\ndef model(x):\n    a = normal(0.0, 5.0) @ \"a\"\n    b = normal(0.0, 1.0) @ \"b\"\n    y = normal(a * x + b, 1.0) @ \"y\"\n    return y\n</pre> @gen def model(x):     a = normal(0.0, 5.0) @ \"a\"     b = normal(0.0, 1.0) @ \"b\"     y = normal(a * x + b, 1.0) @ \"y\"     return y <p>Together with observations, this creates a posterior inference problem.</p> In\u00a0[4]: Copied! <pre>obs = C[\"y\"].set(4.0)\n</pre> obs = C[\"y\"].set(4.0) <p>The key ingredient in MCMC is a transition kernel. We can write it in GenJAX as a function that takes a current trace and returns a new trace.</p> <p>Let's write a simple Metropolis-Hastings (MH) kernel.</p> In\u00a0[5]: Copied! <pre>def metropolis_hastings_move(mh_args, key):\n    # For now, we give the kernel the full state of the model, the proposal, and the observations.\n    trace, model, proposal, proposal_args, observations = mh_args\n    model_args = trace.get_args()\n\n    # The core computation is updating a trace, and for that we will call the model's update method.\n    # The update method takes a random key, a trace, and a choice map object, and argument difference objects.\n    argdiffs = Diff.no_change(model_args)\n    proposal_args_forward = (trace, *proposal_args)\n\n    # We sample the proposed changes to the trace.\n    # This is encapsulated in a simple GenJAX generative function.\n    key, subkey = jax.random.split(key)\n    fwd_choices, fwd_weight, _ = proposal.propose(key, proposal_args_forward)\n\n    new_trace, weight, _, discard = model.update(subkey, trace, fwd_choices, argdiffs)\n\n    # Because we are using MH, we don't directly accept the new trace.\n    # Instead, we compute a (log) acceptance ratio \u03b1 and decide whether to accept the new trace, and otherwise keep the old one.\n    proposal_args_backward = (new_trace, *proposal_args)\n    bwd_weight, _ = proposal.assess(discard, proposal_args_backward)\n    \u03b1 = weight - fwd_weight + bwd_weight\n    key, subkey = jax.random.split(key)\n    ret_fun = jax.lax.cond(\n        jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: new_trace, lambda: trace\n    )\n    return (ret_fun, model, proposal, proposal_args, observations), ret_fun\n</pre> def metropolis_hastings_move(mh_args, key):     # For now, we give the kernel the full state of the model, the proposal, and the observations.     trace, model, proposal, proposal_args, observations = mh_args     model_args = trace.get_args()      # The core computation is updating a trace, and for that we will call the model's update method.     # The update method takes a random key, a trace, and a choice map object, and argument difference objects.     argdiffs = Diff.no_change(model_args)     proposal_args_forward = (trace, *proposal_args)      # We sample the proposed changes to the trace.     # This is encapsulated in a simple GenJAX generative function.     key, subkey = jax.random.split(key)     fwd_choices, fwd_weight, _ = proposal.propose(key, proposal_args_forward)      new_trace, weight, _, discard = model.update(subkey, trace, fwd_choices, argdiffs)      # Because we are using MH, we don't directly accept the new trace.     # Instead, we compute a (log) acceptance ratio \u03b1 and decide whether to accept the new trace, and otherwise keep the old one.     proposal_args_backward = (new_trace, *proposal_args)     bwd_weight, _ = proposal.assess(discard, proposal_args_backward)     \u03b1 = weight - fwd_weight + bwd_weight     key, subkey = jax.random.split(key)     ret_fun = jax.lax.cond(         jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: new_trace, lambda: trace     )     return (ret_fun, model, proposal, proposal_args, observations), ret_fun <p>We define a simple proposal distribution for the changes in the trace using a Gaussian drift around the current value of <code>\"a\"</code>.</p> In\u00a0[6]: Copied! <pre>@gen\ndef prop(tr, *_):\n    orig_a = tr.get_choices()[\"a\"]\n    a = normal(orig_a, 1.0) @ \"a\"\n    return a\n</pre> @gen def prop(tr, *_):     orig_a = tr.get_choices()[\"a\"]     a = normal(orig_a, 1.0) @ \"a\"     return a <p>The overall MH algorithm is a loop that repeatedly applies the MH kernel, which can conveniently be written using <code>jax.lax.scan</code>.</p> In\u00a0[7]: Copied! <pre>def mh(trace, model, proposal, proposal_args, observations, key, num_updates):\n    mh_keys = jax.random.split(key, num_updates)\n    last_carry, mh_chain = jax.lax.scan(\n        metropolis_hastings_move,\n        (trace, model, proposal, proposal_args, observations),\n        mh_keys,\n    )\n    return last_carry[0], mh_chain\n</pre> def mh(trace, model, proposal, proposal_args, observations, key, num_updates):     mh_keys = jax.random.split(key, num_updates)     last_carry, mh_chain = jax.lax.scan(         metropolis_hastings_move,         (trace, model, proposal, proposal_args, observations),         mh_keys,     )     return last_carry[0], mh_chain <p>Our custom MH algorithm is a simple wrapper around the MH kernel using our chosen proposal distribution.</p> In\u00a0[8]: Copied! <pre>def custom_mh(trace, model, observations, key, num_updates):\n    return mh(trace, model, prop, (), observations, key, num_updates)\n</pre> def custom_mh(trace, model, observations, key, num_updates):     return mh(trace, model, prop, (), observations, key, num_updates) <p>We now want to create a function run_inference that takes the inference problem, i.e. the model and observations, a random key, and returns traces from the posterior.</p> In\u00a0[9]: Copied! <pre>def run_inference(model, model_args, obs, key, num_samples):\n    key, subkey1, subkey2 = jax.random.split(key, 3)\n    # We sample once from a default importance sampler to get an initial trace.\n    # The particular initial distribution is not important, as the MH kernel will rejuvenate it.\n    tr, _ = model.importance(subkey1, obs, model_args)\n    # We then run our custom Metropolis-Hastings kernel to rejuvenate the trace.\n    rejuvenated_trace, mh_chain = custom_mh(tr, model, obs, subkey2, num_samples)\n    return rejuvenated_trace, mh_chain\n</pre> def run_inference(model, model_args, obs, key, num_samples):     key, subkey1, subkey2 = jax.random.split(key, 3)     # We sample once from a default importance sampler to get an initial trace.     # The particular initial distribution is not important, as the MH kernel will rejuvenate it.     tr, _ = model.importance(subkey1, obs, model_args)     # We then run our custom Metropolis-Hastings kernel to rejuvenate the trace.     rejuvenated_trace, mh_chain = custom_mh(tr, model, obs, subkey2, num_samples)     return rejuvenated_trace, mh_chain <p>We add a little visualization function to validate the results.</p> In\u00a0[10]: Copied! <pre>def validate_mh(mh_chain):\n    a = mh_chain.get_choices()[\"a\"]\n    b = mh_chain.get_choices()[\"b\"]\n    y = mh_chain.get_retval()\n    x = mh_chain.get_args()[0]\n    plt.plot(range(len(y)), a * x + b)\n    plt.plot(range(len(y)), y, color=\"k\")\n    plt.show()\n</pre> def validate_mh(mh_chain):     a = mh_chain.get_choices()[\"a\"]     b = mh_chain.get_choices()[\"b\"]     y = mh_chain.get_retval()     x = mh_chain.get_args()[0]     plt.plot(range(len(y)), a * x + b)     plt.plot(range(len(y)), y, color=\"k\")     plt.show() <p>Testing the inference function.</p> In\u00a0[11]: Copied! <pre>model_args = (5.0,)\nnum_samples = 40000\nkey, subkey = jax.random.split(key)\n_, mh_chain = run_inference(model, model_args, obs, subkey, num_samples)\nvalidate_mh(mh_chain)\n</pre> model_args = (5.0,) num_samples = 40000 key, subkey = jax.random.split(key) _, mh_chain = run_inference(model, model_args, obs, subkey, num_samples) validate_mh(mh_chain)"},{"location":"cookbook/inactive/library_author/dimap_combinator.html","title":"Dimap combinator","text":"In\u00a0[1]: Copied! <pre>import sys\n\nif \"google.colab\" in sys.modules:\n    %pip install --quiet \"genjax[genstudio]\"\n</pre> import sys  if \"google.colab\" in sys.modules:     %pip install --quiet \"genjax[genstudio]\" <p>!! It is only meant to be use by library authors. It is used to implement other combinators such as <code>or_else</code>, and <code>repeat</code>.</p> In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\n\nfrom genjax import gen, normal, pretty\nfrom genjax._src.core.generative import GenerativeFunction\nfrom genjax._src.core.typing import Callable, ScalarFlag\n\nkey = jax.random.key(0)\npretty()\n</pre> import jax import jax.numpy as jnp  from genjax import gen, normal, pretty from genjax._src.core.generative import GenerativeFunction from genjax._src.core.typing import Callable, ScalarFlag  key = jax.random.key(0) pretty() <p>Here's an example of rewriting the <code>OrElseCombinator</code> combinator using <code>contramap</code> and <code>switch</code>.</p> In\u00a0[3]: Copied! <pre>def NewOrElseCombinator(\n    if_gen_fn: GenerativeFunction,\n    else_gen_fn: GenerativeFunction,\n) -&gt; GenerativeFunction:\n    def argument_mapping(b: ScalarFlag, if_args: tuple, else_args: tuple):\n        idx = jnp.array(jnp.logical_not(b), dtype=int)\n        return (idx, if_args, else_args)\n\n    # The `contramap` method is used to map the input arguments to the expected input of the generative function, and then call the switch combinator\n    return if_gen_fn.switch(else_gen_fn).contramap(argument_mapping)\n</pre> def NewOrElseCombinator(     if_gen_fn: GenerativeFunction,     else_gen_fn: GenerativeFunction, ) -&gt; GenerativeFunction:     def argument_mapping(b: ScalarFlag, if_args: tuple, else_args: tuple):         idx = jnp.array(jnp.logical_not(b), dtype=int)         return (idx, if_args, else_args)      # The `contramap` method is used to map the input arguments to the expected input of the generative function, and then call the switch combinator     return if_gen_fn.switch(else_gen_fn).contramap(argument_mapping) <p>To add a version accessible as decorator</p> In\u00a0[4]: Copied! <pre>def new_or_else(\n    else_gen_fn: GenerativeFunction,\n) -&gt; Callable[[GenerativeFunction], GenerativeFunction]:\n    def decorator(if_gen_fn) -&gt; GenerativeFunction:\n        return NewOrElseCombinator(if_gen_fn, else_gen_fn)\n\n    return decorator\n</pre> def new_or_else(     else_gen_fn: GenerativeFunction, ) -&gt; Callable[[GenerativeFunction], GenerativeFunction]:     def decorator(if_gen_fn) -&gt; GenerativeFunction:         return NewOrElseCombinator(if_gen_fn, else_gen_fn)      return decorator <p>To add a version accessible using postfix syntax, one would need to add the following method as part of the <code>GenerativeFunction</code> dataclass in <code>core.py</code>.</p> In\u00a0[5]: Copied! <pre>def postfix_new_or_else(self, gen_fn: \"GenerativeFunction\", /) -&gt; \"GenerativeFunction\":\n    return new_or_else(gen_fn)(self)\n</pre> def postfix_new_or_else(self, gen_fn: \"GenerativeFunction\", /) -&gt; \"GenerativeFunction\":     return new_or_else(gen_fn)(self) <p>Testing the rewritten version on an example</p> In\u00a0[6]: Copied! <pre>@gen\ndef if_model(x):\n    return normal(x, 1.0) @ \"if_value\"\n\n\n@gen\ndef else_model(x):\n    return normal(x, 5.0) @ \"else_value\"\n\n\n@gen\ndef model(toss: bool):\n    return NewOrElseCombinator(if_model, else_model)(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\nkey, subkey = jax.random.split(key)\ntr = jax.jit(model.simulate)(subkey, (True,))\ntr.get_choices()\n</pre> @gen def if_model(x):     return normal(x, 1.0) @ \"if_value\"   @gen def else_model(x):     return normal(x, 5.0) @ \"else_value\"   @gen def model(toss: bool):     return NewOrElseCombinator(if_model, else_model)(toss, (1.0,), (10.0,)) @ \"tossed\"   key, subkey = jax.random.split(key) tr = jax.jit(model.simulate)(subkey, (True,)) tr.get_choices() Out[6]: <p>Checking that the two versions are equivalent on an example</p> In\u00a0[7]: Copied! <pre>@new_or_else(else_model)\n@gen\ndef or_else_model(x):\n    return normal(x, 1.0) @ \"if_value\"\n\n\n@gen\ndef model_v2(toss: bool):\n    return or_else_model(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\n# reusing subkey to get the same result\ntr2 = jax.jit(model_v2.simulate)(subkey, (True,))\ntr.get_choices() == tr2.get_choices()\n</pre> @new_or_else(else_model) @gen def or_else_model(x):     return normal(x, 1.0) @ \"if_value\"   @gen def model_v2(toss: bool):     return or_else_model(toss, (1.0,), (10.0,)) @ \"tossed\"   # reusing subkey to get the same result tr2 = jax.jit(model_v2.simulate)(subkey, (True,)) tr.get_choices() == tr2.get_choices() Out[7]:"},{"location":"cookbook/inactive/library_author/dimap_combinator.html#what-is-this-magic","title":"What is this magic? \u00b6","text":""},{"location":"cookbook/inactive/update/1_importance.html","title":"Importance","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import gen, normal, pretty\n\npretty()\nkey = jax.random.key(0)\n</pre> import jax import jax.numpy as jnp  from genjax import ChoiceMapBuilder as C from genjax import gen, normal, pretty  pretty() key = jax.random.key(0) <p>One of the most important building block of the library is the <code>update</code> method. Before investigating its details, let's look at the more user-friendly version called <code>importance</code>.</p> <p><code>importance</code> is a method on generative functions. It takes a key, constraints in the form of a choicemap, and arguments for the generative function.  Let's first see how we use it and then explain what happened.</p> In\u00a0[2]: Copied! <pre>@gen\ndef model(x):\n    y = normal(x, 1.0) @ \"y\"\n    z = normal(y, 1.0) @ \"z\"\n    return y + z\n\n\nconstraints = C.n()\nargs = (1.0,)\nkey, subkey = jax.random.split(key)\ntr, w = model.importance(subkey, constraints, args)\n</pre> @gen def model(x):     y = normal(x, 1.0) @ \"y\"     z = normal(y, 1.0) @ \"z\"     return y + z   constraints = C.n() args = (1.0,) key, subkey = jax.random.split(key) tr, w = model.importance(subkey, constraints, args) <p>We obtain a pair of a trace <code>tr</code> and a weight <code>w</code>. <code>tr</code> is produced by the model, and its choicemap satisfies the constraints given by <code>constraints</code>.</p> <p>For the choices that are not constrained, they are sampled from the prior distribution given by the model.</p> In\u00a0[3]: Copied! <pre># we expect normal(0., 1.) for y and constant 4. for z\nconstraints = C[\"z\"].set(4.0)\nargs = (0.0,)\n\nkey, subkey = jax.random.split(key)\nkeys = jax.random.split(subkey, 100000)\ntrs, ws = jax.vmap(lambda key: model.importance(key, constraints, args))(keys)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nys = trs.get_choices()[\"y\"]\nzs = trs.get_choices()[\"z\"]\nplt.hist(ys, bins=200, density=True, alpha=0.5, color=\"b\", label=\"ys\")\nplt.scatter(zs, np.zeros_like(zs), color=\"r\", label=\"zs\")\nplt.title(\"Gaussian Distribution of ys and Constant z\")\nplt.legend()\nplt.show()\n</pre> # we expect normal(0., 1.) for y and constant 4. for z constraints = C[\"z\"].set(4.0) args = (0.0,)  key, subkey = jax.random.split(key) keys = jax.random.split(subkey, 100000) trs, ws = jax.vmap(lambda key: model.importance(key, constraints, args))(keys) import matplotlib.pyplot as plt import numpy as np  ys = trs.get_choices()[\"y\"] zs = trs.get_choices()[\"z\"] plt.hist(ys, bins=200, density=True, alpha=0.5, color=\"b\", label=\"ys\") plt.scatter(zs, np.zeros_like(zs), color=\"r\", label=\"zs\") plt.title(\"Gaussian Distribution of ys and Constant z\") plt.legend() plt.show() <p>The weights computed represent the ratio $\\frac{P(y, 4. ; x)}{P(y ; x)}$ where $P(y, z ; x)$ is the joint density given by the model at the argument $x$, and $P(y ; x)$ is the density of the subpart of the model that does not contain the constrained variables. As \"z\" is constrained in our example, it only leaves \"y\".</p> <p>We can easily check this:</p> In\u00a0[4]: Copied! <pre>numerators, _ = jax.vmap(lambda y: model.assess(C[\"y\"].set(y) ^ C[\"z\"].set(4.0), args))(\n    ys\n)\n\ndenominators = trs.get_subtrace(\"y\").get_score()\n\n# yeah, numerical stability of floats implies it's not even exactly equal ...\njnp.allclose(ws, numerators - denominators, atol=1e-7)\n</pre> numerators, _ = jax.vmap(lambda y: model.assess(C[\"y\"].set(y) ^ C[\"z\"].set(4.0), args))(     ys )  denominators = trs.get_subtrace(\"y\").get_score()  # yeah, numerical stability of floats implies it's not even exactly equal ... jnp.allclose(ws, numerators - denominators, atol=1e-7) <pre>/tmp/ipykernel_8407/1554183388.py:1: DeprecationWarning: Call to deprecated method __xor__. (^ is deprecated, please use | or _.merge(...) instead.) -- Deprecated since version 0.8.0.\n  numerators, _ = jax.vmap(lambda y: model.assess(C[\"y\"].set(y) ^ C[\"z\"].set(4.0), args))(\n</pre> Out[4]: <p>More generally the denominator is the joint on the sampled variables (the constraints are not sampled) and Gen has a way to automatically sampled from the generative function obtained by replacing the sampling operations of the constrained addresses by the values of the constraints. For instance in our example it would mean:</p> In\u00a0[5]: Copied! <pre>@gen\ndef constrained_model(x):\n    y = normal(x, 1.0) @ \"y\"\n    z = 4.0\n    return y + z\n</pre> @gen def constrained_model(x):     y = normal(x, 1.0) @ \"y\"     z = 4.0     return y + z <p>Thanks to the factorisation $P(y, z ; x) = P(y ; x)P(z | y ; x)$, the weight <code>ws</code> simplifies to $P(z | y ; x)$. In fact we can easily check it</p> In\u00a0[6]: Copied! <pre>ws == trs.get_subtrace(\"z\").get_score()\n</pre> ws == trs.get_subtrace(\"z\").get_score() Out[6]: <p>And this time the equality is exact as this is how <code>importance</code> computes it. The algebraic simplification $\\frac{P(y ; x)}{P(y ; x)}=1$ is done automatically.</p> <p>Let's review. <code>importance</code> completes a set of constraints given by a partial choicemap to a full choicemap under the model. It also efficiently computes a weight which simplifies to a distribution of the form $P(\\text{sampled } | \\text{ constraints} ; \\text{arguments})$.</p> <p>The complex recursive nature of this formula becomes a bit more apparent in the following example:</p> In\u00a0[7]: Copied! <pre>@gen\ndef fancier_model(x):\n    y1 = normal(x, 1.0) @ \"y1\"\n    z1 = normal(y1, 1.0) @ \"z1\"\n    y2 = normal(z1, 1.0) @ \"y2\"\n    z2 = normal(z1 + y2, 1.0) @ \"z2\"\n    return y2 + z2\n\n\n# if we constraint `z1` to be 4. and `z2` to be 2. we'd get a constrained model as follows:\n\n\n@gen\ndef constrained_fancier_model(x):\n    y1 = normal(x, 1.0) @ \"y1\"\n    z1 = 4.0\n    y2 = normal(z1, 1.0) @ \"y2\"  # note how the sampled `y2` depends on a constraint\n    z2 = 2.0\n    return y1 + z1 + y2 + z2\n</pre> @gen def fancier_model(x):     y1 = normal(x, 1.0) @ \"y1\"     z1 = normal(y1, 1.0) @ \"z1\"     y2 = normal(z1, 1.0) @ \"y2\"     z2 = normal(z1 + y2, 1.0) @ \"z2\"     return y2 + z2   # if we constraint `z1` to be 4. and `z2` to be 2. we'd get a constrained model as follows:   @gen def constrained_fancier_model(x):     y1 = normal(x, 1.0) @ \"y1\"     z1 = 4.0     y2 = normal(z1, 1.0) @ \"y2\"  # note how the sampled `y2` depends on a constraint     z2 = 2.0     return y1 + z1 + y2 + z2 <p>In the case where we give constraints that are a full choicemap for the model, <code>importance</code> returns the same value as <code>assess</code>.</p> In\u00a0[8]: Copied! <pre>args = (1.0,)\nkey, subkey = jax.random.split(key)\ntr = model.simulate(key, args)\n\nconstraints = tr.get_choices()\nnew_tr, w = model.importance(subkey, constraints, args)\nscore, _ = model.assess(constraints, args)\nw == score\n</pre> args = (1.0,) key, subkey = jax.random.split(key) tr = model.simulate(key, args)  constraints = tr.get_choices() new_tr, w = model.importance(subkey, constraints, args) score, _ = model.assess(constraints, args) w == score Out[8]:"},{"location":"cookbook/inactive/update/1_importance.html#intro-to-the-update-logic","title":"Intro to the <code>update</code> logic\u00b6","text":""},{"location":"cookbook/inactive/update/1_importance.html#but-what-does-this-have-to-do-this-importance-sampling","title":"But what does this have to do this importance sampling?\u00b6","text":"<p>What we effectively did was to sample a value <code>y</code> from the distribution <code>constrained_model</code>, which is called a proposal in importance sampling, often noted $q$. We then computed the weight $\\frac{p(y)}{q(y)}$ under some model $p$. Given that we constrained <code>z</code>, an equivalent view is that we observed <code>z</code> and we have a posterior inference problem: we want to approximately sample from the posterior $P(y | z)$ (all for a given argument <code>x</code>).</p> <p>Note that $P(y | z) = \\frac{P(y,z)}{P(z)}$ by Bayes rule. So our fraction $\\frac{P(y, z ; x)}{P(y ; x)}$ for the weight rewrites as $\\frac{P(y | z)P(z)}{q(y)}= P(z)\\frac{p(y)}{q(y)}$ (1).</p> <p>Also remember that the weight $\\frac{dp}{dq}$ for importance comes from the proper weight guarantee, i.e. it satisfies this equation: $$\\forall f.\\mathbb{E}_{y\\sim p}[f(y)]= \\mathbb{E}_{y\\sim q}[\\frac{dp}{dq}(y)f(y)] =  \\frac{1}{p(z)} \\mathbb{E}_{y\\sim q}[w(y)f(y)] $$</p> <p>where in the last step we used (1) and called <code>w</code> the weight computed by <code>importance</code>.</p> <p>By taking $f:= \\lambda y.1$, we derive that $p(z) = \\mathbb{E}_{y\\sim q}[w(y)]$. That is, by sampling from our proposal distribution, we can estimate the marginal $p(z)$. Theferore with the same samples we can estimate any quantity $\\mathbb{E}_{y\\sim p}[f(y)]$ using our estimate of $\\mathbb{E}_{y\\sim q}[w(y)f(y)]$ and our estimate of $p(z)$. That's the essence of self-normalizing importance sampling.</p>"},{"location":"cookbook/inactive/update/1_importance.html#the-special-case-of-the-fully-constrained-choicemap","title":"The special case of the fully constrained choicemap\u00b6","text":""},{"location":"cookbook/inactive/update/2_update.html","title":"Update","text":"In\u00a0[1]: Copied! <pre>import jax\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import gen, normal, pretty\n\npretty()\nkey = jax.random.key(0)\n</pre> import jax  import genjax from genjax import ChoiceMapBuilder as C from genjax import gen, normal, pretty  pretty() key = jax.random.key(0) <p>Let's now see how <code>importance</code> and <code>update</code> are related.</p> <p>The high level is that</p> <ul> <li><code>importance</code> starts with an empty trace, adds the constraints, and then samples the missing values to form a full choicemap under the moek</li> <li><code>update</code> starts with any trace, overwrites those given by the constraints, and samples the missing ones. The missing ones can come from the initial trace possibly having an incomplete choicemap for the model, but also if some constraints force the computation in the model to take a different path which has different sampled values.<ul> <li>It also returns a weight ratio which generalizes the one from <code>importance</code>.</li> <li>It also takes and returns additional data to make it compositional, i.e. <code>update</code> is defined inductively on the structure of the <code>model</code>.</li> </ul> </li> </ul> In\u00a0[2]: Copied! <pre>@gen\ndef model(x):\n    y = normal(x, 1.0) @ \"y\"\n    z = normal(y, 1.0) @ \"z\"\n    return y + z\n</pre> @gen def model(x):     y = normal(x, 1.0) @ \"y\"     z = normal(y, 1.0) @ \"z\"     return y + z <p>Let's first check that <code>update</code> does not do anything if we provide no changes nor constraints.</p> In\u00a0[3]: Copied! <pre>args = (1.0,)\nkey, subkey = jax.random.split(key)\ntr = model.simulate(subkey, args)\n\n\nconstraints = C.n()\nargdiffs = genjax.Diff.no_change(args)\nkey, subkey = jax.random.split(key)\nnew_trace, _, _, _ = model.update(subkey, tr, constraints, argdiffs)\nnew_trace == tr\n</pre> args = (1.0,) key, subkey = jax.random.split(key) tr = model.simulate(subkey, args)   constraints = C.n() argdiffs = genjax.Diff.no_change(args) key, subkey = jax.random.split(key) new_trace, _, _, _ = model.update(subkey, tr, constraints, argdiffs) new_trace == tr Out[3]: <p>Let's now check that it returns a trace where the constraints overwrite the value from the initial trace.</p> In\u00a0[4]: Copied! <pre>key, subkey = jax.random.split(key)\nconstraints = C[\"y\"].set(3.0)\nnew_tr, _, _, _ = model.update(subkey, tr, constraints, argdiffs)\nnew_tr.get_choices()[\"y\"] == 3.0 and new_tr.get_choices()[\"z\"] == tr.get_choices()[\"z\"]\n</pre> key, subkey = jax.random.split(key) constraints = C[\"y\"].set(3.0) new_tr, _, _, _ = model.update(subkey, tr, constraints, argdiffs) new_tr.get_choices()[\"y\"] == 3.0 and new_tr.get_choices()[\"z\"] == tr.get_choices()[\"z\"] Out[4]: <p>Next, let's look at the new input and new outputs compared to <code>importance</code>.</p> In\u00a0[5]: Copied! <pre>args = (1.0,)\nkey, subkey = jax.random.split(key)\ntr = model.simulate(subkey, args)\n\nconstraints = C[\"z\"].set(3.0)\nargdiffs = genjax.Diff.no_change(args)\nnew_trace, weight, ret_diff, discarded = model.update(subkey, tr, constraints, argdiffs)\nargdiffs, ret_diff, discarded\n</pre> args = (1.0,) key, subkey = jax.random.split(key) tr = model.simulate(subkey, args)  constraints = C[\"z\"].set(3.0) argdiffs = genjax.Diff.no_change(args) new_trace, weight, ret_diff, discarded = model.update(subkey, tr, constraints, argdiffs) argdiffs, ret_diff, discarded Out[5]: <p><code>discarded</code> represents a choicemap of the choices that were overwritten by the constraints.</p> In\u00a0[6]: Copied! <pre>discarded[\"z\"] == tr.get_choices()[\"z\"]\n</pre> discarded[\"z\"] == tr.get_choices()[\"z\"] Out[6]: <p><code>argdiffs</code> and <code>ret_diff</code> use a special <code>Diff</code> type which is a simpler analogue of dual-numbers from automatic differentiation (AD). They represent a pair of a primal value and a tangent value. In AD, the primal would be the point at which we're differentiating the function and the dual would be the derivative of the current variable w.r.t. an outside variable.</p> <p>Here, the tangent type is much simpler and Boolean. It either consists of the <code>NoChange()</code> tag or the <code>UnknownChange</code> tag. This is inspired by the literature on incremental computation, and is only there for efficiently computing the density ratio <code>weight</code> by doing algebraic simplifications at compile time as we have briefly seen for the case of <code>importance</code> in the previous cookbook.</p> <p>The idea is that a change in the argument <code>x</code> of the generative function implies a change to the distribution on <code>y</code>. So given a value of <code>y</code>, when we want to compute its density we need to know the value of <code>x</code>. Maybe a change in <code>x</code> would force resampling a different variable <code>y</code>, which would then force a change on the distribution on <code>z</code>. That's the basic idea behind the <code>Diff</code> system and why it needs to be compositional. It's a form of dependency tracking to check which distributions might have changed given a change in arguments, and importantly know which ones didn't change for sure so we can apply some algebraic simplifications.</p> <p>Let's denote a trace by a pair <code>(x,t)</code> of the argument <code>x</code> and the choicemap <code>t</code>. Given a trace <code>(x,t)</code>, a new argument <code>x'</code>, and a map of constraints <code>u</code>, <code>update</code> returns a new trace <code>(x', t')</code> that is consistent with <code>u</code>. The values of choices in <code>t'</code> are copied from <code>t</code> and <code>u</code> (with <code>u</code> taking precedence) or sampled from the internal proposal $q$ (i.e. the equivalent to <code>constrained_model</code> that we have seen in the <code>importance</code> cookbook).</p> <p>The weight $w$ satisfies $$w_{update} = \\frac{p(t' ; x)}{q(t' ; x', t+u).p(t ; x)}$$ where $t+u$ denotes the choicemap where <code>u</code> overwrites the values in <code>t</code> on their common addresses.</p> <p>Let's contrast it with the weight $w$ computed by importance which we can write as $$w_{importance}\\frac{p(t' ; x)}{q(t' ; x, u)}$$ which we can see as the special case of <code>update</code> with an empty starting trace <code>t</code>.</p> <p>One simple thing is that given a trace with choicemap $y$ and a full choicemap $y'$ used as a constraint, <code>update</code> will not need to call the internal proposal <code>q</code> and the weight returned will be $\\frac{p(y')}{p(y)}$. This is a useful quantity that appears in many SMC algorithms, and for instance in the ratio in the MH acceptance ratio $\\alpha$.</p> <p>Given a current value <code>y</code> for the choicemap and proposed value <code>u</code> for a change in some variables of the choicemap, if we call the model <code>p</code> and the proposal <code>q</code> (a kernel which may depend on <code>y</code> and proposes the value <code>u</code>), we write $y':= y+u$. Then, the MH acceptance ratio is defined as $$\\alpha := \\frac{q(y | y')p(y')}{p(y)q(y' | y)} = \\frac{q(y | y')}{q(y' | y)}w_{update}$$</p> <p><code>update</code> has a derived convenient usage. If you have a trace <code>tr</code> and want to do some inference move, e.g. propose a new value for a specific address \"x\". We obtain a new trace with the new value for \"x\" using <code>update</code>:</p> <pre><code>new_tr, _ = model.update(subkey, tr, C[\"x\"].set(new_val_for_x), genjax.Diff.no_change(args))\n</code></pre>"},{"location":"cookbook/inactive/update/2_update.html#compositional-incremental-weight-computation","title":"Compositional Incremental Weight Computation\u00b6","text":""},{"location":"cookbook/inactive/update/2_update.html#now-what-about-the-weight-what-does-it-compute","title":"Now what about the weight? what does it compute?\u00b6","text":""},{"location":"cookbook/inactive/update/2_update.html#what-to-do-with-the-weight-from-update","title":"What to do with the weight from <code>update</code>?\u00b6","text":""},{"location":"cookbook/inactive/update/2_update.html#a-convenient-usage-of-update","title":"A convenient usage of <code>update</code>\u00b6","text":""},{"location":"cookbook/inactive/update/3_speed_gains.html","title":"Incremental","text":"In\u00a0[1]: Copied! <pre>import timeit\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import gen, normal, pretty\nfrom genjax._src.core.pytree import Const\n\npretty()\nkey = jax.random.PRNGKey(0)\n</pre> import timeit  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  import genjax from genjax import ChoiceMapBuilder as C from genjax import gen, normal, pretty from genjax._src.core.pytree import Const  pretty() key = jax.random.PRNGKey(0) <p>In the previous cookbooks, we have seen that <code>importance</code> and <code>update</code>  do algebraic simplifications in the weight ratios that they are computing. Let's first see the difference in the case of <code>importance</code> by testing a naive version of sampling importance resampling (SIR) to one using <code>importance</code>.</p> <p>Let's define a model to be used in the rest ot the cookbook.</p> In\u00a0[2]: Copied! <pre>@gen\ndef model(size_model: Const[int]):\n    size_model = size_model.unwrap()\n    x = normal(0.0, 1.0) @ \"x\"\n    a = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"\n    b = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"\n    c = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"\n    obs = normal(jnp.sum(a) + jnp.sum(b) + jnp.sum(c) + x, 5.0) @ \"obs\"\n    return obs\n</pre> @gen def model(size_model: Const[int]):     size_model = size_model.unwrap()     x = normal(0.0, 1.0) @ \"x\"     a = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"     b = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"     c = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"     obs = normal(jnp.sum(a) + jnp.sum(b) + jnp.sum(c) + x, 5.0) @ \"obs\"     return obs <p>To compare naive SIR to the one using <code>importance</code> and the default proposal, let's write define the default proposal manually:</p> In\u00a0[3]: Copied! <pre>@gen\ndef default_proposal(size_model: Const[int]):\n    size_model = size_model.unwrap()\n    _ = normal(0.0, 1.0) @ \"x\"\n    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"\n    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"\n    _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"\n    return None\n</pre> @gen def default_proposal(size_model: Const[int]):     size_model = size_model.unwrap()     _ = normal(0.0, 1.0) @ \"x\"     _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"     _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"     _ = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"     return None <p>Let's now write SIR with a parameter controlling whether to call the slow or fast version.</p> In\u00a0[4]: Copied! <pre>obs = C[\"obs\"].set(\n    1.0,\n)\n\n\ndef sir(key, N: int, use_fast: bool, size_model):\n    if use_fast:\n        traces, weights = jax.vmap(model.importance, in_axes=(0, None, None))(\n            jax.random.split(key, N), obs, size_model\n        )\n    else:\n        traces = jax.vmap(default_proposal.simulate, in_axes=(0, None))(\n            jax.random.split(key, N), size_model\n        )\n\n        chm_proposal = traces.get_choices()\n        q_weights, _ = jax.vmap(\n            lambda idx: default_proposal.assess(\n                jax.tree_util.tree_map(lambda v: v[idx], chm_proposal), size_model\n            )\n        )(jnp.arange(N))\n\n        chm_model = chm_proposal | C[\"obs\"].set(jnp.ones(N) * obs[\"obs\"])\n        p_weights, _ = jax.vmap(\n            lambda idx: model.assess(\n                jax.tree_util.tree_map(lambda v: v[idx], chm_model), size_model\n            )\n        )(jnp.arange(N))\n\n        weights = p_weights - q_weights\n\n    idx = genjax.categorical.simulate(key, (weights,)).get_retval()\n    samples = traces.get_choices()\n    resampled = jax.tree_util.tree_map(lambda v: v[idx], samples)\n    return resampled\n</pre> obs = C[\"obs\"].set(     1.0, )   def sir(key, N: int, use_fast: bool, size_model):     if use_fast:         traces, weights = jax.vmap(model.importance, in_axes=(0, None, None))(             jax.random.split(key, N), obs, size_model         )     else:         traces = jax.vmap(default_proposal.simulate, in_axes=(0, None))(             jax.random.split(key, N), size_model         )          chm_proposal = traces.get_choices()         q_weights, _ = jax.vmap(             lambda idx: default_proposal.assess(                 jax.tree_util.tree_map(lambda v: v[idx], chm_proposal), size_model             )         )(jnp.arange(N))          chm_model = chm_proposal | C[\"obs\"].set(jnp.ones(N) * obs[\"obs\"])         p_weights, _ = jax.vmap(             lambda idx: model.assess(                 jax.tree_util.tree_map(lambda v: v[idx], chm_model), size_model             )         )(jnp.arange(N))          weights = p_weights - q_weights      idx = genjax.categorical.simulate(key, (weights,)).get_retval()     samples = traces.get_choices()     resampled = jax.tree_util.tree_map(lambda v: v[idx], samples)     return resampled <p>Let's now compare the speed of the 2 versions (beware there's some variance in the estimate, but adding more trials makes the runtime comparison take a while).</p> In\u00a0[5]: Copied! <pre>obs = C[\"obs\"].set(\n    1.0,\n)\nmodel_sizes = [10, 100, 1000]\nN_sir = 100\nnum_trials = 30\nslow_times = []\nfast_times = []\n\nfor model_size in model_sizes:\n    total_time_slow = 0\n    total_time_fast = 0\n    model_size = Const(model_size)\n    obs = C[\"obs\"].set(\n        1.0,\n    )\n    key, subkey = jax.random.split(key)\n\n    # warm up run to trigger jit compilation\n    jitted = jax.jit(sir, static_argnums=(1, 2))\n    jitted(subkey, N_sir, False, (Const(model_sizes),))\n    jitted(subkey, N_sir, True, (Const(model_sizes),))\n\n    # measure time for each algorithm\n    key, subkey = jax.random.split(key)\n    total_time_slow = timeit.timeit(\n        lambda: jitted(subkey, N_sir, False, (Const(model_sizes),)), number=num_trials\n    )\n    total_time_fast = timeit.timeit(\n        lambda: jitted(subkey, N_sir, True, (Const(model_sizes),)), number=num_trials\n    )\n\n    average_time_slow = total_time_slow / num_trials\n    average_time_fast = total_time_fast / num_trials\n    slow_times.append(average_time_slow)\n    fast_times.append(average_time_fast)\n\nplt.plot(model_sizes, [time for time in slow_times], marker=\"o\", label=\"Slow Algorithm\")\nplt.plot(model_sizes, [time for time in fast_times], marker=\"o\", label=\"Fast Algorithm\")\nplt.xscale(\"log\")\nplt.xlabel(\"Argument (n)\")\nplt.ylabel(\"Average Time (seconds)\")\nplt.title(\"Average Execution Time of MH move for different model sizes\")\nplt.grid(True)\nplt.legend()\nplt.show()\n</pre> obs = C[\"obs\"].set(     1.0, ) model_sizes = [10, 100, 1000] N_sir = 100 num_trials = 30 slow_times = [] fast_times = []  for model_size in model_sizes:     total_time_slow = 0     total_time_fast = 0     model_size = Const(model_size)     obs = C[\"obs\"].set(         1.0,     )     key, subkey = jax.random.split(key)      # warm up run to trigger jit compilation     jitted = jax.jit(sir, static_argnums=(1, 2))     jitted(subkey, N_sir, False, (Const(model_sizes),))     jitted(subkey, N_sir, True, (Const(model_sizes),))      # measure time for each algorithm     key, subkey = jax.random.split(key)     total_time_slow = timeit.timeit(         lambda: jitted(subkey, N_sir, False, (Const(model_sizes),)), number=num_trials     )     total_time_fast = timeit.timeit(         lambda: jitted(subkey, N_sir, True, (Const(model_sizes),)), number=num_trials     )      average_time_slow = total_time_slow / num_trials     average_time_fast = total_time_fast / num_trials     slow_times.append(average_time_slow)     fast_times.append(average_time_fast)  plt.plot(model_sizes, [time for time in slow_times], marker=\"o\", label=\"Slow Algorithm\") plt.plot(model_sizes, [time for time in fast_times], marker=\"o\", label=\"Fast Algorithm\") plt.xscale(\"log\") plt.xlabel(\"Argument (n)\") plt.ylabel(\"Average Time (seconds)\") plt.title(\"Average Execution Time of MH move for different model sizes\") plt.grid(True) plt.legend() plt.show() <p>When doing inference with iterative algorithms like MCMC, we often need to make small adjustments to the choice map. We have seen that <code>update</code> can be used to compute part of the MH acceptance ratio. So now let's try to compare two versions of an MH move, one computing naively thee ratio and one using update.</p> <p>Let's create a very basic kernel to rejuvenate the variable \"x\" in an MH algorithm.</p> In\u00a0[6]: Copied! <pre>@gen\ndef rejuv_x(x):\n    x = normal(x, 1.0) @ \"x\"\n    return x\n</pre> @gen def rejuv_x(x):     x = normal(x, 1.0) @ \"x\"     return x <p>Let's now write 2 versions of computing the MH acceptance ratio as well as the MH algorithm to rejuvenate the variable \"x\".</p> In\u00a0[7]: Copied! <pre>def compute_ratio_slow(key, fwd_choice, fwd_weight, model_args, chm):\n    model_weight_old, _ = model.assess(chm, model_args)\n    new_chm = fwd_choice | chm\n    model_weight_new, _ = model.assess(new_chm, model_args)\n    old_x = C[\"x\"].set(chm[\"x\"])\n    proposal_args_backward = (fwd_choice[\"x\"],)\n    bwd_weight, _ = rejuv_x.assess(old_x, proposal_args_backward)\n    \u03b1 = model_weight_new - model_weight_old - fwd_weight + bwd_weight\n    return \u03b1\n\n\ndef compute_ratio_fast(key, fwd_choice, fwd_weight, model_args, trace):\n    argdiffs = genjax.Diff.no_change(model_args)\n    _, weight, _, discard = model.update(key, trace, fwd_choice, argdiffs)\n    proposal_args_backward = (fwd_choice[\"x\"],)\n    bwd_weight, _ = rejuv_x.assess(discard, proposal_args_backward)\n    \u03b1 = weight - fwd_weight + bwd_weight\n    return \u03b1\n\n\ndef metropolis_hastings_move(key, trace, use_fast):\n    model_args = trace.get_args()\n    proposal_args_forward = (trace.get_choices()[\"x\"],)\n    key, subkey = jax.random.split(key)\n    fwd_choice, fwd_weight, _ = rejuv_x.propose(subkey, proposal_args_forward)\n    key, subkey = jax.random.split(key)\n\n    if use_fast:\n        \u03b1 = compute_ratio_fast(subkey, fwd_choice, fwd_weight, model_args, trace)\n    else:\n        chm = trace.get_choices()\n        \u03b1 = compute_ratio_slow(subkey, fwd_choice, fwd_weight, model_args, chm)\n\n    old_choice = C[\"x\"].set(trace.get_choices()[\"x\"])\n    key, subkey = jax.random.split(key)\n    ret_trace = jax.lax.cond(\n        jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: fwd_choice, lambda: old_choice\n    )\n    return ret_trace\n</pre> def compute_ratio_slow(key, fwd_choice, fwd_weight, model_args, chm):     model_weight_old, _ = model.assess(chm, model_args)     new_chm = fwd_choice | chm     model_weight_new, _ = model.assess(new_chm, model_args)     old_x = C[\"x\"].set(chm[\"x\"])     proposal_args_backward = (fwd_choice[\"x\"],)     bwd_weight, _ = rejuv_x.assess(old_x, proposal_args_backward)     \u03b1 = model_weight_new - model_weight_old - fwd_weight + bwd_weight     return \u03b1   def compute_ratio_fast(key, fwd_choice, fwd_weight, model_args, trace):     argdiffs = genjax.Diff.no_change(model_args)     _, weight, _, discard = model.update(key, trace, fwd_choice, argdiffs)     proposal_args_backward = (fwd_choice[\"x\"],)     bwd_weight, _ = rejuv_x.assess(discard, proposal_args_backward)     \u03b1 = weight - fwd_weight + bwd_weight     return \u03b1   def metropolis_hastings_move(key, trace, use_fast):     model_args = trace.get_args()     proposal_args_forward = (trace.get_choices()[\"x\"],)     key, subkey = jax.random.split(key)     fwd_choice, fwd_weight, _ = rejuv_x.propose(subkey, proposal_args_forward)     key, subkey = jax.random.split(key)      if use_fast:         \u03b1 = compute_ratio_fast(subkey, fwd_choice, fwd_weight, model_args, trace)     else:         chm = trace.get_choices()         \u03b1 = compute_ratio_slow(subkey, fwd_choice, fwd_weight, model_args, chm)      old_choice = C[\"x\"].set(trace.get_choices()[\"x\"])     key, subkey = jax.random.split(key)     ret_trace = jax.lax.cond(         jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: fwd_choice, lambda: old_choice     )     return ret_trace <p>Let's measure the performance of each variant.</p> In\u00a0[8]: Copied! <pre>model_sizes = [1000, 10000, 100000, 1000000, 10000000, 100000000]\nslow_times = []\nfast_times = []\n\nfor model_size in model_sizes:\n    total_time_slow = 0\n    total_time_fast = 0\n    num_trials = 5000 if model_size &lt;= 1000000 else 100\n    model_size = Const(model_size)\n    obs = C[\"obs\"].set(\n        1.0,\n    )\n    key, subkey = jax.random.split(key)\n\n    # create a trace from the model of the right size\n    tr, _ = jax.jit(model.importance, static_argnums=(2))(subkey, obs, (model_size,))\n\n    # warm up run to trigger jit compilation\n    jitted = jax.jit(metropolis_hastings_move, static_argnums=(2))\n    jitted(subkey, tr, False)\n    jitted(subkey, tr, True)\n\n    # measure time for each algorithm\n    key, subkey = jax.random.split(key)\n    total_time_slow = timeit.timeit(\n        lambda: jitted(subkey, tr, False), number=num_trials\n    )\n    total_time_fast = timeit.timeit(lambda: jitted(subkey, tr, True), number=num_trials)\n    average_time_slow = total_time_slow / num_trials\n    average_time_fast = total_time_fast / num_trials\n    slow_times.append(average_time_slow)\n    fast_times.append(average_time_fast)\n</pre> model_sizes = [1000, 10000, 100000, 1000000, 10000000, 100000000] slow_times = [] fast_times = []  for model_size in model_sizes:     total_time_slow = 0     total_time_fast = 0     num_trials = 5000 if model_size &lt;= 1000000 else 100     model_size = Const(model_size)     obs = C[\"obs\"].set(         1.0,     )     key, subkey = jax.random.split(key)      # create a trace from the model of the right size     tr, _ = jax.jit(model.importance, static_argnums=(2))(subkey, obs, (model_size,))      # warm up run to trigger jit compilation     jitted = jax.jit(metropolis_hastings_move, static_argnums=(2))     jitted(subkey, tr, False)     jitted(subkey, tr, True)      # measure time for each algorithm     key, subkey = jax.random.split(key)     total_time_slow = timeit.timeit(         lambda: jitted(subkey, tr, False), number=num_trials     )     total_time_fast = timeit.timeit(lambda: jitted(subkey, tr, True), number=num_trials)     average_time_slow = total_time_slow / num_trials     average_time_fast = total_time_fast / num_trials     slow_times.append(average_time_slow)     fast_times.append(average_time_fast) <p>Plotting the results.</p> In\u00a0[9]: Copied! <pre>plt.figure(figsize=(20, 5))\n\n# First half of the values\nplt.subplot(1, 2, 1)\nplt.plot(\n    model_sizes[: len(model_sizes) // 2],\n    [time * 1000 for time in slow_times[: len(slow_times) // 2]],\n    marker=\"o\",\n    label=\"No incremental computation\",\n)\nplt.plot(\n    model_sizes[: len(model_sizes) // 2],\n    [time * 1000 for time in fast_times[: len(fast_times) // 2]],\n    marker=\"o\",\n    label=\"Default incremental computation\",\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Argument (n)\")\nplt.ylabel(\"Average Time (milliseconds)\")\nplt.title(\"Average Execution Time of MH move for different model sizes (First Half)\")\nplt.grid(True)\nplt.legend()\n\n# Second half of the values\nplt.subplot(1, 2, 2)\nplt.plot(\n    model_sizes[len(model_sizes) // 2 :],\n    [time * 1000 for time in slow_times[len(slow_times) // 2 :]],\n    marker=\"o\",\n    label=\"No incremental computation\",\n)\nplt.plot(\n    model_sizes[len(model_sizes) // 2 :],\n    [time * 1000 for time in fast_times[len(fast_times) // 2 :]],\n    marker=\"o\",\n    label=\"Default incremental computation\",\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Argument (n)\")\nplt.ylabel(\"Average Time (milliseconds)\")\nplt.title(\"Average Execution Time of MH move for different model sizes (Second Half)\")\nplt.grid(True)\nplt.legend()\n\nplt.show()\n</pre> plt.figure(figsize=(20, 5))  # First half of the values plt.subplot(1, 2, 1) plt.plot(     model_sizes[: len(model_sizes) // 2],     [time * 1000 for time in slow_times[: len(slow_times) // 2]],     marker=\"o\",     label=\"No incremental computation\", ) plt.plot(     model_sizes[: len(model_sizes) // 2],     [time * 1000 for time in fast_times[: len(fast_times) // 2]],     marker=\"o\",     label=\"Default incremental computation\", ) plt.xscale(\"log\") plt.xlabel(\"Argument (n)\") plt.ylabel(\"Average Time (milliseconds)\") plt.title(\"Average Execution Time of MH move for different model sizes (First Half)\") plt.grid(True) plt.legend()  # Second half of the values plt.subplot(1, 2, 2) plt.plot(     model_sizes[len(model_sizes) // 2 :],     [time * 1000 for time in slow_times[len(slow_times) // 2 :]],     marker=\"o\",     label=\"No incremental computation\", ) plt.plot(     model_sizes[len(model_sizes) // 2 :],     [time * 1000 for time in fast_times[len(fast_times) // 2 :]],     marker=\"o\",     label=\"Default incremental computation\", ) plt.xscale(\"log\") plt.xlabel(\"Argument (n)\") plt.ylabel(\"Average Time (milliseconds)\") plt.title(\"Average Execution Time of MH move for different model sizes (Second Half)\") plt.grid(True) plt.legend()  plt.show()"},{"location":"cookbook/inactive/update/3_speed_gains.html#compute-gains-via-incremental-computation-or-how-to-not-compute-log-pdfs","title":"Compute gains via incremental computation or how to not compute log pdfs\u00b6","text":""},{"location":"cookbook/inactive/update/4_index_request.html","title":"IndexRequest","text":"In\u00a0[1]: Copied! <pre>import timeit\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import (\n    IndexRequest,\n    StaticRequest,\n    Update,\n    gen,\n    normal,\n    pretty,\n)\nfrom genjax._src.core.pytree import Const\n\npretty()\nkey = jax.random.key(0)\n</pre> import timeit  import jax import jax.numpy as jnp import matplotlib.pyplot as plt  import genjax from genjax import ChoiceMapBuilder as C from genjax import (     IndexRequest,     StaticRequest,     Update,     gen,     normal,     pretty, ) from genjax._src.core.pytree import Const  pretty() key = jax.random.key(0) <p>As we discussed in the previous cookbook entries, a main point of <code>update</code> is to be used for incremental computation: <code>update</code> performs algebraic simplifications of the logpdf-ratios computed in the weight that it returns. This is tracked through the <code>Diff</code> system.</p> <p>A limitation of the current automation is that if an address \"x\" has a tensor value, and any index of \"x\" changes, the system will consider that \"x\" has changed without capturing a finer description of what exactly changed.</p> <p>However, we can manually specify how something has changed in a more specific way.</p> In\u00a0[2]: Copied! <pre>@gen\ndef model(size_model_const: Const[int]):\n    size_model = size_model_const.unwrap()\n    x = normal(0.0, 1.0) @ \"x\"\n    a = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"\n    b = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"\n    c = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"\n    obs = normal(jnp.sum(a) + jnp.sum(b) + jnp.sum(c) + x, 5.0) @ \"obs\"\n    return obs\n</pre> @gen def model(size_model_const: Const[int]):     size_model = size_model_const.unwrap()     x = normal(0.0, 1.0) @ \"x\"     a = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"a\"     b = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"b\"     c = normal.vmap()(jnp.zeros(size_model), jnp.ones(size_model)) @ \"c\"     obs = normal(jnp.sum(a) + jnp.sum(b) + jnp.sum(c) + x, 5.0) @ \"obs\"     return obs <p>Let's create a trace from our model.</p> In\u00a0[3]: Copied! <pre>obs = C[\"obs\"].set(\n    1.0,\n)\nsize_model = 10000\nargs = (Const(size_model),)\nkey, subkey = jax.random.split(key)\ntr, _ = model.importance(subkey, obs, args)\n</pre> obs = C[\"obs\"].set(     1.0, ) size_model = 10000 args = (Const(size_model),) key, subkey = jax.random.split(key) tr, _ = model.importance(subkey, obs, args) <p>Let's first see an equivalent way to perform do what <code>update</code> does. Just like <code>update</code> generalizes <code>importance</code>, there is yet another more general interface, <code>edit</code>, which generalizes <code>update</code>.</p> <p>We will go into the details of <code>edit</code> in a follow up cookbook. For now, let's see the equivalent of <code>update</code> using <code>edit</code>. For this, we introduce a <code>Request</code> to change the trace. <code>edit</code> will then answer the <code>Request</code> and change the trace following the logic of the request. To mimick <code>update</code>, we will perform an <code>Update</code> request.</p> In\u00a0[4]: Copied! <pre>change_in_value_for_a = jnp.ones(size_model)\n\n# usual update\nconstraints = C[\"a\"].set(change_in_value_for_a)\nargdiffs = genjax.Diff.no_change(args)\nkey, subkey = jax.random.split(key)\nnew_tr1, _, _, _ = tr.update(subkey, constraints, argdiffs)\n\n# update using `Request`\nval = C.v(change_in_value_for_a)\nrequest = StaticRequest({\"a\": Update(val)})\nkey, subkey = jax.random.split(key)\nnew_tr2, _, _, _ = request.edit(subkey, tr, args)\n\n# comparing the values of both choicemaps after the update\njax.tree_util.tree_all(\n    jax.tree.map(jnp.allclose, new_tr1.get_choices(), new_tr2.get_choices())\n)\n</pre> change_in_value_for_a = jnp.ones(size_model)  # usual update constraints = C[\"a\"].set(change_in_value_for_a) argdiffs = genjax.Diff.no_change(args) key, subkey = jax.random.split(key) new_tr1, _, _, _ = tr.update(subkey, constraints, argdiffs)  # update using `Request` val = C.v(change_in_value_for_a) request = StaticRequest({\"a\": Update(val)}) key, subkey = jax.random.split(key) new_tr2, _, _, _ = request.edit(subkey, tr, args)  # comparing the values of both choicemaps after the update jax.tree_util.tree_all(     jax.tree.map(jnp.allclose, new_tr1.get_choices(), new_tr2.get_choices()) ) Out[4]: <p>Now let's see how we can efficiently change the value of \"a\" at a specific index. For that, we create a more specific <code>Request</code> called an <code>IndexRequest</code>. This request expects another request for what to do at the given index.</p> In\u00a0[5]: Copied! <pre>request = StaticRequest({\"a\": IndexRequest(jnp.array(3), Update(C.v(42.0)))})\n\nkey, subkey = jax.random.split(key)\nnew_tr, _, _, _ = request.edit(subkey, tr, args)\n\n# Checking we only made one change by checking that only one value in the choicemap is 42\njnp.sum(new_tr.get_choices()[\"a\"] == 42.0) == 1\n</pre> request = StaticRequest({\"a\": IndexRequest(jnp.array(3), Update(C.v(42.0)))})  key, subkey = jax.random.split(key) new_tr, _, _, _ = request.edit(subkey, tr, args)  # Checking we only made one change by checking that only one value in the choicemap is 42 jnp.sum(new_tr.get_choices()[\"a\"] == 42.0) == 1 Out[5]: <p>Now, let's compare the 3 options: naive density ratio computation vs <code>update</code> vs <code>IndexRequest</code>. For this, we will do a comparison of doing an MH move on a specific variable in the model as we did in the previous cookbook, but this time for a specific index of the traced value \"a\". We will also compare</p> In\u00a0[6]: Copied! <pre>IDX_WHERE_CHANGE_A = 3\n\n\n@gen\ndef rejuv_a(a):\n    a = normal(a, 1.0) @ \"a\"\n    return a\n\n\ndef compute_ratio_slow(key, fwd_choice, fwd_weight, model_args, chm):\n    model_weight_old, _ = model.assess(chm, model_args)\n    new_a = chm[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"])\n    new_chm = C[\"a\"].set(new_a) | chm\n    model_weight_new, _ = model.assess(new_chm, model_args)\n\n    old_a = C[\"a\"].set(chm[\"a\", IDX_WHERE_CHANGE_A])\n    proposal_args_backward = (fwd_choice[\"a\"],)\n    bwd_weight, _ = rejuv_a.assess(old_a, proposal_args_backward)\n    \u03b1 = model_weight_new - model_weight_old - fwd_weight + bwd_weight\n    return \u03b1\n\n\ndef compute_ratio_fast(key, fwd_choice, fwd_weight, model_args, trace):\n    argdiffs = genjax.Diff.no_change(model_args)\n    constraint = C[\"a\"].set(\n        trace.get_choices()[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"])\n    )\n    _, weight, _, discard = model.update(key, trace, constraint, argdiffs)\n    proposal_args_backward = (fwd_choice[\"a\"],)\n    bwd_weight, _ = rejuv_a.assess(\n        C[\"a\"].set(discard[\"a\", IDX_WHERE_CHANGE_A]), proposal_args_backward\n    )\n    \u03b1 = weight - fwd_weight + bwd_weight\n    return \u03b1\n\n\ndef compute_ratio_very_fast(key, fwd_choice, fwd_weight, model_args, trace):\n    request = StaticRequest({\n        \"a\": IndexRequest(jnp.array(IDX_WHERE_CHANGE_A), Update(C.v(fwd_choice[\"a\"])))\n    })\n    _, weight, _, _ = request.edit(key, trace, model_args)\n    proposal_args_backward = (fwd_choice[\"a\"],)\n    bwd_weight, _ = rejuv_a.assess(\n        C[\"a\"].set(trace.get_choices()[\"a\", IDX_WHERE_CHANGE_A]), proposal_args_backward\n    )\n    \u03b1 = weight - fwd_weight + bwd_weight\n    return \u03b1\n\n\ndef metropolis_hastings_move(key, trace, which_move):\n    model_args = trace.get_args()\n    proposal_args_forward = (trace.get_choices()[\"a\", IDX_WHERE_CHANGE_A],)\n    key, subkey = jax.random.split(key)\n    fwd_choice, fwd_weight, _ = rejuv_a.propose(subkey, proposal_args_forward)\n    key, subkey = jax.random.split(key)\n\n    if which_move == 0:\n        chm = trace.get_choices()\n        \u03b1 = compute_ratio_slow(subkey, fwd_choice, fwd_weight, model_args, chm)\n    elif which_move == 1:\n        \u03b1 = compute_ratio_fast(subkey, fwd_choice, fwd_weight, model_args, trace)\n    else:\n        \u03b1 = compute_ratio_very_fast(subkey, fwd_choice, fwd_weight, model_args, trace)\n\n    old_chm = C[\"a\"].set(trace.get_choices()[\"a\"])\n    new_chm = C[\"a\"].set(old_chm[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"]))\n    key, subkey = jax.random.split(key)\n    ret_chm = jax.lax.cond(\n        jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: new_chm, lambda: old_chm\n    )\n    return ret_chm\n</pre> IDX_WHERE_CHANGE_A = 3   @gen def rejuv_a(a):     a = normal(a, 1.0) @ \"a\"     return a   def compute_ratio_slow(key, fwd_choice, fwd_weight, model_args, chm):     model_weight_old, _ = model.assess(chm, model_args)     new_a = chm[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"])     new_chm = C[\"a\"].set(new_a) | chm     model_weight_new, _ = model.assess(new_chm, model_args)      old_a = C[\"a\"].set(chm[\"a\", IDX_WHERE_CHANGE_A])     proposal_args_backward = (fwd_choice[\"a\"],)     bwd_weight, _ = rejuv_a.assess(old_a, proposal_args_backward)     \u03b1 = model_weight_new - model_weight_old - fwd_weight + bwd_weight     return \u03b1   def compute_ratio_fast(key, fwd_choice, fwd_weight, model_args, trace):     argdiffs = genjax.Diff.no_change(model_args)     constraint = C[\"a\"].set(         trace.get_choices()[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"])     )     _, weight, _, discard = model.update(key, trace, constraint, argdiffs)     proposal_args_backward = (fwd_choice[\"a\"],)     bwd_weight, _ = rejuv_a.assess(         C[\"a\"].set(discard[\"a\", IDX_WHERE_CHANGE_A]), proposal_args_backward     )     \u03b1 = weight - fwd_weight + bwd_weight     return \u03b1   def compute_ratio_very_fast(key, fwd_choice, fwd_weight, model_args, trace):     request = StaticRequest({         \"a\": IndexRequest(jnp.array(IDX_WHERE_CHANGE_A), Update(C.v(fwd_choice[\"a\"])))     })     _, weight, _, _ = request.edit(key, trace, model_args)     proposal_args_backward = (fwd_choice[\"a\"],)     bwd_weight, _ = rejuv_a.assess(         C[\"a\"].set(trace.get_choices()[\"a\", IDX_WHERE_CHANGE_A]), proposal_args_backward     )     \u03b1 = weight - fwd_weight + bwd_weight     return \u03b1   def metropolis_hastings_move(key, trace, which_move):     model_args = trace.get_args()     proposal_args_forward = (trace.get_choices()[\"a\", IDX_WHERE_CHANGE_A],)     key, subkey = jax.random.split(key)     fwd_choice, fwd_weight, _ = rejuv_a.propose(subkey, proposal_args_forward)     key, subkey = jax.random.split(key)      if which_move == 0:         chm = trace.get_choices()         \u03b1 = compute_ratio_slow(subkey, fwd_choice, fwd_weight, model_args, chm)     elif which_move == 1:         \u03b1 = compute_ratio_fast(subkey, fwd_choice, fwd_weight, model_args, trace)     else:         \u03b1 = compute_ratio_very_fast(subkey, fwd_choice, fwd_weight, model_args, trace)      old_chm = C[\"a\"].set(trace.get_choices()[\"a\"])     new_chm = C[\"a\"].set(old_chm[\"a\"].at[IDX_WHERE_CHANGE_A].set(fwd_choice[\"a\"]))     key, subkey = jax.random.split(key)     ret_chm = jax.lax.cond(         jnp.log(jax.random.uniform(subkey)) &lt; \u03b1, lambda: new_chm, lambda: old_chm     )     return ret_chm In\u00a0[7]: Copied! <pre>model_sizes = [1000, 10000, 100000, 1000000, 10000000, 100000000]\nslow_times = []\nfast_times = []\nvery_fast_times = []\n\n\nfor model_size in model_sizes:\n    total_time_slow = 0\n    total_time_fast = 0\n    total_time_very_fast = 0\n    num_trials = 10000 if model_size &lt;= 1000000 else 200\n    model_size = Const(model_size)\n    obs = C[\"obs\"].set(\n        1.0,\n    )\n    key, subkey = jax.random.split(key)\n\n    # create a trace from the model of the right size\n    tr, _ = jax.jit(model.importance, static_argnums=(2))(subkey, obs, (model_size,))\n\n    # warm up run to trigger jit compilation\n    jitted = jax.jit(metropolis_hastings_move, static_argnums=(2))\n    jitted(subkey, tr, 0)\n    jitted(subkey, tr, 1)\n    jitted(subkey, tr, 2)\n\n    # measure time for each algorithm\n    total_time_slow = timeit.timeit(lambda: jitted(subkey, tr, 0), number=num_trials)\n    total_time_fast = timeit.timeit(lambda: jitted(subkey, tr, 1), number=num_trials)\n    total_time_very_fast = timeit.timeit(\n        lambda: jitted(subkey, tr, 2), number=num_trials\n    )\n    average_time_slow = total_time_slow / num_trials\n    average_time_fast = total_time_fast / num_trials\n    average_time_very_fast = total_time_very_fast / num_trials\n    slow_times.append(average_time_slow)\n    fast_times.append(average_time_fast)\n    very_fast_times.append(average_time_very_fast)\n</pre> model_sizes = [1000, 10000, 100000, 1000000, 10000000, 100000000] slow_times = [] fast_times = [] very_fast_times = []   for model_size in model_sizes:     total_time_slow = 0     total_time_fast = 0     total_time_very_fast = 0     num_trials = 10000 if model_size &lt;= 1000000 else 200     model_size = Const(model_size)     obs = C[\"obs\"].set(         1.0,     )     key, subkey = jax.random.split(key)      # create a trace from the model of the right size     tr, _ = jax.jit(model.importance, static_argnums=(2))(subkey, obs, (model_size,))      # warm up run to trigger jit compilation     jitted = jax.jit(metropolis_hastings_move, static_argnums=(2))     jitted(subkey, tr, 0)     jitted(subkey, tr, 1)     jitted(subkey, tr, 2)      # measure time for each algorithm     total_time_slow = timeit.timeit(lambda: jitted(subkey, tr, 0), number=num_trials)     total_time_fast = timeit.timeit(lambda: jitted(subkey, tr, 1), number=num_trials)     total_time_very_fast = timeit.timeit(         lambda: jitted(subkey, tr, 2), number=num_trials     )     average_time_slow = total_time_slow / num_trials     average_time_fast = total_time_fast / num_trials     average_time_very_fast = total_time_very_fast / num_trials     slow_times.append(average_time_slow)     fast_times.append(average_time_fast)     very_fast_times.append(average_time_very_fast) In\u00a0[8]: Copied! <pre>plt.figure(figsize=(20, 5))\n\n# First half of the values\nplt.subplot(1, 2, 1)\nplt.plot(\n    model_sizes[: len(model_sizes) // 2],\n    [time * 1000 for time in slow_times[: len(slow_times) // 2]],\n    marker=\"o\",\n    label=\"No incremental computation\",\n)\nplt.plot(\n    model_sizes[: len(model_sizes) // 2],\n    [time * 1000 for time in fast_times[: len(fast_times) // 2]],\n    marker=\"o\",\n    label=\"Default incremental computation\",\n)\nplt.plot(\n    model_sizes[: len(model_sizes) // 2],\n    [time * 1000 for time in very_fast_times[: len(very_fast_times) // 2]],\n    marker=\"o\",\n    label=\"Optimized incremental computation\",\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Argument (n)\")\nplt.ylabel(\"Average Time (milliseconds)\")\nplt.title(\"Average Execution Time of MH move for different model sizes (First Half)\")\nplt.grid(True)\nplt.legend()\n\n# Second half of the values\nplt.subplot(1, 2, 2)\nplt.plot(\n    model_sizes[len(model_sizes) // 2 :],\n    [time * 1000 for time in slow_times[len(slow_times) // 2 :]],\n    marker=\"o\",\n    label=\"No incremental computation\",\n)\nplt.plot(\n    model_sizes[len(model_sizes) // 2 :],\n    [time * 1000 for time in fast_times[len(fast_times) // 2 :]],\n    marker=\"o\",\n    label=\"Default incremental computation\",\n)\nplt.plot(\n    model_sizes[len(model_sizes) // 2 :],\n    [time * 1000 for time in very_fast_times[len(very_fast_times) // 2 :]],\n    marker=\"o\",\n    label=\"Optimized incremental computation\",\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Argument (n)\")\nplt.ylabel(\"Average Time (milliseconds)\")\nplt.title(\"Average Execution Time of MH move for different model sizes (Second Half)\")\nplt.grid(True)\nplt.legend()\n\nplt.show()\n</pre> plt.figure(figsize=(20, 5))  # First half of the values plt.subplot(1, 2, 1) plt.plot(     model_sizes[: len(model_sizes) // 2],     [time * 1000 for time in slow_times[: len(slow_times) // 2]],     marker=\"o\",     label=\"No incremental computation\", ) plt.plot(     model_sizes[: len(model_sizes) // 2],     [time * 1000 for time in fast_times[: len(fast_times) // 2]],     marker=\"o\",     label=\"Default incremental computation\", ) plt.plot(     model_sizes[: len(model_sizes) // 2],     [time * 1000 for time in very_fast_times[: len(very_fast_times) // 2]],     marker=\"o\",     label=\"Optimized incremental computation\", ) plt.xscale(\"log\") plt.xlabel(\"Argument (n)\") plt.ylabel(\"Average Time (milliseconds)\") plt.title(\"Average Execution Time of MH move for different model sizes (First Half)\") plt.grid(True) plt.legend()  # Second half of the values plt.subplot(1, 2, 2) plt.plot(     model_sizes[len(model_sizes) // 2 :],     [time * 1000 for time in slow_times[len(slow_times) // 2 :]],     marker=\"o\",     label=\"No incremental computation\", ) plt.plot(     model_sizes[len(model_sizes) // 2 :],     [time * 1000 for time in fast_times[len(fast_times) // 2 :]],     marker=\"o\",     label=\"Default incremental computation\", ) plt.plot(     model_sizes[len(model_sizes) // 2 :],     [time * 1000 for time in very_fast_times[len(very_fast_times) // 2 :]],     marker=\"o\",     label=\"Optimized incremental computation\", ) plt.xscale(\"log\") plt.xlabel(\"Argument (n)\") plt.ylabel(\"Average Time (milliseconds)\") plt.title(\"Average Execution Time of MH move for different model sizes (Second Half)\") plt.grid(True) plt.legend()  plt.show()"},{"location":"cookbook/inactive/update/4_index_request.html#speed-gains-part-2-optimizing-updates-for-vmap","title":"Speed Gains Part 2: Optimizing updates for vmap\u00b6","text":""},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html","title":"Block-Gibbs on Dirichlet Mixture Model","text":"<p>We will now see some of the key ingredients in action in a simple but more realistic setting and write a Dirichlet mixture model in GenJAX.</p> In\u00a0[1]: Copied! <pre>import genstudio.plot as Plot\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\n\nimport genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import categorical, dirichlet, gen, normal, pretty\nfrom genjax._src.core.pytree import Const\n\npretty()\nkey = jax.random.key(0)\n</pre> import genstudio.plot as Plot import jax import jax.numpy as jnp import numpy as np  import genjax from genjax import ChoiceMapBuilder as C from genjax import categorical, dirichlet, gen, normal, pretty from genjax._src.core.pytree import Const  pretty() key = jax.random.key(0) <p>We define the generative model we described above. It has several hyperparameters that are somewhat manually inferred. An extension to the model could instead do inference over these hyperparameters, and fix hyper-hyperparameters instead.</p> In\u00a0[2]: Copied! <pre># Hyper parameters\nPRIOR_VARIANCE = 10.0\nOBS_VARIANCE = 1.0\nN_DATAPOINTS = 5000\nN_CLUSTERS = 40\nALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10))\nPRIOR_MEAN = 50.0\nN_ITER = 50\n\n# Debugging mode\nDEBUG = True\n\n\n# Sub generative functions of the bigger model\n@gen\ndef generate_cluster(mean, var):\n    cluster_mean = normal(mean, var) @ \"mean\"\n    return cluster_mean\n\n\n@gen\ndef generate_cluster_weight(alphas):\n    probs = dirichlet(alphas) @ \"probs\"\n    return probs\n\n\n@gen\ndef generate_datapoint(probs, clusters):\n    idx = categorical(jnp.log(probs)) @ \"idx\"\n    obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"\n    return obs\n\n\n@gen\ndef generate_datapoints(probs, clusters, n_datapoints):\n    idx = categorical(jnp.log(probs), sample_shape=n_datapoints) @ \"idx\"\n    obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"\n    return obs\n\n\n# Main model\n@gen\ndef generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n    clusters = (\n        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n        @ \"clusters\"\n    )\n\n    probs = generate_cluster_weight.inline(\n        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n    )\n\n    datapoints = generate_datapoints(probs, clusters, n_datapoints) @ \"datapoints\"\n\n    return datapoints\n</pre> # Hyper parameters PRIOR_VARIANCE = 10.0 OBS_VARIANCE = 1.0 N_DATAPOINTS = 5000 N_CLUSTERS = 40 ALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10)) PRIOR_MEAN = 50.0 N_ITER = 50  # Debugging mode DEBUG = True   # Sub generative functions of the bigger model @gen def generate_cluster(mean, var):     cluster_mean = normal(mean, var) @ \"mean\"     return cluster_mean   @gen def generate_cluster_weight(alphas):     probs = dirichlet(alphas) @ \"probs\"     return probs   @gen def generate_datapoint(probs, clusters):     idx = categorical(jnp.log(probs)) @ \"idx\"     obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"     return obs   @gen def generate_datapoints(probs, clusters, n_datapoints):     idx = categorical(jnp.log(probs), sample_shape=n_datapoints) @ \"idx\"     obs = normal(clusters[idx], OBS_VARIANCE) @ \"obs\"     return obs   # Main model @gen def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):     clusters = (         generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)         @ \"clusters\"     )      probs = generate_cluster_weight.inline(         alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())     )      datapoints = generate_datapoints(probs, clusters, n_datapoints) @ \"datapoints\"      return datapoints <p>We create some synthetic data to test inference.</p> In\u00a0[3]: Copied! <pre># Generate synthetic data with N_CLUSTERS clusters evenly spaced\npoints_per_cluster = int(N_DATAPOINTS / N_CLUSTERS)\ncluster_indices = jnp.arange(N_CLUSTERS)\noffsets = PRIOR_VARIANCE * (-4 + 8 * cluster_indices / N_CLUSTERS)\n\n# Create keys for each cluster\nkeys = jax.random.split(jax.random.key(0), N_CLUSTERS)\n\n# Generate uniform random points for each cluster\nuniform_points = jax.vmap(lambda k: jax.random.uniform(k, shape=(points_per_cluster,)))(\n    keys\n)\n\n# Add offset and prior mean to each cluster's points\nshifted_points = uniform_points + (PRIOR_MEAN + offsets[:, None])\n\ndatapoints = C[\"datapoints\", \"obs\"].set(shifted_points.reshape(-1))\n</pre> # Generate synthetic data with N_CLUSTERS clusters evenly spaced points_per_cluster = int(N_DATAPOINTS / N_CLUSTERS) cluster_indices = jnp.arange(N_CLUSTERS) offsets = PRIOR_VARIANCE * (-4 + 8 * cluster_indices / N_CLUSTERS)  # Create keys for each cluster keys = jax.random.split(jax.random.key(0), N_CLUSTERS)  # Generate uniform random points for each cluster uniform_points = jax.vmap(lambda k: jax.random.uniform(k, shape=(points_per_cluster,)))(     keys )  # Add offset and prior mean to each cluster's points shifted_points = uniform_points + (PRIOR_MEAN + offsets[:, None])  datapoints = C[\"datapoints\", \"obs\"].set(shifted_points.reshape(-1)) <p>We now write the main inference loop. As we said at the beginning, we do MCMC via Gibbs sampling. Inference therefore consist of a main loop and we evolve a trace over time. The final trace contains a sample from the approximate posterior.</p> In\u00a0[4]: Copied! <pre>def infer(datapoints):\n    key = jax.random.key(32421)\n    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n    key, subkey = jax.random.split(key)\n    initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)\n    constraints = datapoints | initial_weights\n    tr, _ = generate_data.importance(subkey, constraints, args)\n\n    if DEBUG:\n        all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]\n        all_posterior_weights = [tr.get_choices()[\"probs\"]]\n        all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]\n\n        jax.debug.print(\"Initial means: {v}\", v=all_posterior_means[0])\n        jax.debug.print(\"Initial weights: {v}\", v=all_posterior_weights[0])\n\n        for _ in range(N_ITER):\n            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n            key, subkey = jax.random.split(key)\n            tr = jax.jit(update_cluster_means)(subkey, tr)\n            all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])\n\n            # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n            key, subkey = jax.random.split(key)\n            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n            all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])\n\n            # # Gibbs update on `probs`\n            key, subkey = jax.random.split(key)\n            tr = jax.jit(update_cluster_weights)(subkey, tr)\n            all_posterior_weights.append(tr.get_choices()[\"probs\"])\n\n        return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr\n\n    else:\n        # One Gibbs sweep consist of updating each latent variable\n        def update(carry, _):\n            key, tr = carry\n            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n            key, subkey = jax.random.split(key)\n            tr = update_cluster_means(subkey, tr)\n\n            # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n            key, subkey = jax.random.split(key)\n            tr = update_datapoint_assignment(subkey, tr)\n\n            # Gibbs update on `probs`\n            key, subkey = jax.random.split(key)\n            tr = update_cluster_weights(subkey, tr)\n            return (key, tr), None\n\n        # Overall inference performs a fixed number of Gibbs sweeps\n        (key, tr), _ = jax.jit(jax.lax.scan)(update, (key, tr), None, length=N_ITER)\n        return tr\n\n\ndef update_cluster_means(key, trace):\n    # We can update each cluster in parallel\n    # For each cluster, we find the datapoints in that cluster and compute their mean\n    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n    n_clusters = trace.get_args()[0].unwrap()\n    current_means = trace.get_choices()[\"clusters\", \"mean\"]\n\n    # Count number of points per cluster\n    category_counts = jnp.bincount(\n        trace.get_choices()[\"datapoints\", \"idx\"],\n        length=n_clusters,\n        minlength=n_clusters,\n    )\n\n    # Will contain some NaN due to clusters having no datapoint\n    cluster_means = (\n        jax.vmap(\n            lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),\n            in_axes=(0),\n            out_axes=(0),\n        )(jnp.arange(n_clusters))\n        / category_counts\n    )\n\n    # Conjugate update for Normal-iid-Normal distribution\n    # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf\n    # Note that there's a typo in the math for the posterior mean.\n    posterior_means = (\n        PRIOR_VARIANCE\n        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n        * cluster_means\n        + (OBS_VARIANCE / category_counts)\n        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n        * PRIOR_MEAN\n    )\n\n    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n\n    # Gibbs resampling of cluster means\n    key, subkey = jax.random.split(key)\n    new_means = (\n        generate_cluster.vmap()\n        .simulate(key, (posterior_means, posterior_variances))\n        .get_choices()[\"mean\"]\n    )\n\n    # Remove the sampled Nan due to clusters having no datapoint and pick previous mean in that case, i.e. no Gibbs update for them\n    chosen_means = jnp.where(category_counts == 0, current_means, new_means)\n\n    if DEBUG:\n        jax.debug.print(\"Category counts: {v}\", v=category_counts)\n        jax.debug.print(\"Current means: {v}\", v=cluster_means)\n        jax.debug.print(\"Posterior means: {v}\", v=posterior_means)\n        jax.debug.print(fmt=\"Posterior variance: {v}\", v=posterior_variances)\n        jax.debug.print(\"Resampled means: {v}\", v=new_means)\n        jax.debug.print(\"Chosen means: {v}\", v=chosen_means)\n\n    argdiffs = genjax.Diff.no_change(trace.args)\n    new_trace, _, _, _ = trace.update(\n        subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs\n    )\n    return new_trace\n\n\ndef update_datapoint_assignment(key, trace):\n    # We want to update the index for each datapoint, in parallel.\n    # It means we want to resample the i, but instead of being from the prior\n    # P(i | probs), we do it from the local posterior P(i | probs, xs).\n    # We need to do it for all addresses [\"datapoints\", \"idx\", i],\n    # and as these are independent (when conditioned on the rest)\n    # we can resample them in parallel.\n\n    # Conjugate update for a categorical is just exact posterior via enumeration\n    # P(x | y ) = P(x, y) \\ sum_x P(x, y).\n    # P(x | y1, y2) = P(x | y1)\n    # Sampling from\n    # (P(x = 1 | y ), P(x = 2 | y), ...) is the same as\n    # sampling from Categorical(P(x = 1, y), P(x = 2, y))\n    # as the weights need not be normalized\n    # In addition, if the model factorizes as P(x, y1, y2) = P(x, y1)P(y1 | y2),\n    # we can further simplify P(y1 | y2) from the categorical as it does not depend on x. More generally We only need to look at the children and parents of x (\"idx\" in our situation, which are conveniently wrapped in the generate_datapoint generative function).\n\n    def compute_local_density(x, i):\n        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n        clusters = trace.get_choices()[\"clusters\", \"mean\"]\n        probs = trace.get_choices()[\"probs\"]\n        args = (probs, clusters)\n        model_logpdf, _ = generate_datapoint.assess(chm, args)\n        return model_logpdf\n\n    n_clusters = trace.get_args()[0].unwrap()\n    n_datapoints = trace.get_args()[1].unwrap()\n    local_densities = jax.vmap(\n        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n            jnp.arange(n_clusters)\n        )\n    )(jnp.arange(n_datapoints))\n\n    # Conjugate update by sampling from posterior categorical\n    # Note: I think we could've used something like\n    # generate_datapoint.vmap().importance which would perhaps\n    # work in a more general setting but would definitely be slower here.\n    key, subkey = jax.random.split(key)\n    new_datapoint_indexes = genjax.categorical.simulate(\n        key, (local_densities,)\n    ).get_choices()\n    # Gibbs resampling of datapoint assignment to clusters\n    argdiffs = genjax.Diff.no_change(trace.args)\n    new_trace, _, _, _ = trace.update(\n        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n    )\n    return new_trace\n\n\ndef update_cluster_weights(key, trace):\n    # Count number of points per cluster\n    n_clusters = trace.get_args()[0].unwrap()\n    category_counts = jnp.bincount(\n        trace.get_choices()[\"datapoints\", \"idx\"],\n        length=n_clusters,\n        minlength=n_clusters,\n    )\n\n    # Conjugate update for Dirichlet distribution\n    # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial\n    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n\n    # Gibbs resampling of cluster weights\n    key, subkey = jax.random.split(key)\n    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()\n\n    if DEBUG:\n        jax.debug.print(fmt=\"Category counts: {v}\", v=category_counts)\n        jax.debug.print(fmt=\"New alpha: {v}\", v=new_alpha)\n        jax.debug.print(fmt=\"New probs: {v}\", v=new_probs)\n    argdiffs = genjax.Diff.no_change(trace.args)\n    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n    return new_trace\n</pre> def infer(datapoints):     key = jax.random.key(32421)     args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)     key, subkey = jax.random.split(key)     initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)     constraints = datapoints | initial_weights     tr, _ = generate_data.importance(subkey, constraints, args)      if DEBUG:         all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]         all_posterior_weights = [tr.get_choices()[\"probs\"]]         all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]          jax.debug.print(\"Initial means: {v}\", v=all_posterior_means[0])         jax.debug.print(\"Initial weights: {v}\", v=all_posterior_weights[0])          for _ in range(N_ITER):             # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel             key, subkey = jax.random.split(key)             tr = jax.jit(update_cluster_means)(subkey, tr)             all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])              # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel             key, subkey = jax.random.split(key)             tr = jax.jit(update_datapoint_assignment)(subkey, tr)             all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])              # # Gibbs update on `probs`             key, subkey = jax.random.split(key)             tr = jax.jit(update_cluster_weights)(subkey, tr)             all_posterior_weights.append(tr.get_choices()[\"probs\"])          return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr      else:         # One Gibbs sweep consist of updating each latent variable         def update(carry, _):             key, tr = carry             # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel             key, subkey = jax.random.split(key)             tr = update_cluster_means(subkey, tr)              # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel             key, subkey = jax.random.split(key)             tr = update_datapoint_assignment(subkey, tr)              # Gibbs update on `probs`             key, subkey = jax.random.split(key)             tr = update_cluster_weights(subkey, tr)             return (key, tr), None          # Overall inference performs a fixed number of Gibbs sweeps         (key, tr), _ = jax.jit(jax.lax.scan)(update, (key, tr), None, length=N_ITER)         return tr   def update_cluster_means(key, trace):     # We can update each cluster in parallel     # For each cluster, we find the datapoints in that cluster and compute their mean     datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]     datapoints = trace.get_choices()[\"datapoints\", \"obs\"]     n_clusters = trace.get_args()[0].unwrap()     current_means = trace.get_choices()[\"clusters\", \"mean\"]      # Count number of points per cluster     category_counts = jnp.bincount(         trace.get_choices()[\"datapoints\", \"idx\"],         length=n_clusters,         minlength=n_clusters,     )      # Will contain some NaN due to clusters having no datapoint     cluster_means = (         jax.vmap(             lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),             in_axes=(0),             out_axes=(0),         )(jnp.arange(n_clusters))         / category_counts     )      # Conjugate update for Normal-iid-Normal distribution     # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf     # Note that there's a typo in the math for the posterior mean.     posterior_means = (         PRIOR_VARIANCE         / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)         * cluster_means         + (OBS_VARIANCE / category_counts)         / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)         * PRIOR_MEAN     )      posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)      # Gibbs resampling of cluster means     key, subkey = jax.random.split(key)     new_means = (         generate_cluster.vmap()         .simulate(key, (posterior_means, posterior_variances))         .get_choices()[\"mean\"]     )      # Remove the sampled Nan due to clusters having no datapoint and pick previous mean in that case, i.e. no Gibbs update for them     chosen_means = jnp.where(category_counts == 0, current_means, new_means)      if DEBUG:         jax.debug.print(\"Category counts: {v}\", v=category_counts)         jax.debug.print(\"Current means: {v}\", v=cluster_means)         jax.debug.print(\"Posterior means: {v}\", v=posterior_means)         jax.debug.print(fmt=\"Posterior variance: {v}\", v=posterior_variances)         jax.debug.print(\"Resampled means: {v}\", v=new_means)         jax.debug.print(\"Chosen means: {v}\", v=chosen_means)      argdiffs = genjax.Diff.no_change(trace.args)     new_trace, _, _, _ = trace.update(         subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs     )     return new_trace   def update_datapoint_assignment(key, trace):     # We want to update the index for each datapoint, in parallel.     # It means we want to resample the i, but instead of being from the prior     # P(i | probs), we do it from the local posterior P(i | probs, xs).     # We need to do it for all addresses [\"datapoints\", \"idx\", i],     # and as these are independent (when conditioned on the rest)     # we can resample them in parallel.      # Conjugate update for a categorical is just exact posterior via enumeration     # P(x | y ) = P(x, y) \\ sum_x P(x, y).     # P(x | y1, y2) = P(x | y1)     # Sampling from     # (P(x = 1 | y ), P(x = 2 | y), ...) is the same as     # sampling from Categorical(P(x = 1, y), P(x = 2, y))     # as the weights need not be normalized     # In addition, if the model factorizes as P(x, y1, y2) = P(x, y1)P(y1 | y2),     # we can further simplify P(y1 | y2) from the categorical as it does not depend on x. More generally We only need to look at the children and parents of x (\"idx\" in our situation, which are conveniently wrapped in the generate_datapoint generative function).      def compute_local_density(x, i):         datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]         chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)         clusters = trace.get_choices()[\"clusters\", \"mean\"]         probs = trace.get_choices()[\"probs\"]         args = (probs, clusters)         model_logpdf, _ = generate_datapoint.assess(chm, args)         return model_logpdf      n_clusters = trace.get_args()[0].unwrap()     n_datapoints = trace.get_args()[1].unwrap()     local_densities = jax.vmap(         lambda x: jax.vmap(lambda i: compute_local_density(x, i))(             jnp.arange(n_clusters)         )     )(jnp.arange(n_datapoints))      # Conjugate update by sampling from posterior categorical     # Note: I think we could've used something like     # generate_datapoint.vmap().importance which would perhaps     # work in a more general setting but would definitely be slower here.     key, subkey = jax.random.split(key)     new_datapoint_indexes = genjax.categorical.simulate(         key, (local_densities,)     ).get_choices()     # Gibbs resampling of datapoint assignment to clusters     argdiffs = genjax.Diff.no_change(trace.args)     new_trace, _, _, _ = trace.update(         subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs     )     return new_trace   def update_cluster_weights(key, trace):     # Count number of points per cluster     n_clusters = trace.get_args()[0].unwrap()     category_counts = jnp.bincount(         trace.get_choices()[\"datapoints\", \"idx\"],         length=n_clusters,         minlength=n_clusters,     )      # Conjugate update for Dirichlet distribution     # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial     new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts      # Gibbs resampling of cluster weights     key, subkey = jax.random.split(key)     new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()      if DEBUG:         jax.debug.print(fmt=\"Category counts: {v}\", v=category_counts)         jax.debug.print(fmt=\"New alpha: {v}\", v=new_alpha)         jax.debug.print(fmt=\"New probs: {v}\", v=new_probs)     argdiffs = genjax.Diff.no_change(trace.args)     new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)     return new_trace <p>We can now run inference, obtaining the final trace and some intermediate traces for visualizing inference.</p> In\u00a0[5]: Copied! <pre>if DEBUG:\n    (\n        all_posterior_means,\n        all_posterior_weights,\n        all_cluster_assignment,\n        posterior_trace,\n    ) = infer(datapoints)\nelse:\n    posterior_trace = infer(datapoints)\n\nposterior_trace\n</pre> if DEBUG:     (         all_posterior_means,         all_posterior_weights,         all_cluster_assignment,         posterior_trace,     ) = infer(datapoints) else:     posterior_trace = infer(datapoints)  posterior_trace <pre>Initial means: [40.161377 32.067596 43.754623 58.22524  71.114914 48.64424  42.166443\n 49.68216  58.838997 45.826347 67.565094 54.852512 46.537666 61.92414\n 47.47585  37.81533  58.96495  54.572662 53.413666 57.45804  50.388985\n 47.83156  47.369118 60.489502 38.442974 57.56394  61.01972  58.002117\n 63.748646 51.57735  38.8523   61.786335 57.70788  46.753838 62.71544\n 65.316536 53.039482 62.816944 44.667416 52.880417]\nInitial weights: [0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n 0.025 0.025 0.025 0.025]\n</pre> <pre>Category counts: [112 138 113 112 134 132 109 125 137 111 115 131 111 114 120 150 125 133\n 135 124 118 108 134 123 136 122 119 133 108 126 116 127 143 153 126 119\n 111 127 120 150]\nCurrent means: [48.551105 50.543533 49.47744  47.197395 53.045887 50.91979  50.41066\n 48.702236 48.789185 53.10006  48.135967 50.173283 53.305874 50.963715\n 48.833    46.289837 48.820015 48.592518 50.06069  43.21971  49.662766\n 51.40882  49.092323 52.790802 49.188526 51.837326 48.10093  49.705772\n 45.944798 49.341793 47.11504  49.200302 48.26917  49.685734 49.80089\n 49.685    50.09544  48.22027  50.925735 51.197903]\nPosterior variance: [0.00892061 0.00724113 0.00884173 0.00892061 0.00745712 0.00757002\n 0.0091659  0.0079936  0.00729395 0.0090009  0.0086881  0.00762776\n 0.0090009  0.00876424 0.00832639 0.00666222 0.0079936  0.00751315\n 0.00740192 0.00805802 0.0084674  0.00925069 0.00745712 0.00812348\n 0.00734754 0.00819001 0.00839631 0.00751315 0.00925069 0.00793021\n 0.00861326 0.00786782 0.00698812 0.00653168 0.00793021 0.00839631\n 0.0090009  0.00786782 0.00832639 0.00666222]\nPosterior means: [48.5524   50.54314  49.477905 47.199894 53.043613 50.91909  50.410282\n 48.70327  48.790066 53.097267 48.13759  50.173153 53.302895 50.962868\n 48.833973 46.292305 48.820953 48.59358  50.06065  43.225178 49.663048\n 51.407516 49.093    52.788536 49.189125 51.83582  48.102524 49.705994\n 45.948547 49.34231  47.117527 49.20093  48.270374 49.68594  49.80105\n 49.685265 50.095352 48.22167  50.92497  51.1971  ]\nResampled means: [48.55825  50.55419  49.470776 47.21506  53.04305  50.919296 50.41241\n 48.702618 48.78646  53.088394 48.130272 50.174213 53.30334  50.96362\n 48.851543 46.293194 48.818577 48.60128  50.05313  43.226105 49.644917\n 51.40527  49.08746  52.78132  49.196877 51.830276 48.12212  49.710495\n 45.938034 49.34437  47.11272  49.205276 48.27407  49.67587  49.80486\n 49.684532 50.112442 48.228474 50.92309  51.20577 ]\nChosen means: [48.55825  50.55419  49.470776 47.21506  53.04305  50.919296 50.41241\n 48.702618 48.78646  53.088394 48.130272 50.174213 53.30334  50.96362\n 48.851543 46.293194 48.818577 48.60128  50.05313  43.226105 49.644917\n 51.40527  49.08746  52.78132  49.196877 51.830276 48.12212  49.710495\n 45.938034 49.34437  47.11272  49.205276 48.27407  49.67587  49.80486\n 49.684532 50.112442 48.228474 50.92309  51.20577 ]\n</pre> <pre>Category counts: [   9   17    7   15  122   13    8   11   13  155   11   12 1980   13\n    9   56    9   11   12 2180    5   11    9   51    7   20   16   10\n   66    7   28   12   14    8   10   10    9   10   18   16]\nNew alpha: [   9.3125   17.3125    7.3125   15.3125  122.3125   13.3125    8.3125\n   11.3125   13.3125  155.3125   11.3125   12.3125 1980.3125   13.3125\n    9.3125   56.3125    9.3125   11.3125   12.3125 2180.3125    5.3125\n   11.3125    9.3125   51.3125    7.3125   20.3125   16.3125   10.3125\n   66.3125    7.3125   28.3125   12.3125   14.3125    8.3125   10.3125\n   10.3125    9.3125   10.3125   18.3125   16.3125]\nNew probs: [0.00189926 0.00248475 0.0011278  0.00288892 0.0236311  0.00198729\n 0.00118441 0.00294586 0.00234889 0.03112511 0.0016432  0.00269484\n 0.40460277 0.00145802 0.00170202 0.01265933 0.00196078 0.00179546\n 0.00420049 0.43071568 0.00069633 0.00147408 0.00237535 0.01011323\n 0.00069506 0.00471409 0.00409952 0.00223707 0.01044026 0.00110905\n 0.00644143 0.0016642  0.00338145 0.00168727 0.00287666 0.00158927\n 0.00199235 0.00114861 0.00308817 0.00312046]\nCategory counts: [   9   17    7   15  122   13    8   11   13  155   11   12 1980   13\n    9   56    9   11   12 2180    5   11    9   51    7   20   16   10\n   66    7   28   12   14    8   10   10    9   10   18   16]\nCurrent means: [48.3503   51.0182   49.699253 46.7219   57.953323 51.28109  51.09111\n 48.378513 48.29023  58.85107  48.412537 50.050426 73.18892  51.23476\n 48.858047 45.76423  48.422256 48.53585  50.32243  26.954638 49.733826\n 52.249588 49.431244 54.54483  49.46888  52.63449  47.459064 49.56484\n 45.256035 49.20731  46.395695 48.940018 47.703197 50.104774 49.71475\n 49.669403 50.19495  47.578484 51.26888  51.89612 ]\nPosterior variance: [0.1098901  0.05847953 0.14084508 0.06622516 0.00819001 0.07633588\n 0.12345678 0.09009009 0.07633588 0.00644745 0.09009009 0.08264463\n 0.00050503 0.07633588 0.1098901  0.01782531 0.1098901  0.09009009\n 0.08264463 0.00045869 0.19607843 0.09009009 0.1098901  0.01956947\n 0.14084508 0.04975124 0.0621118  0.09900989 0.01512859 0.14084508\n 0.03558719 0.08264463 0.07092199 0.12345678 0.09900989 0.09900989\n 0.1098901  0.09900989 0.05524862 0.0621118 ]\nPosterior means: [48.36843  51.012245 49.703487 46.743614 57.946808 51.27131  51.07764\n 48.393124 48.30328  58.845364 48.426838 50.05001  73.187744 51.225334\n 48.870598 45.77178  48.439598 48.54904  50.319767 26.955692 49.739044\n 52.22932  49.437496 54.53594  49.476357 52.621384 47.474846 49.56914\n 45.26321  49.21847  46.408524 48.94878  47.71949  50.10348  49.717567\n 49.672672 50.19281  47.602455 51.261868 51.884342]\nResampled means: [48.26658  51.035984 49.44021  46.686794 57.94227  51.224907 51.103603\n 48.445908 48.296204 58.846733 48.42595  50.088627 73.18736  51.085728\n 48.872852 45.76648  48.36272  48.377502 50.316067 26.956404 49.55857\n 52.278164 49.283646 54.53155  49.407547 52.575245 47.37057  49.63649\n 45.261692 49.10549  46.400024 48.991364 47.75156  50.06408  49.37957\n 49.885742 50.077755 47.58485  51.334045 51.865147]\nChosen means: [48.26658  51.035984 49.44021  46.686794 57.94227  51.224907 51.103603\n 48.445908 48.296204 58.846733 48.42595  50.088627 73.18736  51.085728\n 48.872852 45.76648  48.36272  48.377502 50.316067 26.956404 49.55857\n 52.278164 49.283646 54.53155  49.407547 52.575245 47.37057  49.63649\n 45.261692 49.10549  46.400024 48.991364 47.75156  50.06408  49.37957\n 49.885742 50.077755 47.58485  51.334045 51.865147]\nCategory counts: [   6   13    4   17  154   13   12   17   13  454    9   16 1499   15\n    7  138   11   10   29 1661    0    9   12  142    2   56   25    8\n  501    4   30    3   12    8   15    7    8    6   19   35]\nNew alpha: [6.3125000e+00 1.3312500e+01 4.3125000e+00 1.7312500e+01 1.5431250e+02\n 1.3312500e+01 1.2312500e+01 1.7312500e+01 1.3312500e+01 4.5431250e+02\n 9.3125000e+00 1.6312500e+01 1.4993125e+03 1.5312500e+01 7.3125000e+00\n 1.3831250e+02 1.1312500e+01 1.0312500e+01 2.9312500e+01 1.6613125e+03\n 3.1250000e-01 9.3125000e+00 1.2312500e+01 1.4231250e+02 2.3125000e+00\n 5.6312500e+01 2.5312500e+01 8.3125000e+00 5.0131250e+02 4.3125000e+00\n 3.0312500e+01 3.3125000e+00 1.2312500e+01 8.3125000e+00 1.5312500e+01\n 7.3125000e+00 8.3125000e+00 6.3125000e+00 1.9312500e+01 3.5312500e+01]\nNew probs: [1.7880501e-03 2.6458497e-03 6.5198482e-04 3.2564767e-03 2.9793091e-02\n 2.4859312e-03 2.1520744e-03 5.6812679e-03 2.1166757e-03 9.0967804e-02\n 2.7477716e-03 3.4865218e-03 2.9469055e-01 2.6578207e-03 1.3730587e-03\n 2.6505841e-02 2.9044235e-03 2.1698035e-03 4.5546470e-03 3.3812034e-01\n 1.4779117e-05 1.9852251e-03 1.5465210e-03 2.8276537e-02 2.2804660e-04\n 1.1101911e-02 3.8824102e-03 2.7555102e-03 1.0261990e-01 5.8655109e-04\n 7.2166594e-03 4.6558920e-04 2.3459627e-03 9.7187527e-04 1.6815295e-03\n 1.5860586e-03 1.0851868e-03 2.1350540e-03 4.3881638e-03 4.3765390e-03]\nCategory counts: [   6   13    4   17  154   13   12   17   13  454    9   16 1499   15\n    7  138   11   10   29 1661    0    9   12  142    2   56   25    8\n  501    4   30    3   12    8   15    7    8    6   19   35]\nCurrent means: [47.81455  51.616352 49.277615 47.094162 57.616882 51.725407 51.252853\n 48.244137 48.34731  61.638596 48.172947 50.797592 77.4957   51.253555\n 49.20161  43.85784  48.182377 47.83187  50.30712  22.795555       nan\n 52.190456 49.17903  54.544834 49.06475  52.970554 47.385933 49.942406\n 40.528355 48.83827  46.231007 49.321194 47.58856  50.83484  49.565285\n 50.272373 50.259727 47.38881  51.33322  51.805378]\nPosterior variance: [1.6393442e-01 7.6335877e-02 2.4390244e-01 5.8479533e-02 6.4892923e-03\n 7.6335877e-02 8.2644626e-02 5.8479533e-02 7.6335877e-02 2.2021581e-03\n 1.0989010e-01 6.2111799e-02 6.6706690e-04 6.6225164e-02 1.4084508e-01\n 7.2411294e-03 9.0090089e-02 9.9009894e-02 3.4364261e-02 6.0201070e-04\n 1.0000000e+01 1.0989010e-01 8.2644626e-02 7.0372974e-03 4.7619051e-01\n 1.7825313e-02 3.9840639e-02 1.2345678e-01 1.9956096e-03 2.4390244e-01\n 3.3222590e-02 3.2258067e-01 8.2644626e-02 1.2345678e-01 6.6225164e-02\n 1.4084508e-01 1.2345678e-01 1.6393442e-01 5.2356020e-02 2.8490029e-02]\nPosterior means: [47.850372 51.60401  49.295235 47.111156 57.611942 51.712234 51.2425\n 48.254406 48.359924 61.63603  48.193027 50.792637 77.49386  51.245255\n 49.212852 43.86229  48.198753 47.853333 50.306065 22.797194       nan\n 52.16639  49.185818 54.541637 49.109283 52.965256 47.39635  49.943115\n 40.530247 48.866604 46.243526 49.343094 47.60849  50.82453  49.568165\n 50.268536 50.25652  47.431614 51.326244 51.800232]\nResampled means: [47.571293 51.561016 49.57103  47.065582 57.60884  51.71842  51.147987\n 48.30018  48.338913 61.638508 48.23913  50.86512  77.4943   51.187626\n 49.08549  43.863163 48.39046  47.856056 50.29154  22.796968       nan\n 52.2755   49.18443  54.541786 49.078922 52.96913  47.357506 49.91127\n 40.52969  48.609577 46.219105 48.567707 47.50982  50.94427  49.646065\n 50.107346 50.30451  47.274677 51.30897  51.84024 ]\nChosen means: [47.571293 51.561016 49.57103  47.065582 57.60884  51.71842  51.147987\n 48.30018  48.338913 61.638508 48.23913  50.86512  77.4943   51.187626\n 49.08549  43.863163 48.39046  47.856056 50.29154  22.796968 49.55857\n 52.2755   49.18443  54.541786 49.078922 52.96913  47.357506 49.91127\n 40.52969  48.609577 46.219105 48.567707 47.50982  50.94427  49.646065\n 50.107346 50.30451  47.274677 51.30897  51.84024 ]\nCategory counts: [  11   13    1   21  211   14   17   30   18  630   18   15 1250   11\n    7  169   16   10   28 1375    0   16   12  163    1   54   19   13\n  692    1   61    1   15    5    6   10    7   14   20   25]\nNew alpha: [1.1312500e+01 1.3312500e+01 1.3125000e+00 2.1312500e+01 2.1131250e+02\n 1.4312500e+01 1.7312500e+01 3.0312500e+01 1.8312500e+01 6.3031250e+02\n 1.8312500e+01 1.5312500e+01 1.2503125e+03 1.1312500e+01 7.3125000e+00\n 1.6931250e+02 1.6312500e+01 1.0312500e+01 2.8312500e+01 1.3753125e+03\n 3.1250000e-01 1.6312500e+01 1.2312500e+01 1.6331250e+02 1.3125000e+00\n 5.4312500e+01 1.9312500e+01 1.3312500e+01 6.9231250e+02 1.3125000e+00\n 6.1312500e+01 1.3125000e+00 1.5312500e+01 5.3125000e+00 6.3125000e+00\n 1.0312500e+01 7.3125000e+00 1.4312500e+01 2.0312500e+01 2.5312500e+01]\nNew probs: [2.1673818e-03 1.7163822e-03 8.1811310e-04 4.2350576e-03 4.2799558e-02\n 2.7764970e-03 3.9303540e-03 5.7810517e-03 2.9422438e-03 1.2634218e-01\n 2.8213260e-03 2.7249898e-03 2.5118956e-01 2.5427374e-03 1.6928094e-03\n 3.7217326e-02 4.6840007e-03 2.8148382e-03 4.7610039e-03 2.7655897e-01\n 2.5362361e-04 3.4750560e-03 2.9044515e-03 2.7890552e-02 2.7338593e-04\n 1.1126963e-02 3.3351232e-03 2.2516837e-03 1.3335088e-01 1.6660364e-04\n 1.3140402e-02 1.4395178e-04 3.4992164e-03 1.5792440e-03 2.1839882e-03\n 2.2756658e-03 1.1726766e-03 2.0768612e-03 4.6465592e-03 3.7363614e-03]\nCategory counts: [  11   13    1   21  211   14   17   30   18  630   18   15 1250   11\n    7  169   16   10   28 1375    0   16   12  163    1   54   19   13\n  692    1   61    1   15    5    6   10    7   14   20   25]\nCurrent means: [47.39785  51.875507 50.21505  47.12551  57.647923 51.44297  50.916122\n 48.02084  48.269955 64.443245 48.291588 50.655987 79.48817  51.132618\n 48.342808 43.86549  47.91075  47.37451  50.184063 20.496754       nan\n 52.266758 49.01793  54.63084  48.333443 52.93877  46.93995  49.607124\n 37.093113 46.40639  46.192722 48.725506 47.59996  50.771637 49.858982\n 50.18091  50.3991   46.710514 51.819206 52.083218]\nPosterior variance: [9.0090089e-02 7.6335877e-02 9.0909088e-01 4.7393363e-02 4.7370913e-03\n 7.0921987e-02 5.8479533e-02 3.3222590e-02 5.5248618e-02 1.5870497e-03\n 5.5248618e-02 6.6225164e-02 7.9993601e-04 9.0090089e-02 1.4084508e-01\n 5.9136604e-03 6.2111799e-02 9.9009894e-02 3.5587188e-02 7.2721986e-04\n 1.0000000e+01 6.2111799e-02 8.2644626e-02 6.1312076e-03 9.0909088e-01\n 1.8484289e-02 5.2356020e-02 7.6335877e-02 1.4448779e-03 9.0909088e-01\n 1.6366612e-02 9.0909088e-01 6.6225164e-02 1.9607843e-01 1.6393442e-01\n 9.9009894e-02 1.4084508e-01 7.0921987e-02 4.9751244e-02 3.9840639e-02]\nPosterior means: [47.421295 51.861187 50.195503 47.139133 57.6443   51.43274  50.910767\n 48.027416 48.279514 64.440956 48.301025 50.651646 79.48581  51.122414\n 48.366146 43.869114 47.92373  47.4005   50.18341  20.498898       nan\n 52.252678 49.026047 54.628002 48.48495  52.933342 46.95597  49.610123\n 37.09498  46.733086 46.19895  48.841373 47.615856 50.756508 49.86129\n 50.17911  50.39348  46.733845 51.810154 52.07492 ]\nResampled means: [47.30411  52.00231  50.446846 47.061993 57.644936 51.527733 50.987686\n 48.058838 48.407818 64.44234  48.312874 50.54396  79.48622  51.04276\n 48.500233 43.873783 47.821304 47.39117  50.199932 20.499535       nan\n 52.259933 48.932796 54.621555 49.038933 52.931484 46.953037 49.53742\n 37.092857 45.87163  46.179974 49.528522 47.7202   50.746693 50.02338\n 50.251102 50.572582 46.671837 51.988567 52.059753]\nChosen means: [47.30411  52.00231  50.446846 47.061993 57.644936 51.527733 50.987686\n 48.058838 48.407818 64.44234  48.312874 50.54396  79.48622  51.04276\n 48.500233 43.873783 47.821304 47.39117  50.199932 20.499535 49.55857\n 52.259933 48.932796 54.621555 49.038933 52.931484 46.953037 49.53742\n 37.092857 45.87163  46.179974 49.528522 47.7202   50.746693 50.02338\n 50.251102 50.572582 46.671837 51.988567 52.059753]\nCategory counts: [   7    9    1   28  323   17   28   31   14  659   16   18 1118   17\n    9  289   18   12   18 1230    0   18   14  154    1   59   15    6\n  720    1   66    1   12    9   11    9    4    8   20   10]\nNew alpha: [7.3125000e+00 9.3125000e+00 1.3125000e+00 2.8312500e+01 3.2331250e+02\n 1.7312500e+01 2.8312500e+01 3.1312500e+01 1.4312500e+01 6.5931250e+02\n 1.6312500e+01 1.8312500e+01 1.1183125e+03 1.7312500e+01 9.3125000e+00\n 2.8931250e+02 1.8312500e+01 1.2312500e+01 1.8312500e+01 1.2303125e+03\n 3.1250000e-01 1.8312500e+01 1.4312500e+01 1.5431250e+02 1.3125000e+00\n 5.9312500e+01 1.5312500e+01 6.3125000e+00 7.2031250e+02 1.3125000e+00\n 6.6312500e+01 1.3125000e+00 1.2312500e+01 9.3125000e+00 1.1312500e+01\n 9.3125000e+00 4.3125000e+00 8.3125000e+00 2.0312500e+01 1.0312500e+01]\nNew probs: [1.0484123e-03 2.4039901e-03 3.2780448e-04 8.9008873e-03 6.3795947e-02\n 4.2834594e-03 5.2645109e-03 8.5770758e-03 2.4766033e-03 1.3452582e-01\n 2.4886839e-03 4.1137994e-03 2.2325648e-01 4.6034381e-03 2.2076806e-03\n 5.7211764e-02 3.2629785e-03 2.6335120e-03 3.9636041e-03 2.4250145e-01\n 2.1505499e-05 2.9059877e-03 1.7649684e-03 2.7178811e-02 4.8947573e-04\n 1.3483511e-02 3.8566892e-03 1.2811716e-03 1.4238575e-01 4.0745517e-04\n 1.2738028e-02 2.7574910e-04 2.9633243e-03 1.9458992e-03 1.9879607e-03\n 1.0513082e-03 4.9322203e-04 1.1878294e-03 4.2131674e-03 1.5200591e-03]\nCategory counts: [   7    9    1   28  323   17   28   31   14  659   16   18 1118   17\n    9  289   18   12   18 1230    0   18   14  154    1   59   15    6\n  720    1   66    1   12    9   11    9    4    8   20   10]\nCurrent means: [47.5665   51.656178 50.594707 46.927044 58.47989  51.27839  50.795383\n 47.922024 48.672974 66.31128  48.6865   50.697826 80.53894  51.196953\n 48.528244 43.01227  47.92112  47.435852 50.304123 19.346678       nan\n 52.62142  49.017117 54.580574 50.893482 52.66872  46.8165   50.146633\n 34.965893 42.873196 46.31348  50.494926 47.79755  50.269093 49.98281\n 49.804783 49.90245  46.301067 51.882484 51.96656 ]\nPosterior variance: [1.4084508e-01 1.0989010e-01 9.0909088e-01 3.5587188e-02 3.0950170e-03\n 5.8479533e-02 3.5587188e-02 3.2154340e-02 7.0921987e-02 1.5172205e-03\n 6.2111799e-02 5.5248618e-02 8.9437439e-04 5.8479533e-02 1.0989010e-01\n 3.4590107e-03 5.5248618e-02 8.2644626e-02 5.5248618e-02 8.1294205e-04\n 1.0000000e+01 5.5248618e-02 7.0921987e-02 6.4892923e-03 9.0909088e-01\n 1.6920473e-02 6.6225164e-02 1.6393442e-01 1.3886960e-03 9.0909088e-01\n 1.5128593e-02 9.0909088e-01 8.2644626e-02 1.0989010e-01 9.0090089e-02\n 1.0989010e-01 2.4390244e-01 1.2345678e-01 4.9751244e-02 9.9009894e-02]\nPosterior means: [47.600773 51.63798  50.540646 46.93798  58.47727  51.270912 50.792553\n 47.928703 48.68239  66.30881  48.69466  50.69397  80.53621  51.189953\n 48.54442  43.01469  47.932606 47.457047 50.302444 19.349169       nan\n 52.60694  49.02409  54.577602 50.81226  52.664207 46.837585 50.144226\n 34.967983 43.521088 46.319054 50.449936 47.815754 50.266136 49.982967\n 49.80693  49.90483  46.346733 51.87312  51.947086]\nResampled means: [47.828007 51.598907 50.26712  46.960922 58.4742   51.2921   50.80302\n 47.98146  48.73236  66.30938  48.726753 50.662422 80.5354   51.12578\n 48.52776  43.012943 47.939507 47.497765 50.337616 19.347212       nan\n 52.545887 48.94067  54.578136 51.107822 52.66197  46.845142 50.30564\n 34.97025  45.91524  46.317657 50.1744   47.79839  50.145996 49.88021\n 49.73512  50.241524 46.441093 51.926933 52.063957]\nChosen means: [47.828007 51.598907 50.26712  46.960922 58.4742   51.2921   50.80302\n 47.98146  48.73236  66.30938  48.726753 50.662422 80.5354   51.12578\n 48.52776  43.012943 47.939507 47.497765 50.337616 19.347212 49.55857\n 52.545887 48.94067  54.578136 51.107822 52.66197  46.845142 50.30564\n 34.97025  45.91524  46.317657 50.1744   47.79839  50.145996 49.88021\n 49.73512  50.241524 46.441093 51.926933 52.063957]\nCategory counts: [   6    6    0   47  373   18   26   36   17  709   11   24 1000   16\n    9  360   14   13   17 1124    0   19   13  168    3   72   16    5\n  745    4   63    3   10   10    5    5    5    5   20    3]\nNew alpha: [6.3125000e+00 6.3125000e+00 3.1250000e-01 4.7312500e+01 3.7331250e+02\n 1.8312500e+01 2.6312500e+01 3.6312500e+01 1.7312500e+01 7.0931250e+02\n 1.1312500e+01 2.4312500e+01 1.0003125e+03 1.6312500e+01 9.3125000e+00\n 3.6031250e+02 1.4312500e+01 1.3312500e+01 1.7312500e+01 1.1243125e+03\n 3.1250000e-01 1.9312500e+01 1.3312500e+01 1.6831250e+02 3.3125000e+00\n 7.2312500e+01 1.6312500e+01 5.3125000e+00 7.4531250e+02 4.3125000e+00\n 6.3312500e+01 3.3125000e+00 1.0312500e+01 1.0312500e+01 5.3125000e+00\n 5.3125000e+00 5.3125000e+00 5.3125000e+00 2.0312500e+01 3.3125000e+00]\nNew probs: [7.1326061e-04 1.0461451e-03 1.0887781e-08 8.2955621e-03 7.9418242e-02\n 3.9928402e-03 4.7319275e-03 6.1478820e-03 3.6614372e-03 1.3740961e-01\n 2.5528162e-03 3.5433522e-03 1.9913024e-01 3.7447643e-03 1.4898101e-03\n 7.3318720e-02 3.2863538e-03 2.6568412e-03 2.8536820e-03 2.2291099e-01\n 1.0118150e-05 4.0252367e-03 4.0440694e-03 3.3246946e-02 3.4518112e-04\n 1.8657612e-02 3.3503885e-03 1.0110930e-03 1.4722630e-01 1.4468585e-03\n 1.0326716e-02 7.4384233e-04 1.2104714e-03 2.9335415e-03 9.4662694e-04\n 1.3073983e-03 1.9155390e-03 2.3673971e-03 3.3520185e-03 6.2791962e-04]\nCategory counts: [   6    6    0   47  373   18   26   36   17  709   11   24 1000   16\n    9  360   14   13   17 1124    0   19   13  168    3   72   16    5\n  745    4   63    3   10   10    5    5    5    5   20    3]\nCurrent means: [47.332397 51.630253       nan 46.729717 59.172085 51.16313  50.84512\n 48.33057  48.83966  67.79423  48.690014 50.25913  81.48343  51.147327\n 48.43475  42.31971  47.59215  47.22457  50.255646 18.492432       nan\n 52.605515 49.396225 54.845768 51.324566 52.432114 46.922245 50.947887\n 33.459164 45.659206 46.20593  50.318787 47.743675 49.9695   49.078297\n 49.833294 50.4654   46.65812  52.149376 52.274017]\nPosterior variance: [1.63934425e-01 1.63934425e-01 1.00000000e+01 2.12314241e-02\n 2.68024649e-03 5.52486181e-02 3.83141749e-02 2.77008321e-02\n 5.84795326e-02 1.41023833e-03 9.00900885e-02 4.14937735e-02\n 9.99900047e-04 6.21117987e-02 1.09890103e-01 2.77700624e-03\n 7.09219873e-02 7.63358772e-02 5.84795326e-02 8.89600604e-04\n 1.00000000e+01 5.23560196e-02 7.63358772e-02 5.94883971e-03\n 3.22580665e-01 1.38696255e-02 6.21117987e-02 1.96078435e-01\n 1.34210172e-03 2.43902445e-01 1.58478618e-02 3.22580665e-01\n 9.90098938e-02 9.90098938e-02 1.96078435e-01 1.96078435e-01\n 1.96078435e-01 1.96078435e-01 4.97512445e-02 3.22580665e-01]\nPosterior means: [47.376125 51.603523       nan 46.73666  59.16963  51.156708 50.84188\n 48.335194 48.846447 67.79172  48.701817 50.258053 81.48028  51.1402\n 48.451954 42.321842 47.60923  47.245754 50.25415  18.495234       nan\n 52.591877 49.400833 54.842884 51.28184  52.428738 46.94136  50.929302\n 33.461384 45.76508  46.21194  50.308506 47.76601  49.9698   49.09637\n 49.836563 50.456276 46.723648 52.138683 52.200665]\nResampled means: [47.49356  51.927406       nan 46.71819  59.171284 51.177567 50.878338\n 48.351284 48.744583 67.79259  48.54787  50.362663 81.47964  51.191933\n 48.401867 42.3215   47.586163 47.22909  50.38704  18.494173       nan\n 52.625637 49.293224 54.84654  51.34701  52.435898 46.98339  50.868553\n 33.461903 46.008972 46.20115  49.932728 47.75515  49.90669  48.81847\n 49.526375 50.49478  46.769543 52.16988  52.354877]\nChosen means: [47.49356  51.927406 50.26712  46.71819  59.171284 51.177567 50.878338\n 48.351284 48.744583 67.79259  48.54787  50.362663 81.47964  51.191933\n 48.401867 42.3215   47.586163 47.22909  50.38704  18.494173 49.55857\n 52.625637 49.293224 54.84654  51.34701  52.435898 46.98339  50.868553\n 33.461903 46.008972 46.20115  49.932728 47.75515  49.90669  48.81847\n 49.526375 50.49478  46.769543 52.16988  52.354877]\nCategory counts: [   1    3    0   44  407   21   21   34   22  704   16   18  921   18\n    6  448   19    8   18 1004    0   11   22  216    1   90   24    4\n  753    9   63    7   11   12    3    3    7   13   15    3]\nNew alpha: [1.3125000e+00 3.3125000e+00 3.1250000e-01 4.4312500e+01 4.0731250e+02\n 2.1312500e+01 2.1312500e+01 3.4312500e+01 2.2312500e+01 7.0431250e+02\n 1.6312500e+01 1.8312500e+01 9.2131250e+02 1.8312500e+01 6.3125000e+00\n 4.4831250e+02 1.9312500e+01 8.3125000e+00 1.8312500e+01 1.0043125e+03\n 3.1250000e-01 1.1312500e+01 2.2312500e+01 2.1631250e+02 1.3125000e+00\n 9.0312500e+01 2.4312500e+01 4.3125000e+00 7.5331250e+02 9.3125000e+00\n 6.3312500e+01 7.3125000e+00 1.1312500e+01 1.2312500e+01 3.3125000e+00\n 3.3125000e+00 7.3125000e+00 1.3312500e+01 1.5312500e+01 3.3125000e+00]\nNew probs: [2.1196724e-04 4.5253025e-04 3.2575281e-05 9.5339343e-03 8.0842450e-02\n 3.0001272e-03 4.3563158e-03 5.6997277e-03 4.6140766e-03 1.3624157e-01\n 2.9122075e-03 5.5064419e-03 1.7984831e-01 2.4481947e-03 1.2753750e-03\n 9.3407750e-02 4.9266312e-03 1.0861974e-03 4.7688335e-03 2.0985861e-01\n 4.8795211e-04 2.4426975e-03 4.3230578e-03 3.9631508e-02 7.3840463e-04\n 2.0077281e-02 4.4969236e-03 4.2944416e-04 1.4620233e-01 2.2138201e-03\n 1.1991677e-02 1.5104315e-03 1.9393002e-03 2.5195284e-03 5.1665300e-04\n 8.5688470e-04 1.5809762e-03 2.1947860e-03 3.8904811e-03 9.3227089e-04]\nCategory counts: [   1    3    0   44  407   21   21   34   22  704   16   18  921   18\n    6  448   19    8   18 1004    0   11   22  216    1   90   24    4\n  753    9   63    7   11   12    3    3    7   13   15    3]\nCurrent means: [46.974678 50.026287       nan 46.330044 60.208897 51.06257  50.896297\n 48.457096 48.689053 69.1429   47.95723  50.31358  82.09974  51.147327\n 48.495136 41.245296 47.42284  46.71911  50.480453 17.532934       nan\n 52.364307 49.29354  55.23518  52.573154 52.56875  46.67065  51.397575\n 31.597698 45.965088 45.92332  49.49292  47.225513 50.091312 48.422256\n 49.780476 50.550938 46.400322 51.988346 51.72092 ]\nPosterior variance: [9.0909088e-01 3.2258067e-01 1.0000000e+01 2.2675738e-02 2.4563989e-03\n 4.7393363e-02 4.7393363e-02 2.9325515e-02 4.5248870e-02 1.4202528e-03\n 6.2111799e-02 5.5248618e-02 1.0856584e-03 5.5248618e-02 1.6393442e-01\n 2.2316447e-03 5.2356020e-02 1.2345678e-01 5.5248618e-02 9.9591678e-04\n 1.0000000e+01 9.0090089e-02 4.5248870e-02 4.6274872e-03 9.0909088e-01\n 1.1098780e-02 4.1493773e-02 2.4390244e-01 1.3278449e-03 1.0989010e-01\n 1.5847862e-02 1.4084508e-01 9.0090089e-02 8.2644626e-02 3.2258067e-01\n 3.2258067e-01 1.4084508e-01 7.6335877e-02 6.6225164e-02 3.2258067e-01]\nPosterior means: [47.24971  50.02544        nan 46.338367 60.20639  51.05753  50.892048\n 48.461624 48.694984 69.14018  47.969917 50.311848 82.096245 51.140987\n 48.519802 41.24725  47.436337 46.759613 50.4778   17.53617        nan\n 52.34301  49.296738 55.232754 52.339233 52.565895 46.684464 51.36349\n 31.60014  46.00943  45.929783 49.50006  47.25051  50.09056  48.473152\n 49.78756  50.543175 46.4278   51.97518  51.66541 ]\nResampled means: [46.191177 49.97323        nan 46.353462 60.205658 51.04557  50.897343\n 48.459435 48.68233  69.139824 47.973633 50.211475 82.0926   51.193787\n 48.434708 41.24564  47.455452 46.851673 50.439686 17.537254       nan\n 52.552547 49.30614  55.232113 51.906845 52.566788 46.657997 51.151287\n 31.599327 46.119785 45.91741  49.408722 47.387657 50.13795  48.686172\n 49.945545 50.40278  46.420628 52.03433  51.47237 ]\nChosen means: [46.191177 49.97323  50.26712  46.353462 60.205658 51.04557  50.897343\n 48.459435 48.68233  69.139824 47.973633 50.211475 82.0926   51.193787\n 48.434708 41.24564  47.455452 46.851673 50.439686 17.537254 49.55857\n 52.552547 49.30614  55.232113 51.906845 52.566788 46.657997 51.151287\n 31.599327 46.119785 45.91741  49.408722 47.387657 50.13795  48.686172\n 49.945545 50.40278  46.420628 52.03433  51.47237 ]\nCategory counts: [  1   4   0  58 455  17  27  37  30 669  19  24 875   9   4 451  27   3\n  23 952   2  21  24 233   2  95  22   1 730  14 106   6  13   7   3   5\n   4   6  13   8]\nNew alpha: [1.312500e+00 4.312500e+00 3.125000e-01 5.831250e+01 4.553125e+02\n 1.731250e+01 2.731250e+01 3.731250e+01 3.031250e+01 6.693125e+02\n 1.931250e+01 2.431250e+01 8.753125e+02 9.312500e+00 4.312500e+00\n 4.513125e+02 2.731250e+01 3.312500e+00 2.331250e+01 9.523125e+02\n 2.312500e+00 2.131250e+01 2.431250e+01 2.333125e+02 2.312500e+00\n 9.531250e+01 2.231250e+01 1.312500e+00 7.303125e+02 1.431250e+01\n 1.063125e+02 6.312500e+00 1.331250e+01 7.312500e+00 3.312500e+00\n 5.312500e+00 4.312500e+00 6.312500e+00 1.331250e+01 8.312500e+00]\nNew probs: [1.12070746e-04 5.61572437e-04 1.39575477e-07 1.21237952e-02\n 8.70418549e-02 4.88283578e-03 5.92409726e-03 8.17592442e-03\n 6.22691307e-03 1.32962987e-01 5.61798224e-03 4.72867256e-03\n 1.65950254e-01 7.79046968e-04 1.38890930e-03 9.07041878e-02\n 5.62213594e-03 8.65971844e-04 4.28505894e-03 1.88465029e-01\n 2.35402054e-04 4.50017350e-03 3.95431463e-03 4.88412604e-02\n 5.88222058e-04 2.07980461e-02 2.95081455e-03 1.99118542e-04\n 1.52881265e-01 2.08916282e-03 2.00091079e-02 1.72951492e-03\n 2.81626149e-03 1.01801055e-03 1.47231622e-04 9.92273679e-04\n 4.46267310e-04 2.30870559e-03 4.69900295e-03 2.37620948e-03]\nCategory counts: [  1   4   0  58 455  17  27  37  30 669  19  24 875   9   4 451  27   3\n  23 952   2  21  24 233   2  95  22   1 730  14 106   6  13   7   3   5\n   4   6  13   8]\nCurrent means: [46.553684 50.509796       nan 45.647263 61.186718 50.98982  50.83519\n 48.41371  48.720894 70.11232  47.65002  50.29746  82.48516  51.558365\n 48.15818  40.010532 47.10794  46.20149  50.904076 17.134108 49.779804\n 52.887093 49.145176 55.557438 53.173378 52.63635  46.29965  50.95144\n 30.572725 45.33523  45.188854 49.480473 47.320698 49.749287 48.55322\n 50.062363 49.96749  46.715595 51.81576  51.938717]\nPosterior variance: [9.09090877e-01 2.43902445e-01 1.00000000e+01 1.72117036e-02\n 2.19731918e-03 5.84795326e-02 3.69003676e-02 2.69541796e-02\n 3.32225896e-02 1.49454491e-03 5.23560196e-02 4.14937735e-02\n 1.14272663e-03 1.09890103e-01 2.43902445e-01 2.21680338e-03\n 3.69003676e-02 3.22580665e-01 4.32900414e-02 1.05030986e-03\n 4.76190507e-01 4.73933630e-02 4.14937735e-02 4.29000426e-03\n 4.76190507e-01 1.05152475e-02 4.52488698e-02 9.09090877e-01\n 1.36967539e-03 7.09219873e-02 9.42507107e-03 1.63934425e-01\n 7.63358772e-02 1.40845075e-01 3.22580665e-01 1.96078435e-01\n 2.43902445e-01 1.63934425e-01 7.63358772e-02 1.23456784e-01]\nPosterior means: [46.86699  50.497364       nan 45.654755 61.184258 50.98403  50.832108\n 48.417984 48.725143 70.109314 47.662327 50.296223 82.48145  51.54124\n 48.203106 40.01275  47.118614 46.324024 50.90016  17.13756  49.790287\n 52.87341  49.14872  55.555054 53.022263 52.633575 46.31639  50.86495\n 30.575388 45.368317 45.193394 49.488987 47.34115  49.752815 48.59989\n 50.061142 49.968285 46.769436 51.8019   51.91478 ]\nResampled means: [47.24992  50.43668        nan 45.64226  61.187237 50.857918 50.85741\n 48.43646  48.787346 70.10977  47.736137 50.21902  82.480835 51.725555\n 48.362053 40.01132  47.12486  46.671288 50.968216 17.138233 48.73583\n 52.894943 49.14806  55.55691  53.2842   52.63431  46.26711  51.45684\n 30.575949 45.299133 45.181717 49.6406   47.395813 49.945343 48.853664\n 49.918262 50.107838 46.749577 51.855827 51.95197 ]\nChosen means: [47.24992  50.43668  50.26712  45.64226  61.187237 50.857918 50.85741\n 48.43646  48.787346 70.10977  47.736137 50.21902  82.480835 51.725555\n 48.362053 40.01132  47.12486  46.671288 50.968216 17.138233 48.73583\n 52.894943 49.14806  55.55691  53.2842   52.63431  46.26711  51.45684\n 30.575949 45.299133 45.181717 49.6406   47.395813 49.945343 48.853664\n 49.918262 50.107838 46.749577 51.855827 51.95197 ]\nCategory counts: [  1   7   0  58 466  21  25  38  26 662  28  23 838   4   3 472  24   4\n  18 879   3  20  20 264   1  95  14   1 746  16 132  12  17   9   1   9\n   2   8  18  15]\nNew alpha: [1.312500e+00 7.312500e+00 3.125000e-01 5.831250e+01 4.663125e+02\n 2.131250e+01 2.531250e+01 3.831250e+01 2.631250e+01 6.623125e+02\n 2.831250e+01 2.331250e+01 8.383125e+02 4.312500e+00 3.312500e+00\n 4.723125e+02 2.431250e+01 4.312500e+00 1.831250e+01 8.793125e+02\n 3.312500e+00 2.031250e+01 2.031250e+01 2.643125e+02 1.312500e+00\n 9.531250e+01 1.431250e+01 1.312500e+00 7.463125e+02 1.631250e+01\n 1.323125e+02 1.231250e+01 1.731250e+01 9.312500e+00 1.312500e+00\n 9.312500e+00 2.312500e+00 8.312500e+00 1.831250e+01 1.531250e+01]\nNew probs: [1.7255213e-04 1.7437905e-03 7.0047790e-05 1.3498584e-02 8.8707238e-02\n 4.2700893e-03 3.6912155e-03 9.9082692e-03 5.0118752e-03 1.2570179e-01\n 4.0599168e-03 4.5133037e-03 1.7171872e-01 1.4297430e-03 4.0831853e-04\n 9.4778776e-02 4.4669481e-03 6.7734107e-04 4.7777286e-03 1.8098311e-01\n 8.7095948e-04 2.7769036e-03 4.8450013e-03 5.0776511e-02 7.5217139e-04\n 1.8370463e-02 1.5096143e-03 2.7587797e-04 1.5220124e-01 4.0762364e-03\n 2.5588537e-02 1.8118835e-03 3.4500062e-03 1.6716123e-03 4.7863432e-05\n 1.5112355e-03 1.2982346e-04 1.3468731e-03 4.7776964e-03 2.6198598e-03]\nCategory counts: [  1   7   0  58 466  21  25  38  26 662  28  23 838   4   3 472  24   4\n  18 879   3  20  20 264   1  95  14   1 746  16 132  12  17   9   1   9\n   2   8  18  15]\nCurrent means: [46.96401  50.630222       nan 45.33648  61.732082 50.724968 50.865448\n 48.691673 48.56413  70.81073  47.71924  50.213295 82.76319  51.999947\n 47.366413 39.325752 46.892506 47.093456 50.669594 16.53939  48.49992\n 53.00602  48.533047 55.919613 54.705997 52.490086 45.693462 50.984196\n 29.522243 45.14081  44.61482  49.679348 47.518337 49.202652 50.338783\n 50.292072 49.750435 46.436775 51.89885  52.41743 ]\nPosterior variance: [9.09090877e-01 1.40845075e-01 1.00000000e+01 1.72117036e-02\n 2.14546220e-03 4.73933630e-02 3.98406386e-02 2.62467209e-02\n 3.83141749e-02 1.51034596e-03 3.55871879e-02 4.32900414e-02\n 1.19317509e-03 2.43902445e-01 3.22580665e-01 2.11819517e-03\n 4.14937735e-02 2.43902445e-01 5.52486181e-02 1.13752706e-03\n 3.22580665e-01 4.97512445e-02 4.97512445e-02 3.78644443e-03\n 9.09090877e-01 1.05152475e-02 7.09219873e-02 9.09090877e-01\n 1.34030299e-03 6.21117987e-02 7.57002225e-03 8.26446265e-02\n 5.84795326e-02 1.09890103e-01 9.09090877e-01 1.09890103e-01\n 4.76190507e-01 1.23456784e-01 5.52486181e-02 6.62251636e-02]\nPosterior means: [47.24001  50.621346       nan 45.344505 61.72957  50.72153  50.862003\n 48.695107 48.56963  70.80759  47.727356 50.21237  82.759285 51.951168\n 47.45137  39.32801  46.9054   47.16435  50.665894 16.543196 48.548313\n 52.991062 48.540344 55.91737  54.278183 52.48747  45.724007 50.894726\n 29.524988 45.17099  44.618893 49.682    47.53285  49.21142  50.307987\n 50.288864 49.762318 46.480766 51.88836  52.401424]\nResampled means: [47.543415 50.35954        nan 45.337807 61.727367 50.75066  50.82397\n 48.68751  48.58279  70.809944 47.73279  50.184345 82.75879  52.207493\n 47.32297  39.327106 46.952652 47.227066 50.734917 16.544348 48.26844\n 52.821487 48.57457  55.915943 55.76784  52.493423 45.68177  51.161404\n 29.524757 45.180626 44.61609  49.798714 47.480343 49.214035 50.325737\n 50.26275  50.75731  46.38484  52.03652  52.479916]\nChosen means: [47.543415 50.35954  50.26712  45.337807 61.727367 50.75066  50.82397\n 48.68751  48.58279  70.809944 47.73279  50.184345 82.75879  52.207493\n 47.32297  39.327106 46.952652 47.227066 50.734917 16.544348 48.26844\n 52.821487 48.57457  55.915943 55.76784  52.493423 45.68177  51.161404\n 29.524757 45.180626 44.61609  49.798714 47.480343 49.214035 50.325737\n 50.26275  50.75731  46.38484  52.03652  52.479916]\nCategory counts: [  1   8   2  73 448  19  18  47  23 691  25  24 779   6   5 480  28   3\n  31 870   6  17  15 310   4  86  11   2 685  19 175  10  18   8   0   6\n   1   7  25  14]\nNew alpha: [1.312500e+00 8.312500e+00 2.312500e+00 7.331250e+01 4.483125e+02\n 1.931250e+01 1.831250e+01 4.731250e+01 2.331250e+01 6.913125e+02\n 2.531250e+01 2.431250e+01 7.793125e+02 6.312500e+00 5.312500e+00\n 4.803125e+02 2.831250e+01 3.312500e+00 3.131250e+01 8.703125e+02\n 6.312500e+00 1.731250e+01 1.531250e+01 3.103125e+02 4.312500e+00\n 8.631250e+01 1.131250e+01 2.312500e+00 6.853125e+02 1.931250e+01\n 1.753125e+02 1.031250e+01 1.831250e+01 8.312500e+00 3.125000e-01\n 6.312500e+00 1.312500e+00 7.312500e+00 2.531250e+01 1.431250e+01]\nNew probs: [9.75410658e-05 1.90390670e-03 2.84772133e-04 1.50025031e-02\n 9.22242180e-02 2.42311996e-03 3.99559783e-03 9.00209788e-03\n 5.70507348e-03 1.34769231e-01 6.61733467e-03 4.76223929e-03\n 1.55421838e-01 9.98117728e-04 6.55599288e-04 9.41333994e-02\n 5.23053575e-03 3.28179216e-04 8.04566126e-03 1.77873328e-01\n 1.89502956e-03 3.00383172e-03 3.42095038e-03 6.20650053e-02\n 4.35220252e-04 1.80077255e-02 3.31702223e-03 1.14749106e-04\n 1.29299209e-01 4.52292338e-03 3.37460563e-02 1.87533966e-03\n 4.67356667e-03 1.38486072e-03 1.67539947e-05 1.77470304e-03\n 1.19174285e-04 1.53805560e-03 4.72347159e-03 4.59202426e-03]\n</pre> <pre>Category counts: [  1   8   2  73 448  19  18  47  23 691  25  24 779   6   5 480  28   3\n  31 870   6  17  15 310   4  86  11   2 685  19 175  10  18   8   0   6\n   1   7  25  14]\nCurrent means: [48.44412  50.327583 49.621346 45.262417 62.378178 50.256073 50.71908\n 48.504234 48.542152 71.49419  47.50138  50.49149  83.24108  52.364574\n 48.563988 38.242016 47.051888 47.920345 50.97668  16.468143 47.708622\n 52.87016  48.46423  56.310257 54.86656  52.66448  45.939266 51.525986\n 28.911205 44.72741  43.821114 49.908104 47.459503 49.24861        nan\n 50.491776 52.178345 46.398464 51.676743 52.478638]\nPosterior variance: [9.0909088e-01 1.2345678e-01 4.7619051e-01 1.3679891e-02 2.2316447e-03\n 5.2356020e-02 5.5248618e-02 2.1231424e-02 4.3290041e-02 1.4469686e-03\n 3.9840639e-02 4.1493773e-02 1.2835324e-03 1.6393442e-01 1.9607843e-01\n 2.0828994e-03 3.5587188e-02 3.2258067e-01 3.2154340e-02 1.1492933e-03\n 1.6393442e-01 5.8479533e-02 6.6225164e-02 3.2247661e-03 2.4390244e-01\n 1.1614402e-02 9.0090089e-02 4.7619051e-01 1.4596410e-03 5.2356020e-02\n 5.7110223e-03 9.9009894e-02 5.5248618e-02 1.2345678e-01 1.0000000e+01\n 1.6393442e-01 9.0909088e-01 1.4084508e-01 3.9840639e-02 7.0921987e-02]\nPosterior means: [48.585564 50.32354  49.639374 45.268898 62.375412 50.254734 50.71511\n 48.50741  48.548466 71.49107  47.511337 50.48945  83.23682  52.32581\n 48.592144 38.244465 47.06238  47.987434 50.973537 16.471998 47.74618\n 52.853374 48.474403 56.30822  54.747868 52.66138  45.97585  51.45332\n 28.914282 44.755016 43.824642 49.90901  47.47354  49.25789        nan\n 50.48371  51.980316 46.44919  51.670063 52.46106 ]\nResampled means: [49.972168 50.368374 49.871727 45.270363 62.374104 50.256958 50.630146\n 48.491043 48.519253 71.492134 47.52078  50.516293 83.23463  52.375313\n 48.763275 38.246532 47.056793 48.232246 50.977425 16.472376 47.762177\n 52.806744 48.445076 56.307068 54.842422 52.6551   45.85516  50.80916\n 28.91604  44.659687 43.820034 49.84225  47.51146  49.24012        nan\n 50.337452 51.840244 46.436718 51.645786 52.50118 ]\nChosen means: [49.972168 50.368374 49.871727 45.270363 62.374104 50.256958 50.630146\n 48.491043 48.519253 71.492134 47.52078  50.516293 83.23463  52.375313\n 48.763275 38.246532 47.056793 48.232246 50.977425 16.472376 47.762177\n 52.806744 48.445076 56.307068 54.842422 52.6551   45.85516  50.80916\n 28.91604  44.659687 43.820034 49.84225  47.51146  49.24012  50.325737\n 50.337452 51.840244 46.436718 51.645786 52.50118 ]\nCategory counts: [  1   8   1  64 485  10  20  42  35 640  27  24 750   2   1 494  30   2\n  39 840   6  10   8 335   2 116  13   0 660  19 208  11  29   7   1  11\n   0   9  13  27]\nNew alpha: [1.312500e+00 8.312500e+00 1.312500e+00 6.431250e+01 4.853125e+02\n 1.031250e+01 2.031250e+01 4.231250e+01 3.531250e+01 6.403125e+02\n 2.731250e+01 2.431250e+01 7.503125e+02 2.312500e+00 1.312500e+00\n 4.943125e+02 3.031250e+01 2.312500e+00 3.931250e+01 8.403125e+02\n 6.312500e+00 1.031250e+01 8.312500e+00 3.353125e+02 2.312500e+00\n 1.163125e+02 1.331250e+01 3.125000e-01 6.603125e+02 1.931250e+01\n 2.083125e+02 1.131250e+01 2.931250e+01 7.312500e+00 1.312500e+00\n 1.131250e+01 3.125000e-01 9.312500e+00 1.331250e+01 2.731250e+01]\nNew probs: [5.7480967e-04 1.4994700e-03 3.0095348e-04 1.1686050e-02 9.5154449e-02\n 1.9017545e-03 3.4956592e-03 1.0954513e-02 6.5235398e-03 1.3139531e-01\n 5.4558325e-03 6.0229660e-03 1.5705052e-01 2.5057344e-04 1.4572061e-04\n 9.3264803e-02 5.3448556e-03 5.4595462e-04 7.4963598e-03 1.5684442e-01\n 1.1840909e-03 2.3546708e-03 1.2630308e-03 7.3216803e-02 6.0553622e-04\n 2.3319080e-02 3.0539313e-03 7.5271746e-05 1.2869376e-01 5.3290543e-03\n 4.0202804e-02 2.4825064e-03 7.3668319e-03 4.5061536e-04 6.8469199e-05\n 2.3138809e-03 1.5969066e-05 2.6252246e-03 2.5482019e-03 6.9222078e-03]\nCategory counts: [  1   8   1  64 485  10  20  42  35 640  27  24 750   2   1 494  30   2\n  39 840   6  10   8 335   2 116  13   0 660  19 208  11  29   7   1  11\n   0   9  13  27]\nCurrent means: [50.66751  50.205837 50.733574 45.3675   63.381996 50.36564  50.38019\n 48.3326   48.434235 72.35693  47.442207 50.506763 83.48977  52.467308\n 48.918564 37.487213 46.88409  48.437355 50.872704 16.240767 48.08356\n 52.83175  48.67267  56.745087 54.70683  52.753716 46.625122       nan\n 28.186459 44.79764  43.25035  50.05466  47.195625 48.468727 50.49217\n 50.87903        nan 46.58785  51.59312  52.965263]\nPosterior variance: [9.09090877e-01 1.23456784e-01 9.09090877e-01 1.56006245e-02\n 2.06143060e-03 9.90098938e-02 4.97512445e-02 2.37529706e-02\n 2.84900293e-02 1.56225590e-03 3.69003676e-02 4.14937735e-02\n 1.33315567e-03 4.76190507e-01 9.09090877e-01 2.02388177e-03\n 3.32225896e-02 4.76190507e-01 2.55754478e-02 1.19033456e-03\n 1.63934425e-01 9.90098938e-02 1.23456784e-01 2.98418384e-03\n 4.76190507e-01 8.61326419e-03 7.63358772e-02 1.00000000e+01\n 1.51492201e-03 5.23560196e-02 4.80538188e-03 9.00900885e-02\n 3.43642607e-02 1.40845075e-01 9.09090877e-01 9.00900885e-02\n 1.00000000e+01 1.09890103e-01 7.63358772e-02 3.69003676e-02]\nPosterior means: [50.60683  50.203297 50.66689  45.374725 63.37924  50.362015 50.3783\n 48.33656  48.438694 72.35345  47.45165  50.504658 83.485306 52.349815\n 49.01688  37.489746 46.894444 48.511765 50.87047  16.244783 48.114975\n 52.803707 48.689056 56.743073 54.482693 52.751347 46.650883       nan\n 28.189762 44.824883 43.253597 50.05417  47.20526  48.49029  50.44743\n 50.87111        nan 46.625347 51.58096  52.954323]\nResampled means: [51.780113 50.109566 50.84643  45.358025 63.376545 50.489815 50.38161\n 48.368156 48.42543  72.35379  47.457268 50.505123 83.48402  52.810825\n 48.268654 37.489105 46.84598  49.299854 50.877327 16.24648  47.929005\n 52.887367 48.82587  56.742073 55.441647 52.74524  46.725468       nan\n 28.190561 44.932938 43.253742 50.05856  47.255016 48.344154 49.795094\n 50.93613        nan 46.62006  51.485767 52.960278]\nChosen means: [51.780113 50.109566 50.84643  45.358025 63.376545 50.489815 50.38161\n 48.368156 48.42543  72.35379  47.457268 50.505123 83.48402  52.810825\n 48.268654 37.489105 46.84598  49.299854 50.877327 16.24648  47.929005\n 52.887367 48.82587  56.742073 55.441647 52.74524  46.725468 50.80916\n 28.190561 44.932938 43.253742 50.05856  47.255016 48.344154 49.795094\n 50.93613  51.840244 46.62006  51.485767 52.960278]\nCategory counts: [  4   5   2  52 491  14  19  55  30 627  27  35 747   1   1 452  18   2\n  34 779  10  20   4 306   3 121  20   0 702  30 261  11  30   3   0  11\n   0  15  23  35]\nNew alpha: [4.312500e+00 5.312500e+00 2.312500e+00 5.231250e+01 4.913125e+02\n 1.431250e+01 1.931250e+01 5.531250e+01 3.031250e+01 6.273125e+02\n 2.731250e+01 3.531250e+01 7.473125e+02 1.312500e+00 1.312500e+00\n 4.523125e+02 1.831250e+01 2.312500e+00 3.431250e+01 7.793125e+02\n 1.031250e+01 2.031250e+01 4.312500e+00 3.063125e+02 3.312500e+00\n 1.213125e+02 2.031250e+01 3.125000e-01 7.023125e+02 3.031250e+01\n 2.613125e+02 1.131250e+01 3.031250e+01 3.312500e+00 3.125000e-01\n 1.131250e+01 3.125000e-01 1.531250e+01 2.331250e+01 3.531250e+01]\nNew probs: [5.50613215e-04 5.89958043e-04 8.51037330e-04 9.91903804e-03\n 9.98352915e-02 2.02510133e-03 3.71100078e-03 1.04941111e-02\n 5.26443031e-03 1.27247512e-01 6.88638166e-03 6.65142480e-03\n 1.51559144e-01 3.51287483e-04 8.47567047e-04 8.90266150e-02\n 2.78795138e-03 1.17548414e-04 7.49827968e-03 1.46313846e-01\n 2.54772091e-03 4.66085877e-03 1.20894052e-03 5.79411946e-02\n 1.02937956e-04 2.56634541e-02 3.61685851e-03 1.36729077e-05\n 1.44556224e-01 7.72151444e-03 5.51608168e-02 1.26656611e-03\n 7.85974879e-03 3.19221464e-04 1.77793263e-05 1.31565589e-03\n 2.93793692e-05 2.48040981e-03 4.59480612e-03 6.39364868e-03]\nCategory counts: [  4   5   2  52 491  14  19  55  30 627  27  35 747   1   1 452  18   2\n  34 779  10  20   4 306   3 121  20   0 702  30 261  11  30   3   0  11\n   0  15  23  35]\nCurrent means: [52.458218 50.369743 49.598595 45.142715 63.56568  50.46014  50.44936\n 48.34883  48.32494  72.52333  47.428776 50.45614  83.51126  52.271763\n 46.514797 36.867764 46.848923 49.310368 50.72709  15.749187 48.281227\n 53.45974  48.588776 57.154617 54.65057  53.09019  46.68873        nan\n 27.566729 44.97453  42.58965  49.95918  47.324566 48.262024       nan\n 50.279484       nan 46.603985 51.689762 53.486435]\nPosterior variance: [2.4390244e-01 1.9607843e-01 4.7619051e-01 1.9193858e-02 2.0362451e-03\n 7.0921987e-02 5.2356020e-02 1.8148821e-02 3.3222590e-02 1.5946421e-03\n 3.6900368e-02 2.8490029e-02 1.3385089e-03 9.0909088e-01 9.0909088e-01\n 2.2119000e-03 5.5248618e-02 4.7619051e-01 2.9325515e-02 1.2835324e-03\n 9.9009894e-02 4.9751244e-02 2.4390244e-01 3.2669061e-03 3.2258067e-01\n 8.2576387e-03 4.9751244e-02 1.0000000e+01 1.4242986e-03 3.3222590e-02\n 3.8299502e-03 9.0090089e-02 3.3222590e-02 3.2258067e-01 1.0000000e+01\n 9.0090089e-02 1.0000000e+01 6.6225164e-02 4.3290041e-02 2.8490029e-02]\nPosterior means: [52.398262 50.362495 49.617706 45.15204  63.562916 50.45688  50.44701\n 48.351826 48.330505 72.51974  47.438267 50.454838 83.506775 52.065243\n 46.83164  36.870667 46.866333 49.343204 50.72496  15.753584 48.29824\n 53.442528 48.6232   57.15228  54.500553 53.08764  46.7052         nan\n 27.569923 44.991226 42.592487 49.95955  47.333454 48.31809        nan\n 50.276966       nan 46.626476 51.68245  53.4765  ]\nResampled means: [52.099438 50.266426 49.798897 45.152996 63.565094 50.480465 50.335827\n 48.367916 48.29999  72.51891  47.430344 50.432423 83.50855  51.337162\n 46.489227 36.868164 46.75797  50.37275  50.78295  15.754236 48.296654\n 53.537384 48.525536 57.152035 54.414055 53.087955 46.70978        nan\n 27.569798 44.98782  42.58551  50.006172 47.352467 48.387543       nan\n 50.3885         nan 46.625797 51.681763 53.518127]\nChosen means: [52.099438 50.266426 49.798897 45.152996 63.565094 50.480465 50.335827\n 48.367916 48.29999  72.51891  47.430344 50.432423 83.50855  51.337162\n 46.489227 36.868164 46.75797  50.37275  50.78295  15.754236 48.296654\n 53.537384 48.525536 57.152035 54.414055 53.087955 46.70978  50.80916\n 27.569798 44.98782  42.58551  50.006172 47.352467 48.387543 49.795094\n 50.3885   51.840244 46.625797 51.681763 53.518127]\nCategory counts: [  3   4   5  59 467   7  11  44  32 622  33  32 743   2   4 477  17   1\n  54 750  17  24   4 309   3 141  17   0 653  47 287   4  41   0   0  10\n   0  14  23  39]\nNew alpha: [3.312500e+00 4.312500e+00 5.312500e+00 5.931250e+01 4.673125e+02\n 7.312500e+00 1.131250e+01 4.431250e+01 3.231250e+01 6.223125e+02\n 3.331250e+01 3.231250e+01 7.433125e+02 2.312500e+00 4.312500e+00\n 4.773125e+02 1.731250e+01 1.312500e+00 5.431250e+01 7.503125e+02\n 1.731250e+01 2.431250e+01 4.312500e+00 3.093125e+02 3.312500e+00\n 1.413125e+02 1.731250e+01 3.125000e-01 6.533125e+02 4.731250e+01\n 2.873125e+02 4.312500e+00 4.131250e+01 3.125000e-01 3.125000e-01\n 1.031250e+01 3.125000e-01 1.431250e+01 2.331250e+01 3.931250e+01]\nNew probs: [3.95546667e-04 9.26361710e-04 6.23643631e-04 1.15436278e-02\n 8.69047493e-02 1.68144773e-03 3.25225107e-03 9.66177508e-03\n 6.37842622e-03 1.23308174e-01 6.28617359e-03 4.93882457e-03\n 1.48969725e-01 4.53699147e-04 8.27542390e-04 9.04025435e-02\n 4.04022262e-03 2.01229297e-04 1.05713550e-02 1.56989202e-01\n 2.83473823e-03 2.89690215e-03 9.57830227e-04 6.04429953e-02\n 8.31141486e-04 3.16211991e-02 3.16904671e-03 4.76798505e-06\n 1.33656248e-01 9.90589708e-03 5.73817268e-02 9.76520590e-04\n 9.30155534e-03 8.09296471e-05 6.41729184e-06 1.63199892e-03\n 1.21376498e-12 2.39990931e-03 5.21365786e-03 8.32996424e-03]\nCategory counts: [  3   4   5  59 467   7  11  44  32 622  33  32 743   2   4 477  17   1\n  54 750  17  24   4 309   3 141  17   0 653  47 287   4  41   0   0  10\n   0  14  23  39]\nCurrent means: [51.808483 50.997692 50.368397 45.03441  63.883102 50.744488 50.782078\n 48.555283 48.1733   72.62267  47.394314 50.538452 83.540375 51.659737\n 46.4226   35.772213 46.796494 48.478615 50.732597 15.501671 48.4006\n 54.03686  48.353374 57.72406  55.148346 53.23645  46.22799        nan\n 26.733555 45.195686 41.955124 50.59927  47.170322       nan       nan\n 50.412052       nan 46.857746 51.302288 54.08124 ]\nPosterior variance: [3.2258067e-01 2.4390244e-01 1.9607843e-01 1.6920473e-02 2.1408692e-03\n 1.4084508e-01 9.0090089e-02 2.2675738e-02 3.1152649e-02 1.6074587e-03\n 3.0211482e-02 3.1152649e-02 1.3457140e-03 4.7619051e-01 2.4390244e-01\n 2.0959966e-03 5.8479533e-02 9.0909088e-01 1.8484289e-02 1.3331557e-03\n 5.8479533e-02 4.1493773e-02 2.4390244e-01 3.2351988e-03 3.2258067e-01\n 7.0871720e-03 5.8479533e-02 1.0000000e+01 1.5311592e-03 2.1231424e-02\n 3.4831068e-03 2.4390244e-01 2.4330901e-02 1.0000000e+01 1.0000000e+01\n 9.9009894e-02 1.0000000e+01 7.0921987e-02 4.3290041e-02 2.5575448e-02]\nPosterior means: [51.75015  50.973362 50.361176 45.042812 63.880135 50.734    50.775032\n 48.558563 48.178993 72.61903  47.402187 50.536774 83.535866 51.5807\n 46.509857 35.775196 46.815228 48.616924 50.731247 15.50627  48.409954\n 54.020107 48.39354  57.72156  54.982273 53.234154 46.250046       nan\n 26.737118 45.205887 41.957924 50.584656 47.177208       nan       nan\n 50.40797        nan 46.88003  51.296654 54.0708  ]\nResampled means: [51.969234 50.99517  50.222664 45.059414 63.878696 50.842064 50.73667\n 48.57162  48.23932  72.61995  47.411255 50.578503 83.5339   52.45607\n 46.7497   35.773148 46.834297 48.45116  50.774162 15.506861 48.39413\n 54.0277   48.4398   57.72023  54.82436  53.227547 46.27574        nan\n 26.740126 45.205597 41.96135  50.62169  47.139923       nan       nan\n 50.59326        nan 46.82869  51.222286 54.024208]\nChosen means: [51.969234 50.99517  50.222664 45.059414 63.878696 50.842064 50.73667\n 48.57162  48.23932  72.61995  47.411255 50.578503 83.5339   52.45607\n 46.7497   35.773148 46.834297 48.45116  50.774162 15.506861 48.39413\n 54.0277   48.4398   57.72023  54.82436  53.227547 46.27574  50.80916\n 26.740126 45.205597 41.96135  50.62169  47.139923 48.387543 49.795094\n 50.59326  51.840244 46.82869  51.222286 54.024208]\nCategory counts: [  3   4   2  69 433   8  15  45  29 608  30  32 741   4   3 478  25   1\n  51 747  11  13   4 341   3 156   7   0 628  58 292   5  52   1   0   6\n   0  13  33  49]\nNew alpha: [3.312500e+00 4.312500e+00 2.312500e+00 6.931250e+01 4.333125e+02\n 8.312500e+00 1.531250e+01 4.531250e+01 2.931250e+01 6.083125e+02\n 3.031250e+01 3.231250e+01 7.413125e+02 4.312500e+00 3.312500e+00\n 4.783125e+02 2.531250e+01 1.312500e+00 5.131250e+01 7.473125e+02\n 1.131250e+01 1.331250e+01 4.312500e+00 3.413125e+02 3.312500e+00\n 1.563125e+02 7.312500e+00 3.125000e-01 6.283125e+02 5.831250e+01\n 2.923125e+02 5.312500e+00 5.231250e+01 1.312500e+00 3.125000e-01\n 6.312500e+00 3.125000e-01 1.331250e+01 3.331250e+01 4.931250e+01]\nNew probs: [4.57213959e-04 5.41546324e-04 4.11632092e-04 1.27824871e-02\n 8.97891894e-02 2.51931092e-03 2.93767359e-03 1.05638672e-02\n 8.19684379e-03 1.14445135e-01 4.11683740e-03 7.74668111e-03\n 1.42698258e-01 9.43477033e-04 2.25406548e-04 9.87161249e-02\n 7.52458395e-03 1.68234576e-04 1.35036502e-02 1.53152034e-01\n 2.16076011e-03 3.59008787e-03 7.48401624e-04 6.67696744e-02\n 8.28044489e-04 3.30540650e-02 1.26991700e-03 1.72285564e-08\n 1.16641164e-01 1.14026573e-02 6.11879155e-02 9.55115655e-04\n 1.03240283e-02 2.85747141e-04 4.46512473e-07 1.65055471e-03\n 7.74601504e-05 2.56015710e-03 7.06391782e-03 7.98929110e-03]\nCategory counts: [  3   4   2  69 433   8  15  45  29 608  30  32 741   4   3 478  25   1\n  51 747  11  13   4 341   3 156   7   0 628  58 292   5  52   1   0   6\n   0  13  33  49]\nCurrent means: [52.161163 50.50918  50.46067  45.043655 64.423775 50.294605 50.771114\n 48.55644  48.194923 72.75807  46.91361  50.48223  83.554405 52.74115\n 46.751465 35.37449  46.78631  48.44412  50.624996 15.479738 48.591366\n 54.29083  48.23803  58.290546 55.243702 53.347717 46.11414        nan\n 26.464443 44.959396 41.494717 49.89142  47.185043 48.098957       nan\n 51.029373       nan 46.937664 51.519917 54.182514]\nPosterior variance: [3.22580665e-01 2.43902445e-01 4.76190507e-01 1.44717805e-02\n 2.30893563e-03 1.23456784e-01 6.62251636e-02 2.21729502e-02\n 3.43642607e-02 1.64446642e-03 3.32225896e-02 3.11526489e-02\n 1.34934566e-03 2.43902445e-01 3.22580665e-01 2.09161267e-03\n 3.98406386e-02 9.09090877e-01 1.95694715e-02 1.33850891e-03\n 9.00900885e-02 7.63358772e-02 2.43902445e-01 2.93169147e-03\n 3.22580665e-01 6.40614983e-03 1.40845075e-01 1.00000000e+01\n 1.59210328e-03 1.72117036e-02 3.42348497e-03 1.96078435e-01\n 1.91938579e-02 9.09090877e-01 1.00000000e+01 1.63934425e-01\n 1.00000000e+01 7.63358772e-02 3.02114822e-02 2.03665998e-02]\nPosterior means: [52.09145  50.49676  50.438732 45.050827 64.42044  50.290966 50.76601\n 48.55964  48.201126 72.754326 46.923862 50.48073  83.54987  52.674294\n 46.85626  35.37755  46.799114 48.585564 50.62377  15.484359 48.604057\n 54.25807  48.281006 58.288116 55.07455  53.345573 46.16887        nan\n 26.46819  44.96807  41.497627 49.893547 47.19045  48.271782       nan\n 51.012497       nan 46.96104  51.515327 54.173996]\nResampled means: [51.778454 50.65812  50.437397 45.033436 64.419525 50.30869  50.712044\n 48.564934 48.217964 72.75213  46.95244  50.435547 83.55037  52.371468\n 46.870842 35.3771   46.730667 47.509846 50.64816  15.484293 48.572617\n 54.29923  48.338905 58.29456  54.61717  53.350925 46.106586       nan\n 26.468958 44.99735  41.498302 50.016335 47.200733 47.369595       nan\n 51.223503       nan 47.019794 51.448093 54.220955]\nChosen means: [51.778454 50.65812  50.437397 45.033436 64.419525 50.30869  50.712044\n 48.564934 48.217964 72.75213  46.95244  50.435547 83.55037  52.371468\n 46.870842 35.3771   46.730667 47.509846 50.64816  15.484293 48.572617\n 54.29923  48.338905 58.29456  54.61717  53.350925 46.106586 50.80916\n 26.468958 44.99735  41.498302 50.016335 47.200733 47.369595 49.795094\n 51.223503 51.840244 47.019794 51.448093 54.220955]\nCategory counts: [  2   1   3  89 451  11  13  52  24 570  17  35 731   1   0 448  34   2\n  61 739  15  20   6 358   5 159   2   0 621  62 317   3  48   1   0  13\n   0  15  29  42]\nNew alpha: [2.312500e+00 1.312500e+00 3.312500e+00 8.931250e+01 4.513125e+02\n 1.131250e+01 1.331250e+01 5.231250e+01 2.431250e+01 5.703125e+02\n 1.731250e+01 3.531250e+01 7.313125e+02 1.312500e+00 3.125000e-01\n 4.483125e+02 3.431250e+01 2.312500e+00 6.131250e+01 7.393125e+02\n 1.531250e+01 2.031250e+01 6.312500e+00 3.583125e+02 5.312500e+00\n 1.593125e+02 2.312500e+00 3.125000e-01 6.213125e+02 6.231250e+01\n 3.173125e+02 3.312500e+00 4.831250e+01 1.312500e+00 3.125000e-01\n 1.331250e+01 3.125000e-01 1.531250e+01 2.931250e+01 4.231250e+01]\nNew probs: [1.17521174e-03 9.74987488e-05 3.86229483e-04 1.83930509e-02\n 8.88599828e-02 3.79774044e-03 1.88979856e-03 1.01459995e-02\n 3.01949191e-03 1.13226585e-01 1.84643606e-03 4.76751011e-03\n 1.52285963e-01 3.81575985e-04 2.27941865e-09 9.11459923e-02\n 6.20168122e-03 3.81296442e-04 1.18881203e-02 1.42691597e-01\n 3.32214939e-03 3.50082526e-03 1.78895739e-03 7.69537315e-02\n 8.03280389e-04 2.89097130e-02 4.63055214e-04 6.90974121e-05\n 1.21075727e-01 1.27112456e-02 6.50436282e-02 4.65485093e-04\n 1.08919116e-02 1.11119734e-04 1.52141647e-05 2.85429182e-03\n 3.30163493e-06 2.37085391e-03 8.94798711e-03 7.11640203e-03]\nCategory counts: [  2   1   3  89 451  11  13  52  24 570  17  35 731   1   0 448  34   2\n  61 739  15  20   6 358   5 159   2   0 621  62 317   3  48   1   0  13\n   0  15  29  42]\nCurrent means: [52.38581  52.27214  50.714767 44.949074 65.0924   50.13946  50.13482\n 48.646824 48.38838  73.21808  46.83548  50.382942 83.628204 52.126312\n       nan 34.89329  46.15352  47.552677 50.538105 15.421519 48.76348\n 54.600525 48.31547  58.589172 54.54917  53.504574 47.552326       nan\n 26.286446 44.83308  40.938133 50.626232 47.288136 48.173595       nan\n 51.35458        nan 47.025867 51.65054  54.63077 ]\nPosterior variance: [4.7619051e-01 9.0909088e-01 3.2258067e-01 1.1223345e-02 2.2168034e-03\n 9.0090089e-02 7.6335877e-02 1.9193858e-02 4.1493773e-02 1.7540783e-03\n 5.8479533e-02 2.8490029e-02 1.3678020e-03 9.0909088e-01 1.0000000e+01\n 2.2316447e-03 2.9325515e-02 4.7619051e-01 1.6366612e-02 1.3529969e-03\n 6.6225164e-02 4.9751244e-02 1.6393442e-01 2.7925160e-03 1.9607843e-01\n 6.2853550e-03 4.7619051e-01 1.0000000e+01 1.6100467e-03 1.6103059e-02\n 3.1535793e-03 3.2258067e-01 2.0790022e-02 9.0909088e-01 1.0000000e+01\n 7.6335877e-02 1.0000000e+01 6.6225164e-02 3.4364261e-02 2.3752971e-02]\nPosterior means: [52.272198 52.065586 50.69171  44.954742 65.08906  50.138206 50.13379\n 48.64942  48.395065 73.21401  46.853985 50.38185  83.623604 51.933014\n       nan 34.89666  46.164803 47.669216 50.537224 15.426198 48.77167\n 54.577637 48.343086 58.586773 54.459972 53.50237  47.66888        nan\n 26.290262 44.8414   40.94099  50.606033 47.293777 48.339634       nan\n 51.34424        nan 47.045567 51.644867 54.61977 ]\nResampled means: [52.46595  53.024315 50.709507 44.94504  65.09227  50.146793 50.100792\n 48.611645 48.424084 73.214554 46.81184  50.38269  83.625916 53.336548\n       nan 34.89911  46.14252  48.424255 50.554794 15.426304 48.847336\n 54.54357  48.297623 58.592747 54.19883  53.49225  46.791092       nan\n 26.287336 44.84804  40.938633 50.177395 47.31273  48.776417       nan\n 51.28399        nan 46.95205  51.62748  54.5733  ]\nChosen means: [52.46595  53.024315 50.709507 44.94504  65.09227  50.146793 50.100792\n 48.611645 48.424084 73.214554 46.81184  50.38269  83.625916 53.336548\n 46.870842 34.89911  46.14252  48.424255 50.554794 15.426304 48.847336\n 54.54357  48.297623 58.592747 54.19883  53.49225  46.791092 50.80916\n 26.287336 44.84804  40.938633 50.177395 47.31273  48.776417 49.795094\n 51.28399  51.840244 46.95205  51.62748  54.5733  ]\nCategory counts: [  7   0   0  95 484  12  10  52  21 563   8  25 696   1   0 436  26   2\n  53 730  19  28   3 348   6 143   2   1 595  73 354   1  59   2   0  22\n   0  13  52  58]\nNew alpha: [7.312500e+00 3.125000e-01 3.125000e-01 9.531250e+01 4.843125e+02\n 1.231250e+01 1.031250e+01 5.231250e+01 2.131250e+01 5.633125e+02\n 8.312500e+00 2.531250e+01 6.963125e+02 1.312500e+00 3.125000e-01\n 4.363125e+02 2.631250e+01 2.312500e+00 5.331250e+01 7.303125e+02\n 1.931250e+01 2.831250e+01 3.312500e+00 3.483125e+02 6.312500e+00\n 1.433125e+02 2.312500e+00 1.312500e+00 5.953125e+02 7.331250e+01\n 3.543125e+02 1.312500e+00 5.931250e+01 2.312500e+00 3.125000e-01\n 2.231250e+01 3.125000e-01 1.331250e+01 5.231250e+01 5.831250e+01]\nNew probs: [5.53623424e-04 6.71854941e-05 1.73792650e-05 1.93275586e-02\n 9.73945335e-02 3.25491326e-03 1.93246093e-03 9.41995811e-03\n 5.38600516e-03 1.09348126e-01 1.92798686e-03 4.18385025e-03\n 1.39381498e-01 6.73603499e-04 1.76259655e-05 8.77400115e-02\n 5.63173089e-03 8.32784688e-04 1.03981262e-02 1.53623939e-01\n 2.45416258e-03 6.38507539e-03 1.40874658e-03 6.30565509e-02\n 9.02390049e-04 2.74923816e-02 5.74995007e-04 2.43396469e-04\n 1.16729468e-01 1.41512649e-02 7.22115859e-02 1.14792885e-04\n 1.19977798e-02 2.20109214e-04 1.35121634e-04 3.84131633e-03\n 6.66408084e-08 2.60680774e-03 1.13198133e-02 1.30412579e-02]\nCategory counts: [  7   0   0  95 484  12  10  52  21 563   8  25 696   1   0 436  26   2\n  53 730  19  28   3 348   6 143   2   1 595  73 354   1  59   2   0  22\n   0  13  52  58]\nCurrent means: [52.487514       nan       nan 44.91776  65.468155 49.81493  49.814453\n 48.632366 48.05768  73.86622  46.561493 50.250607 83.898445 54.448006\n       nan 34.181686 46.345238 49.323444 50.53909  15.354408 48.85719\n 55.004173 48.00366  58.800224 54.314503 53.607487 46.35243  48.378395\n 25.944597 44.689327 40.527004 50.96304  47.370476 47.75454        nan\n 51.412785       nan 46.878723 51.295147 54.807343]\nPosterior variance: [1.40845075e-01 1.00000000e+01 1.00000000e+01 1.05152475e-02\n 2.06568884e-03 8.26446265e-02 9.90098938e-02 1.91938579e-02\n 4.73933630e-02 1.77588360e-03 1.23456784e-01 3.98406386e-02\n 1.43657520e-03 9.09090877e-01 1.00000000e+01 2.29305192e-03\n 3.83141749e-02 4.76190507e-01 1.88323930e-02 1.36967539e-03\n 5.23560196e-02 3.55871879e-02 3.22580665e-01 2.87273759e-03\n 1.63934425e-01 6.98811980e-03 4.76190507e-01 9.09090877e-01\n 1.68038986e-03 1.36798909e-02 2.82406085e-03 9.09090877e-01\n 1.69204734e-02 4.76190507e-01 1.00000000e+01 4.52488698e-02\n 1.00000000e+01 7.63358772e-02 1.91938579e-02 1.72117036e-02]\nPosterior means: [52.452477       nan       nan 44.923103 65.464966 49.816463 49.816288\n 48.63499  48.066883 73.861984 46.603943 50.24961  83.89357  54.043644\n       nan 34.185314 46.359238 49.35566  50.538074 15.359155 48.863174\n 54.986366 48.068058 58.7977   54.24377  53.60496  46.52612  48.525818\n 25.948639 44.696594 40.529682 50.875492 47.374928 47.861465       nan\n 51.40639        nan 46.90255  51.292664 54.79907 ]\nResampled means: [52.364983       nan       nan 44.919765 65.46381  49.857327 49.7181\n 48.61851  48.143475 73.862144 46.663307 50.25166  83.896034 55.85802\n       nan 34.18826  46.304985 50.47348  50.5274   15.358143 48.86103\n 54.972214 47.72297  58.79662  54.369667 53.60536  46.772152 49.767803\n 25.946922 44.70841  40.528233 51.804516 47.37206  48.25525        nan\n 51.420113       nan 47.027935 51.312702 54.82441 ]\nChosen means: [52.364983 53.024315 50.709507 44.919765 65.46381  49.857327 49.7181\n 48.61851  48.143475 73.862144 46.663307 50.25166  83.896034 55.85802\n 46.870842 34.18826  46.304985 50.47348  50.5274   15.358143 48.86103\n 54.972214 47.72297  58.79662  54.369667 53.60536  46.772152 49.767803\n 25.946922 44.70841  40.528233 51.804516 47.37206  48.25525  49.795094\n 51.420113 51.840244 47.027935 51.312702 54.82441 ]\nCategory counts: [  4   1   0  98 486  16  11  43  36 607  15  12 641   9   1 473  25  11\n  54 705  11  43  10 322   4 141   2   0 570  93 343   1  49   0   0  19\n   0  14  54  76]\nNew alpha: [4.312500e+00 1.312500e+00 3.125000e-01 9.831250e+01 4.863125e+02\n 1.631250e+01 1.131250e+01 4.331250e+01 3.631250e+01 6.073125e+02\n 1.531250e+01 1.231250e+01 6.413125e+02 9.312500e+00 1.312500e+00\n 4.733125e+02 2.531250e+01 1.131250e+01 5.431250e+01 7.053125e+02\n 1.131250e+01 4.331250e+01 1.031250e+01 3.223125e+02 4.312500e+00\n 1.413125e+02 2.312500e+00 3.125000e-01 5.703125e+02 9.331250e+01\n 3.433125e+02 1.312500e+00 4.931250e+01 3.125000e-01 3.125000e-01\n 1.931250e+01 3.125000e-01 1.431250e+01 5.431250e+01 7.631250e+01]\nNew probs: [5.19645284e-04 1.28823347e-04 7.48874936e-06 2.08237488e-02\n 9.94206518e-02 2.33354419e-03 2.61380291e-03 9.26231872e-03\n 6.12888299e-03 1.25395983e-01 2.77870358e-03 2.98502971e-03\n 1.23296998e-01 2.00855639e-03 4.91544488e-04 9.53863189e-02\n 4.67956299e-03 1.78524887e-03 1.16895176e-02 1.37271509e-01\n 1.44212251e-03 9.72309615e-03 1.84434617e-03 6.75229207e-02\n 5.26134623e-04 3.24342512e-02 4.40413889e-04 9.95115546e-09\n 1.07956588e-01 1.93340573e-02 6.81864992e-02 4.67171449e-05\n 9.30889416e-03 4.59534058e-05 2.87953808e-05 3.74271837e-03\n 3.68633046e-05 3.12265079e-03 9.65835247e-03 1.55903855e-02]\nCategory counts: [  4   1   0  98 486  16  11  43  36 607  15  12 641   9   1 473  25  11\n  54 705  11  43  10 322   4 141   2   0 570  93 343   1  49   0   0  19\n   0  14  54  76]\nCurrent means: [51.95565  52.086338       nan 44.83455  65.61099  49.757565 50.1285\n 48.47998  48.110405 74.38119  46.54004  50.133915 84.3537   55.878277\n 46.820255 33.673515 46.424423 50.72029  50.54253  15.161133 49.036343\n 55.30679  47.314346 59.146812 54.542953 53.54481  46.910088       nan\n 25.330671 44.320442 40.270836 52.43161  47.429276       nan       nan\n 51.64335        nan 46.801167 51.307793 55.28075 ]\nPosterior variance: [2.4390244e-01 9.0909088e-01 1.0000000e+01 1.0193680e-02 2.0571898e-03\n 6.2111799e-02 9.0090089e-02 2.3201857e-02 2.7700832e-02 1.6471752e-03\n 6.6225164e-02 8.2644626e-02 1.5598191e-03 1.0989010e-01 9.0909088e-01\n 2.1137181e-03 3.9840639e-02 9.0090089e-02 1.8484289e-02 1.4182386e-03\n 9.0090089e-02 2.3201857e-02 9.9009894e-02 3.1046257e-03 2.4390244e-01\n 7.0871720e-03 4.7619051e-01 1.0000000e+01 1.7540783e-03 1.0741139e-02\n 2.9146022e-03 9.0909088e-01 2.0366600e-02 1.0000000e+01 1.0000000e+01\n 5.2356020e-02 1.0000000e+01 7.0921987e-02 1.8484289e-02 1.3140605e-02]\nPosterior means: [51.907955 51.896675       nan 44.839813 65.60778  49.75907  50.127346\n 48.48351  48.11564  74.37717  46.562954 50.13281  84.348335 55.813683\n 47.109325 33.676964 46.43867  50.713802 50.54153  15.166075 49.045025\n 55.29448  47.340935 59.143974 54.43215  53.542297 47.057224       nan\n 25.334997 44.32654  40.27367  52.210556 47.43451        nan       nan\n 51.634747       nan 46.823856 51.305378 55.273815]\nResampled means: [52.22246  51.18682        nan 44.82637  65.60758  49.796207 50.162277\n 48.50007  48.12465  74.37893  46.541866 49.945393 84.34856  55.73138\n 47.16697  33.6751   46.47154  50.671124 50.524315 15.166265 48.9951\n 55.313812 47.232914 59.14155  54.58761  53.544086 47.78888        nan\n 25.338017 44.32581  40.27361  51.50878  47.410873       nan       nan\n 51.67362        nan 46.970184 51.327717 55.285606]\nChosen means: [52.22246  51.18682  50.709507 44.82637  65.60758  49.796207 50.162277\n 48.50007  48.12465  74.37893  46.541866 49.945393 84.34856  55.73138\n 47.16697  33.6751   46.47154  50.671124 50.524315 15.166265 48.9951\n 55.313812 47.232914 59.14155  54.58761  53.544086 47.78888  49.767803\n 25.338017 44.32581  40.27361  51.50878  47.410873 48.25525  49.795094\n 51.67362  51.840244 46.970184 51.327717 55.285606]\nCategory counts: [  3   0   0 105 472   6  12  42  35 614  13  11 626  14   3 483  24  10\n  57 663  10  58  13 307   4 137   7   0 588 111 333   1  45   0   0  23\n   0  12  69  89]\nNew alpha: [3.312500e+00 3.125000e-01 3.125000e-01 1.053125e+02 4.723125e+02\n 6.312500e+00 1.231250e+01 4.231250e+01 3.531250e+01 6.143125e+02\n 1.331250e+01 1.131250e+01 6.263125e+02 1.431250e+01 3.312500e+00\n 4.833125e+02 2.431250e+01 1.031250e+01 5.731250e+01 6.633125e+02\n 1.031250e+01 5.831250e+01 1.331250e+01 3.073125e+02 4.312500e+00\n 1.373125e+02 7.312500e+00 3.125000e-01 5.883125e+02 1.113125e+02\n 3.333125e+02 1.312500e+00 4.531250e+01 3.125000e-01 3.125000e-01\n 2.331250e+01 3.125000e-01 1.231250e+01 6.931250e+01 8.931250e+01]\nNew probs: [1.0126240e-03 1.7629663e-05 1.7853152e-05 1.9094646e-02 9.5397457e-02\n 8.6730911e-04 2.9868223e-03 9.3878917e-03 7.0895334e-03 1.1876182e-01\n 1.8820737e-03 2.1900993e-03 1.3107361e-01 3.5304704e-03 2.7596194e-04\n 9.7001828e-02 5.2733496e-03 1.0589143e-03 1.4671464e-02 1.2943423e-01\n 2.1019971e-03 1.1797468e-02 2.7264454e-03 6.0493395e-02 4.6705676e-04\n 2.9943962e-02 1.2871934e-03 6.6768515e-09 1.2130058e-01 1.8849092e-02\n 5.9338003e-02 3.7183959e-04 8.3698193e-03 1.5454173e-04 9.5853429e-05\n 6.0368939e-03 5.0103813e-06 2.2793300e-03 1.5063850e-02 1.8292073e-02]\nCategory counts: [  3   0   0 105 472   6  12  42  35 614  13  11 626  14   3 483  24  10\n  57 663  10  58  13 307   4 137   7   0 588 111 333   1  45   0   0  23\n   0  12  69  89]\nCurrent means: [52.658916       nan       nan 44.609146 65.84088  49.65028  50.115845\n 48.345806 48.27264  74.5459   46.53332  50.108604 84.486084 56.141327\n 46.61653  33.404823 46.67662  50.570908 50.57959  14.824567 49.197697\n 55.81433  47.331577 59.66999  54.10599  53.468185 48.02641        nan\n 24.788593 44.046066 39.96754  52.102142 47.376987       nan       nan\n 51.507008       nan 47.008137 51.203453 55.747772]\nPosterior variance: [3.22580665e-01 1.00000000e+01 1.00000000e+01 9.51474812e-03\n 2.11819517e-03 1.63934425e-01 8.26446265e-02 2.37529706e-02\n 2.84900293e-02 1.62839936e-03 7.63358772e-02 9.00900885e-02\n 1.59718900e-03 7.09219873e-02 3.22580665e-01 2.06996477e-03\n 4.14937735e-02 9.90098938e-02 1.75131354e-02 1.50806818e-03\n 9.90098938e-02 1.72117036e-02 7.63358772e-02 3.25626833e-03\n 2.43902445e-01 7.29394564e-03 1.40845075e-01 1.00000000e+01\n 1.70039118e-03 9.00090020e-03 3.00210132e-03 9.09090877e-01\n 2.21729502e-02 1.00000000e+01 1.00000000e+01 4.32900414e-02\n 1.00000000e+01 8.26446265e-02 1.44717805e-02 1.12233451e-02]\nPosterior means: [52.573147       nan       nan 44.614277 65.837524 49.65601  50.11489\n 48.34973  48.27756  74.5419   46.559784 50.107628 84.480576 56.097775\n 46.725677 33.408257 46.69041  50.56525  50.578575 14.829871 49.205635\n 55.80432  47.351944 59.66684  54.005848 53.465656 48.054203       nan\n 24.792881 44.051426 39.97055  51.91104  47.3828         nan       nan\n 51.500484       nan 47.032864 51.20171  55.74132 ]\nResampled means: [52.879013       nan       nan 44.612263 65.83939  49.7814   50.245808\n 48.324745 48.268925 74.54091  46.634705 50.156254 84.47979  56.16387\n 47.06816  33.407253 46.70108  50.470577 50.591915 14.832874 49.046524\n 55.821815 47.34488  59.66528  54.24615  53.463367 47.908268       nan\n 24.793262 44.04193  39.974266 50.108757 47.417953       nan       nan\n 51.41782        nan 47.022514 51.216152 55.74067 ]\nChosen means: [52.879013 51.18682  50.709507 44.612263 65.83939  49.7814   50.245808\n 48.324745 48.268925 74.54091  46.634705 50.156254 84.47979  56.16387\n 47.06816  33.407253 46.70108  50.470577 50.591915 14.832874 49.046524\n 55.821815 47.34488  59.66528  54.24615  53.463367 47.908268 49.767803\n 24.793262 44.04193  39.974266 50.108757 47.417953 48.25525  49.795094\n 51.41782  51.840244 47.022514 51.216152 55.74067 ]\n</pre> <pre>Category counts: [  6   0   0 103 441   4   8  58  42 599  10  11 625  21   3 467  26   4\n  63 626   8  62  19 335   0 136   5   0 622 123 326   2  46   0   0  13\n   0  12  74 100]\nNew alpha: [6.312500e+00 3.125000e-01 3.125000e-01 1.033125e+02 4.413125e+02\n 4.312500e+00 8.312500e+00 5.831250e+01 4.231250e+01 5.993125e+02\n 1.031250e+01 1.131250e+01 6.253125e+02 2.131250e+01 3.312500e+00\n 4.673125e+02 2.631250e+01 4.312500e+00 6.331250e+01 6.263125e+02\n 8.312500e+00 6.231250e+01 1.931250e+01 3.353125e+02 3.125000e-01\n 1.363125e+02 5.312500e+00 3.125000e-01 6.223125e+02 1.233125e+02\n 3.263125e+02 2.312500e+00 4.631250e+01 3.125000e-01 3.125000e-01\n 1.331250e+01 3.125000e-01 1.231250e+01 7.431250e+01 1.003125e+02]\nNew probs: [6.30164461e-04 4.21892473e-05 2.07204766e-06 2.00168956e-02\n 9.28333551e-02 3.41175270e-04 2.37114774e-03 1.18548917e-02\n 9.50117689e-03 1.31389424e-01 1.18149142e-03 2.72683287e-03\n 1.11796945e-01 4.94735688e-03 7.79976195e-04 8.91217142e-02\n 5.31393848e-03 1.72602653e-04 1.47178508e-02 1.20143764e-01\n 2.17816606e-03 1.21191246e-02 3.99389304e-03 6.62586614e-02\n 5.94516166e-07 2.95022782e-02 7.22188153e-04 1.31197958e-05\n 1.20642178e-01 2.55010072e-02 6.73852563e-02 2.75583734e-04\n 1.05297901e-02 1.07815567e-05 1.03960083e-05 1.47636991e-03\n 3.65277515e-06 3.43346363e-03 1.85732134e-02 1.74854230e-02]\nCategory counts: [  6   0   0 103 441   4   8  58  42 599  10  11 625  21   3 467  26   4\n  63 626   8  62  19 335   0 136   5   0 622 123 326   2  46   0   0  13\n   0  12  74 100]\nCurrent means: [52.607983       nan       nan 44.394646 66.357635 49.88905  50.33603\n 48.266964 48.139874 74.669205 46.86089  50.128414 84.49509  55.96239\n 46.498306 33.24319  46.34943  51.655327 50.589874 14.506864 49.26416\n 55.891277 47.175262 60.216652       nan 53.540222 47.56932        nan\n 24.494093 43.655155 39.575134 51.101486 47.29506        nan       nan\n 51.53151        nan 46.78593  51.31214  55.8261  ]\nPosterior variance: [1.6393442e-01 1.0000000e+01 1.0000000e+01 9.6993214e-03 2.2670596e-03\n 2.4390244e-01 1.2345678e-01 1.7211704e-02 2.3752971e-02 1.6691705e-03\n 9.9009894e-02 9.0090089e-02 1.5997441e-03 4.7393363e-02 3.2258067e-01\n 2.1408692e-03 3.8314175e-02 2.4390244e-01 1.5847862e-02 1.5971890e-03\n 1.2345678e-01 1.6103059e-02 5.2356020e-02 2.9841838e-03 1.0000000e+01\n 7.3475381e-03 1.9607843e-01 1.0000000e+01 1.6074587e-03 8.1234770e-03\n 3.0665440e-03 4.7619051e-01 2.1691974e-02 1.0000000e+01 1.0000000e+01\n 7.6335877e-02 1.0000000e+01 8.2644626e-02 1.3495277e-02 9.9900104e-03]\nPosterior means: [52.565228       nan       nan 44.400085 66.35393  49.891758 50.33188\n 48.269947 48.14429  74.665085 46.891968 50.12726  84.48956  55.93413\n 46.611267 33.24678  46.363415 51.614956 50.58894  14.512532 49.273243\n 55.88179  47.190052 60.2136         nan 53.537624 47.61698        nan\n 24.498194 43.66031  39.57833  51.049034 47.300926       nan       nan\n 51.519817       nan 46.812496 51.310375 55.82028 ]\nResampled means: [52.475998       nan       nan 44.394966 66.34895  49.64899  50.535816\n 48.23242  48.173904 74.66457  46.949863 50.076054 84.48984  55.86026\n 46.43058  33.249573 46.389717 51.80691  50.58597  14.512575 49.203236\n 55.896664 47.237    60.212715       nan 53.52616  47.624783       nan\n 24.498447 43.649147 39.580177 50.64512  47.2651         nan       nan\n 51.488792       nan 46.8697   51.2964   55.834988]\nChosen means: [52.475998 51.18682  50.709507 44.394966 66.34895  49.64899  50.535816\n 48.23242  48.173904 74.66457  46.949863 50.076054 84.48984  55.86026\n 46.43058  33.249573 46.389717 51.80691  50.58597  14.512575 49.203236\n 55.896664 47.237    60.212715 54.24615  53.52616  47.624783 49.767803\n 24.498447 43.649147 39.580177 50.64512  47.2651   48.25525  49.795094\n 51.488792 51.840244 46.8697   51.2964   55.834988]\nCategory counts: [  3   0   0  89 438   1   5  58  41 567   3   8 625  29   6 445  25   1\n  67 625  14  64  19 354   0 143   4   0 605 151 334   1  60   0   0   5\n   0  21  98  91]\nNew alpha: [3.312500e+00 3.125000e-01 3.125000e-01 8.931250e+01 4.383125e+02\n 1.312500e+00 5.312500e+00 5.831250e+01 4.131250e+01 5.673125e+02\n 3.312500e+00 8.312500e+00 6.253125e+02 2.931250e+01 6.312500e+00\n 4.453125e+02 2.531250e+01 1.312500e+00 6.731250e+01 6.253125e+02\n 1.431250e+01 6.431250e+01 1.931250e+01 3.543125e+02 3.125000e-01\n 1.433125e+02 4.312500e+00 3.125000e-01 6.053125e+02 1.513125e+02\n 3.343125e+02 1.312500e+00 6.031250e+01 3.125000e-01 3.125000e-01\n 5.312500e+00 3.125000e-01 2.131250e+01 9.831250e+01 9.131250e+01]\nNew probs: [2.11271385e-04 1.35838782e-05 1.27796957e-04 1.72586627e-02\n 8.72449651e-02 1.00530677e-04 8.74617661e-04 1.00507680e-02\n 8.78628064e-03 1.06764004e-01 1.16220664e-03 1.86295179e-03\n 1.28985152e-01 6.08831085e-03 1.51184166e-03 8.56430903e-02\n 5.16845938e-03 2.78661428e-05 1.49070835e-02 1.31678000e-01\n 2.93801539e-03 1.15314191e-02 3.04642506e-03 6.82654828e-02\n 3.05764661e-05 2.79406942e-02 5.56102546e-04 6.20915216e-07\n 1.27709493e-01 2.58768499e-02 6.25157654e-02 7.57156406e-04\n 1.23303346e-02 7.23897174e-05 1.20159850e-04 8.93591554e-04\n 1.18760967e-04 4.11548698e-03 2.15926152e-02 2.11207438e-02]\nCategory counts: [  3   0   0  89 438   1   5  58  41 567   3   8 625  29   6 445  25   1\n  67 625  14  64  19 354   0 143   4   0 605 151 334   1  60   0   0   5\n   0  21  98  91]\nCurrent means: [53.368908         nan        nan 44.185307  66.94094   50.13239\n 50.627583  48.22239   48.208336  74.91104   46.747406  49.34056\n 84.49509   56.13311   46.897816  32.761677  46.63569   52.22962\n 50.429424  14.4979315 50.087307  56.076782  47.2142    60.574764\n        nan 53.596214  47.322098         nan 24.358988  43.243004\n 38.959534  50.72257   47.107735         nan        nan 50.92023\n        nan 46.302166  51.55475   56.257233 ]\nPosterior variance: [3.22580665e-01 1.00000000e+01 1.00000000e+01 1.12233451e-02\n 2.28258385e-03 9.09090877e-01 1.96078435e-01 1.72117036e-02\n 2.43309010e-02 1.76335755e-03 3.22580665e-01 1.23456784e-01\n 1.59974408e-03 3.43642607e-02 1.63934425e-01 2.24668602e-03\n 3.98406386e-02 9.09090877e-01 1.49031300e-02 1.59974408e-03\n 7.09219873e-02 1.56006245e-02 5.23560196e-02 2.82406085e-03\n 1.00000000e+01 6.98811980e-03 2.43902445e-01 1.00000000e+01\n 1.65261945e-03 6.61813328e-03 2.99311569e-03 9.09090877e-01\n 1.66389346e-02 1.00000000e+01 1.00000000e+01 1.96078435e-01\n 1.00000000e+01 4.73933630e-02 1.01936804e-02 1.09769488e-02]\nPosterior means: [53.260235       nan       nan 44.191833 66.93707  50.120358 50.615276\n 48.22545  48.212696 74.906654 46.85233  49.3487   84.48956  56.112034\n 46.94867  32.765553 46.649094 52.02693  50.428787 14.503611 50.08669\n 56.0673   47.228786 60.57178        nan 53.593697 47.387413       nan\n 24.363226 43.24748  38.96284  50.656883 47.11255        nan       nan\n 50.902187       nan 46.31969  51.55316  56.250362]\nResampled means: [52.962532       nan       nan 44.182682 66.934    50.01998  50.56685\n 48.21082  48.196022 74.90759  47.13692  49.287994 84.49061  56.082504\n 46.839577 32.767048 46.7041   50.835552 50.420532 14.50062  50.015728\n 56.10189  47.256382 60.567444       nan 53.600033 47.894764       nan\n 24.365772 43.26063  38.963196 51.431324 47.08353        nan       nan\n 50.601242       nan 46.34021  51.536587 56.2393  ]\nChosen means: [52.962532 51.18682  50.709507 44.182682 66.934    50.01998  50.56685\n 48.21082  48.196022 74.90759  47.13692  49.287994 84.49061  56.082504\n 46.839577 32.767048 46.7041   50.835552 50.420532 14.50062  50.015728\n 56.10189  47.256382 60.567444 54.24615  53.600033 47.894764 49.767803\n 24.365772 43.26063  38.963196 51.431324 47.08353  48.25525  49.795094\n 50.601242 51.840244 46.34021  51.536587 56.2393  ]\nCategory counts: [  0   0   1  93 474   0   7  40  44 526   9  10 623  32   9 432  29   0\n  68 626  22  53  14 337   0 137   6   0 574 165 360   5  59   0   1   2\n   0  19 101 122]\nNew alpha: [3.125000e-01 3.125000e-01 1.312500e+00 9.331250e+01 4.743125e+02\n 3.125000e-01 7.312500e+00 4.031250e+01 4.431250e+01 5.263125e+02\n 9.312500e+00 1.031250e+01 6.233125e+02 3.231250e+01 9.312500e+00\n 4.323125e+02 2.931250e+01 3.125000e-01 6.831250e+01 6.263125e+02\n 2.231250e+01 5.331250e+01 1.431250e+01 3.373125e+02 3.125000e-01\n 1.373125e+02 6.312500e+00 3.125000e-01 5.743125e+02 1.653125e+02\n 3.603125e+02 5.312500e+00 5.931250e+01 3.125000e-01 1.312500e+00\n 2.312500e+00 3.125000e-01 1.931250e+01 1.013125e+02 1.223125e+02]\nNew probs: [1.13273654e-06 5.27891934e-05 6.20529827e-05 1.98495425e-02\n 9.72557217e-02 7.68149960e-07 6.45295426e-04 6.90966612e-03\n 1.05500240e-02 1.06586933e-01 2.42289808e-03 2.32456834e-03\n 1.21450998e-01 8.18770658e-03 1.80907350e-03 8.88506621e-02\n 6.34521805e-03 4.03653848e-06 1.36121241e-02 1.22314282e-01\n 4.54292959e-03 9.76609066e-03 2.06454960e-03 6.91548064e-02\n 1.76261587e-04 2.79934369e-02 1.94880564e-03 3.14684039e-05\n 1.04894288e-01 3.40029150e-02 7.49290809e-02 9.44607367e-04\n 1.18854735e-02 6.46591525e-06 3.61342798e-04 4.28394735e-04\n 1.06061827e-06 3.35966423e-03 1.89394839e-02 2.53333170e-02]\nCategory counts: [  0   0   1  93 474   0   7  40  44 526   9  10 623  32   9 432  29   0\n  68 626  22  53  14 337   0 137   6   0 574 165 360   5  59   0   1   2\n   0  19 101 122]\nCurrent means: [      nan       nan 50.504818 44.14718  67.331436       nan 50.613228\n 48.2564   48.273582 75.28151  46.933273 49.257927 84.50904  56.100006\n 47.04862  32.123737 46.956173       nan 50.281166 14.506758 49.75228\n 56.482956 47.379124 60.79012        nan 53.66179  47.505474       nan\n 24.138206 43.086468 38.52197  51.724556 46.84524        nan 48.63548\n 50.74783        nan 46.719814 51.802006 56.51408 ]\nPosterior variance: [1.0000000e+01 1.0000000e+01 9.0909088e-01 1.0741139e-02 2.1092596e-03\n 1.0000000e+01 1.4084508e-01 2.4937658e-02 2.2675738e-02 1.9007794e-03\n 1.0989010e-01 9.9009894e-02 1.6048789e-03 3.1152649e-02 1.0989010e-01\n 2.3142791e-03 3.4364261e-02 1.0000000e+01 1.4684288e-02 1.5971890e-03\n 4.5248870e-02 1.8832393e-02 7.0921987e-02 2.9664787e-03 1.0000000e+01\n 7.2939456e-03 1.6393442e-01 1.0000000e+01 1.7418569e-03 6.0569351e-03\n 2.7770062e-03 1.9607843e-01 1.6920473e-02 1.0000000e+01 9.0909088e-01\n 4.7619051e-01 1.0000000e+01 5.2356020e-02 9.8911971e-03 8.1900079e-03]\nPosterior means: [      nan       nan 50.458927 44.153465 67.32778        nan 50.604588\n 48.26075  48.2775   75.27671  46.966976 49.26527  84.50351  56.081\n 47.081055 32.127872 46.966633       nan 50.280754 14.512426 49.7534\n 56.47075  47.397713 60.786915       nan 53.65912  47.546364       nan\n 24.14271  43.090656 38.525154 51.690742 46.850582       nan 48.75953\n 50.712215       nan 46.73699  51.800224 56.508743]\nResampled means: [      nan       nan 49.866882 44.157433 67.33187        nan 50.77381\n 48.251736 48.28439  75.275475 46.77693  49.27621  84.504265 56.040867\n 46.9859   32.125362 46.945946       nan 50.286263 14.509905 49.760643\n 56.48236  47.3632   60.786747       nan 53.654045 47.595894       nan\n 24.14558  43.096577 38.53053  51.786613 46.882935       nan 48.60341\n 50.012817       nan 46.750515 51.78942  56.488697]\nChosen means: [52.962532 51.18682  49.866882 44.157433 67.33187  50.01998  50.77381\n 48.251736 48.28439  75.275475 46.77693  49.27621  84.504265 56.040867\n 46.9859   32.125362 46.945946 50.835552 50.286263 14.509905 49.760643\n 56.48236  47.3632   60.786747 54.24615  53.654045 47.595894 49.767803\n 24.14558  43.096577 38.53053  51.786613 46.882935 48.25525  48.60341\n 50.012817 51.840244 46.750515 51.78942  56.488697]\nCategory counts: [  0   1   1  94 477   0   3  40  40 508  12  11 619  37   4 475  37   0\n  80 625  22  53   6 319   1 145  11   0 523 176 354   3  53   0   2   1\n   0  24  92 151]\nNew alpha: [3.125000e-01 1.312500e+00 1.312500e+00 9.431250e+01 4.773125e+02\n 3.125000e-01 3.312500e+00 4.031250e+01 4.031250e+01 5.083125e+02\n 1.231250e+01 1.131250e+01 6.193125e+02 3.731250e+01 4.312500e+00\n 4.753125e+02 3.731250e+01 3.125000e-01 8.031250e+01 6.253125e+02\n 2.231250e+01 5.331250e+01 6.312500e+00 3.193125e+02 1.312500e+00\n 1.453125e+02 1.131250e+01 3.125000e-01 5.233125e+02 1.763125e+02\n 3.543125e+02 3.312500e+00 5.331250e+01 3.125000e-01 2.312500e+00\n 1.312500e+00 3.125000e-01 2.431250e+01 9.231250e+01 1.513125e+02]\nNew probs: [2.7061264e-14 1.0102497e-04 3.9649368e-04 1.9037515e-02 9.6542321e-02\n 6.4238287e-05 5.7832740e-04 7.9233535e-03 6.5184650e-03 1.0014368e-01\n 2.8404510e-03 1.8451994e-03 1.2255353e-01 6.7550614e-03 7.6169049e-04\n 9.2134774e-02 5.6979368e-03 4.5110076e-07 1.6616356e-02 1.2751143e-01\n 4.6129744e-03 7.5059030e-03 1.4483384e-03 6.9339916e-02 1.8508681e-04\n 2.4075419e-02 2.8154235e-03 5.7850029e-06 1.0112865e-01 3.3934318e-02\n 7.8736424e-02 6.6373323e-04 1.2288015e-02 2.0643082e-04 2.2716208e-04\n 6.1714230e-04 8.2841332e-05 3.6348635e-03 1.9542765e-02 3.0926380e-02]\nCategory counts: [  0   1   1  94 477   0   3  40  40 508  12  11 619  37   4 475  37   0\n  80 625  22  53   6 319   1 145  11   0 523 176 354   3  53   0   2   1\n   0  24  92 151]\nCurrent means: [       nan 50.665203  50.83135   44.195225  67.61912          nan\n 51.42172   48.01704   48.282818  75.512596  46.831165  49.0099\n 84.53707   55.985535  47.075928  31.647778  46.945618         nan\n 50.342705  14.4979315 49.876915  56.937115  47.37717   61.258186\n 52.22962   53.597412  47.45203          nan 23.705576  42.810177\n 38.3593    51.933884  46.788887         nan 47.867302  50.76721\n        nan 46.807358  51.822357  56.94306  ]\nPosterior variance: [1.0000000e+01 9.0909088e-01 9.0909088e-01 1.0626993e-02 2.0959966e-03\n 1.0000000e+01 3.2258067e-01 2.4937658e-02 2.4937658e-02 1.9681165e-03\n 8.2644626e-02 9.0090089e-02 1.6152480e-03 2.6954180e-02 2.4390244e-01\n 2.1048200e-03 2.6954180e-02 1.0000000e+01 1.2484395e-02 1.5997441e-03\n 4.5248870e-02 1.8832393e-02 1.6393442e-01 3.1338139e-03 9.0909088e-01\n 6.8917987e-03 9.0090089e-02 1.0000000e+01 1.9116804e-03 5.6785913e-03\n 2.8240609e-03 3.2258067e-01 1.8832393e-02 1.0000000e+01 4.7619051e-01\n 9.0909088e-01 1.0000000e+01 4.1493773e-02 1.0857764e-02 6.6181333e-03]\nPosterior means: [      nan 50.604733 50.755775 44.201393 67.61543        nan 51.37586\n 48.021984 48.2871   75.507576 46.857357 49.018818 84.531494 55.9694\n 47.147247 31.65164  46.95385        nan 50.34228  14.503611 49.877472\n 56.924053 47.420166 61.254658 52.02693  53.59493  47.474987       nan\n 23.710602 42.81426  38.362587 51.871502 46.794937       nan 47.968857\n 50.697464       nan 46.820602 51.820374 56.93847 ]\nResampled means: [      nan 50.886185 50.256966 44.19914  67.61851        nan 51.488247\n 48.002243 48.29867  75.507126 46.863827 49.04138  84.53141  55.98984\n 47.232273 31.651987 46.973427       nan 50.346798 14.501753 49.893887\n 56.937798 47.26357  61.257084 51.98892  53.59759  47.331974       nan\n 23.71408  42.8062   38.36023  52.4707   46.77241        nan 48.62526\n 51.379192       nan 46.854176 51.816055 56.93504 ]\nChosen means: [52.962532 50.886185 50.256966 44.19914  67.61851  50.01998  51.488247\n 48.002243 48.29867  75.507126 46.863827 49.04138  84.53141  55.98984\n 47.232273 31.651987 46.973427 50.835552 50.346798 14.501753 49.893887\n 56.937798 47.26357  61.257084 51.98892  53.59759  47.331974 49.767803\n 23.71408  42.8062   38.36023  52.4707   46.77241  48.25525  48.62526\n 51.379192 51.840244 46.854176 51.816055 56.93504 ]\nCategory counts: [  0   0   0 109 449   1   1  37  31 513  14  11 612  38   4 483  35   0\n  74 618  30  41   8 329   0 144  15   0 508 168 359   2  59   0   1   4\n   0  21 110 171]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 1.093125e+02 4.493125e+02\n 1.312500e+00 1.312500e+00 3.731250e+01 3.131250e+01 5.133125e+02\n 1.431250e+01 1.131250e+01 6.123125e+02 3.831250e+01 4.312500e+00\n 4.833125e+02 3.531250e+01 3.125000e-01 7.431250e+01 6.183125e+02\n 3.031250e+01 4.131250e+01 8.312500e+00 3.293125e+02 3.125000e-01\n 1.443125e+02 1.531250e+01 3.125000e-01 5.083125e+02 1.683125e+02\n 3.593125e+02 2.312500e+00 5.931250e+01 3.125000e-01 1.312500e+00\n 4.312500e+00 3.125000e-01 2.131250e+01 1.103125e+02 1.713125e+02]\nNew probs: [7.2184761e-05 6.7532572e-05 1.7559207e-05 2.3426261e-02 9.4435208e-02\n 4.6346649e-05 1.9206892e-04 7.3551997e-03 4.9451706e-03 1.0019397e-01\n 2.8814392e-03 3.2654097e-03 1.2278604e-01 7.6588392e-03 9.4267342e-04\n 9.8728180e-02 6.6081369e-03 3.1564072e-05 1.0927072e-02 1.3219835e-01\n 6.9093662e-03 6.1615729e-03 2.4984267e-03 5.8352184e-02 1.4219140e-06\n 2.8873231e-02 1.6093648e-03 3.0302361e-04 1.0118557e-01 3.3103149e-02\n 7.6049037e-02 2.4088955e-04 1.2240850e-02 4.2394118e-04 5.7477353e-04\n 9.9227740e-04 9.6257224e-07 4.8153475e-03 1.8276764e-02 3.0608665e-02]\nCategory counts: [  0   0   0 109 449   1   1  37  31 513  14  11 612  38   4 483  35   0\n  74 618  30  41   8 329   0 144  15   0 508 168 359   2  59   0   1   4\n   0  21 110 171]\nCurrent means: [      nan       nan       nan 44.128983 67.84847  50.338783 50.64495\n 48.13268  48.32005  75.59316  46.94513  48.704247 84.587715 55.583908\n 47.467407 31.394184 46.812496       nan 50.29608  14.447742 49.92968\n 57.55511  47.578293 61.706543       nan 53.847504 46.960564       nan\n 23.44825  42.56272  38.164856 53.207382 46.923294       nan 46.041267\n 51.181522       nan 46.867603 51.76486  57.19459 ]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 9.1659026e-03 2.2266756e-03\n 9.0909088e-01 9.0909088e-01 2.6954180e-02 3.2154340e-02 1.9489379e-03\n 7.0921987e-02 9.0090089e-02 1.6337200e-03 2.6246721e-02 2.4390244e-01\n 2.0699648e-03 2.8490029e-02 1.0000000e+01 1.3495277e-02 1.6178612e-03\n 3.3222590e-02 2.4330901e-02 1.2345678e-01 3.0385901e-03 1.0000000e+01\n 6.9396249e-03 6.6225164e-02 1.0000000e+01 1.9681165e-03 5.9488397e-03\n 2.7847395e-03 4.7619051e-01 1.6920473e-02 1.0000000e+01 9.0909088e-01\n 2.4390244e-01 1.0000000e+01 4.7393363e-02 9.0826526e-03 5.8445353e-03]\nPosterior means: [      nan       nan       nan 44.134365 67.8445   50.307987 50.586323\n 48.13771  48.32545  75.58817  46.966797 48.71592  84.58207  55.569252\n 47.52918  31.398035 46.821575       nan 50.295685 14.453493 49.929913\n 57.53673  47.60819  61.702988       nan 53.844833 46.980694       nan\n 23.453474 42.567142 38.16815  53.05465  46.9285         nan 46.401154\n 51.152706       nan 46.882446 51.763252 57.19039 ]\nResampled means: [      nan       nan       nan 44.130833 67.84028  50.533836 51.756172\n 48.129883 48.31738  75.587    46.836655 48.73281  84.58114  55.554356\n 47.472233 31.399939 46.85263        nan 50.273735 14.455692 49.948486\n 57.52231  47.7081   61.69536        nan 53.85037  47.063522       nan\n 23.452818 42.57208  38.168026 52.75849  46.902187       nan 44.909096\n 51.156498       nan 46.833946 51.761314 57.19112 ]\nChosen means: [52.962532 50.886185 50.256966 44.130833 67.84028  50.533836 51.756172\n 48.129883 48.31738  75.587    46.836655 48.73281  84.58114  55.554356\n 47.472233 31.399939 46.85263  50.835552 50.273735 14.455692 49.948486\n 57.52231  47.7081   61.69536  51.98892  53.85037  47.063522 49.767803\n 23.452818 42.57208  38.168026 52.75849  46.902187 48.25525  44.909096\n 51.156498 51.840244 46.833946 51.761314 57.19112 ]\nCategory counts: [  0   3   0 117 417   1   1  47  22 513  14  20 609  42   7 467  34   0\n  64 615  41  37  13 336   0 151   8   3 509 187 348   3  48   0   1   1\n   0  32 101 188]\nNew alpha: [3.125000e-01 3.312500e+00 3.125000e-01 1.173125e+02 4.173125e+02\n 1.312500e+00 1.312500e+00 4.731250e+01 2.231250e+01 5.133125e+02\n 1.431250e+01 2.031250e+01 6.093125e+02 4.231250e+01 7.312500e+00\n 4.673125e+02 3.431250e+01 3.125000e-01 6.431250e+01 6.153125e+02\n 4.131250e+01 3.731250e+01 1.331250e+01 3.363125e+02 3.125000e-01\n 1.513125e+02 8.312500e+00 3.312500e+00 5.093125e+02 1.873125e+02\n 3.483125e+02 3.312500e+00 4.831250e+01 3.125000e-01 1.312500e+00\n 1.312500e+00 3.125000e-01 3.231250e+01 1.013125e+02 1.883125e+02]\nNew probs: [2.88106935e-06 1.52018154e-04 3.44513028e-05 2.70696767e-02\n 8.32427442e-02 7.74487780e-05 4.45622136e-05 8.31453130e-03\n 3.50612449e-03 9.77649987e-02 2.90313596e-03 3.17832804e-03\n 1.27452344e-01 8.19270965e-03 1.81918754e-03 8.89649093e-02\n 7.14152632e-03 2.70216347e-04 1.12701990e-02 1.23697855e-01\n 8.37881118e-03 9.91108548e-03 4.17949120e-03 6.56166300e-02\n 7.39883180e-05 3.13556194e-02 1.14286214e-03 4.55620844e-04\n 9.03932303e-02 4.31565754e-02 7.43773878e-02 1.31575123e-03\n 1.08288871e-02 2.67955911e-05 9.20890816e-05 3.06321715e-04\n 1.44504302e-04 5.33681270e-03 2.17073299e-02 3.61000784e-02]\nCategory counts: [  0   3   0 117 417   1   1  47  22 513  14  20 609  42   7 467  34   0\n  64 615  41  37  13 336   0 151   8   3 509 187 348   3  48   0   1   1\n   0  32 101 188]\nCurrent means: [      nan 51.920284       nan 44.37426  68.16612  50.50239  52.383488\n 48.32953  48.37885  75.642395 46.883125 49.194    84.60794  55.77404\n 47.728447 31.25182  46.745476       nan 50.302452 14.425735 50.08545\n 57.962975 47.39484  62.216373       nan 53.82524  47.233322 49.21819\n 23.406683 42.045944 37.78721  52.925518 46.632603       nan 44.681355\n 50.984196       nan 46.72364  52.019375 57.549618]\nPosterior variance: [1.00000000e+01 3.22580665e-01 1.00000000e+01 8.53971019e-03\n 2.39750650e-03 9.09090877e-01 9.09090877e-01 2.12314241e-02\n 4.52488698e-02 1.94893789e-03 7.09219873e-02 4.97512445e-02\n 1.64176663e-03 2.37529706e-02 1.40845075e-01 2.14086915e-03\n 2.93255150e-02 1.00000000e+01 1.56006245e-02 1.62575196e-03\n 2.43309010e-02 2.69541796e-02 7.63358772e-02 2.97530484e-03\n 1.00000000e+01 6.61813328e-03 1.23456784e-01 3.22580665e-01\n 1.96425058e-03 5.34473546e-03 2.87273759e-03 3.22580665e-01\n 2.07900219e-02 1.00000000e+01 9.09090877e-01 9.09090877e-01\n 1.00000000e+01 3.11526489e-02 9.89119709e-03 5.31632081e-03]\nPosterior means: [      nan 51.85834        nan 44.379063 68.161766 50.456722 52.16681\n 48.333076 48.386185 75.6374   46.90523  49.19801  84.60226  55.760323\n 47.760437 31.255835 46.755024       nan 50.30198  14.431519 50.085243\n 57.94151  47.414726 62.212738       nan 53.822712 47.26748  49.243412\n 23.411907 42.050198 37.790718 52.83115  46.639606       nan 45.16487\n 50.894726       nan 46.73385  52.01738  57.5456  ]\nResampled means: [      nan 51.324306       nan 44.376503 68.15907  50.699078 52.886883\n 48.36277  48.34537  75.63562  46.873127 49.13327  84.60377  55.77431\n 47.84722  31.257137 46.734478       nan 50.270996 14.431129 50.080894\n 57.947437 47.54765  62.208347       nan 53.827984 47.44052  49.51603\n 23.40898  42.05283  37.797546 52.545055 46.646217       nan 45.787285\n 51.148026       nan 46.754395 52.019157 57.53841 ]\nChosen means: [52.962532 51.324306 50.256966 44.376503 68.15907  50.699078 52.886883\n 48.36277  48.34537  75.63562  46.873127 49.13327  84.60377  55.77431\n 47.84722  31.257137 46.734478 50.835552 50.270996 14.431129 50.080894\n 57.947437 47.54765  62.208347 51.98892  53.827984 47.44052  49.51603\n 23.40898  42.05283  37.797546 52.545055 46.646217 48.25525  45.787285\n 51.148026 51.840244 46.754395 52.019157 57.53841 ]\nCategory counts: [  0   1   0 123 383   1   1  37  17 514  11  21 606  50  12 441  30   1\n  62 617  31  52  23 366   0 163   9   2 507 217 337   4  70   0   1   2\n   0  20 104 164]\nNew alpha: [3.125000e-01 1.312500e+00 3.125000e-01 1.233125e+02 3.833125e+02\n 1.312500e+00 1.312500e+00 3.731250e+01 1.731250e+01 5.143125e+02\n 1.131250e+01 2.131250e+01 6.063125e+02 5.031250e+01 1.231250e+01\n 4.413125e+02 3.031250e+01 1.312500e+00 6.231250e+01 6.173125e+02\n 3.131250e+01 5.231250e+01 2.331250e+01 3.663125e+02 3.125000e-01\n 1.633125e+02 9.312500e+00 2.312500e+00 5.073125e+02 2.173125e+02\n 3.373125e+02 4.312500e+00 7.031250e+01 3.125000e-01 1.312500e+00\n 2.312500e+00 3.125000e-01 2.031250e+01 1.043125e+02 1.643125e+02]\nNew probs: [2.18680580e-04 3.65563668e-04 2.08969068e-05 2.63083763e-02\n 7.73552209e-02 1.00433317e-04 2.29720798e-04 9.27761756e-03\n 2.02215137e-03 1.06505141e-01 2.30623758e-03 5.33590931e-03\n 1.16724677e-01 1.09118475e-02 1.71987235e-03 8.95362496e-02\n 9.07636341e-03 2.61166628e-04 1.16285020e-02 1.17904104e-01\n 6.38938183e-03 9.25990753e-03 4.38194489e-03 6.95883408e-02\n 1.57432296e-05 3.36295143e-02 1.87595235e-03 1.30905572e-03\n 1.01365454e-01 4.36724350e-02 6.92752078e-02 9.39358317e-04\n 1.36687178e-02 1.80969888e-04 2.94700236e-04 4.59343428e-05\n 2.19483276e-08 4.03690245e-03 1.99624207e-02 3.22987773e-02]\nCategory counts: [  0   1   0 123 383   1   1  37  17 514  11  21 606  50  12 441  30   1\n  62 617  31  52  23 366   0 163   9   2 507 217 337   4  70   0   1   2\n   0  20 104 164]\nCurrent means: [      nan 50.559174       nan 44.32967  68.51212  50.493614 52.27214\n 48.45697  48.423725 75.68019  47.13719  48.94205  84.63175  55.883125\n 47.685627 31.048399 46.83685  52.178345 50.371357 14.441557 50.186546\n 58.02097  47.666798 62.513256       nan 53.92306  47.308746 48.37358\n 23.42206  41.686703 37.25731  52.565727 46.75865        nan 46.429462\n 51.548576       nan 46.35463  51.840897 57.686813]\nPosterior variance: [1.0000000e+01 9.0909088e-01 1.0000000e+01 8.1234770e-03 2.6102844e-03\n 9.0909088e-01 9.0909088e-01 2.6954180e-02 5.8479533e-02 1.9451469e-03\n 9.0090089e-02 4.7393363e-02 1.6498928e-03 1.9960081e-02 8.2644626e-02\n 2.2670596e-03 3.3222590e-02 9.0909088e-01 1.6103059e-02 1.6204830e-03\n 3.2154340e-02 1.9193858e-02 4.3290041e-02 2.7314941e-03 1.0000000e+01\n 6.1312076e-03 1.0989010e-01 4.7619051e-01 1.9719976e-03 4.6061720e-03\n 2.9664787e-03 2.4390244e-01 1.4265335e-02 1.0000000e+01 9.0909088e-01\n 4.7619051e-01 1.0000000e+01 4.9751244e-02 9.6061481e-03 6.0938448e-03]\nPosterior means: [      nan 50.508343       nan 44.33428  68.50729  50.448742 52.065586\n 48.46113  48.43294  75.6752   47.162983 48.947063 84.62604  55.87138\n 47.704758 31.052694 46.84736  51.980316 50.370758 14.44732  50.185944\n 58.005573 47.6769   62.509834       nan 53.92066  47.33832  48.451027\n 23.427301 41.69053  37.261086 52.50315  46.763275       nan 46.75406\n 51.474834       nan 46.372765 51.83913  57.68213 ]\nResampled means: [      nan 51.47635        nan 44.33099  68.50866  51.120132 51.656677\n 48.45733  48.4433   75.67171  47.227142 48.89718  84.627174 55.865555\n 47.713192 31.055859 46.850872 52.59696  50.348305 14.447564 50.186478\n 58.01448  47.63015  62.510685       nan 53.919315 47.04642  48.540627\n 23.42967  41.686077 37.262573 52.18765  46.74556        nan 45.1639\n 51.90899        nan 46.314117 51.831303 57.676975]\nChosen means: [52.962532 51.47635  50.256966 44.33099  68.50866  51.120132 51.656677\n 48.45733  48.4433   75.67171  47.227142 48.89718  84.627174 55.865555\n 47.713192 31.055859 46.850872 52.59696  50.348305 14.447564 50.186478\n 58.01448  47.63015  62.510685 51.98892  53.919315 47.04642  48.540627\n 23.42967  41.686077 37.262573 52.18765  46.74556  48.25525  45.1639\n 51.90899  51.840244 46.314117 51.831303 57.676975]\nCategory counts: [  1   1   0 144 386   1   0  43   7 508  10  33 604  63   6 406  53   2\n  60 612  26  53  19 359   0 159   7   2 509 226 350   7  58   0   3   0\n   0  18 102 162]\nNew alpha: [1.312500e+00 1.312500e+00 3.125000e-01 1.443125e+02 3.863125e+02\n 1.312500e+00 3.125000e-01 4.331250e+01 7.312500e+00 5.083125e+02\n 1.031250e+01 3.331250e+01 6.043125e+02 6.331250e+01 6.312500e+00\n 4.063125e+02 5.331250e+01 2.312500e+00 6.031250e+01 6.123125e+02\n 2.631250e+01 5.331250e+01 1.931250e+01 3.593125e+02 3.125000e-01\n 1.593125e+02 7.312500e+00 2.312500e+00 5.093125e+02 2.263125e+02\n 3.503125e+02 7.312500e+00 5.831250e+01 3.125000e-01 3.312500e+00\n 3.125000e-01 3.125000e-01 1.831250e+01 1.023125e+02 1.623125e+02]\nNew probs: [6.13181182e-05 3.09520168e-04 3.07778478e-08 2.60262862e-02\n 7.55651295e-02 2.23085342e-04 5.94459243e-06 8.09589867e-03\n 9.73823131e-04 1.03210062e-01 1.84325071e-03 5.96961565e-03\n 1.14085093e-01 1.17808394e-02 1.79958367e-03 7.88100138e-02\n 1.09924078e-02 7.25794671e-05 1.30708665e-02 1.32914364e-01\n 4.61537251e-03 1.06584392e-02 5.42533398e-03 7.25701004e-02\n 3.11878030e-05 3.34164836e-02 1.45495380e-03 6.37155608e-04\n 1.12550065e-01 4.47362661e-02 6.57488331e-02 1.64330937e-03\n 9.74102318e-03 4.65584963e-05 1.07976503e-03 1.27418476e-04\n 1.62076700e-04 4.55974042e-03 1.91918239e-02 2.57946551e-02]\nCategory counts: [  1   1   0 144 386   1   0  43   7 508  10  33 604  63   6 406  53   2\n  60 612  26  53  19 359   0 159   7   2 509 226 350   7  58   0   3   0\n   0  18 102 162]\nCurrent means: [54.3871   50.55954        nan 44.317703 68.630325 50.30497        nan\n 48.620304 48.929607 75.75384  47.935772 49.126724 84.64553  55.98511\n 48.347797 30.701487 46.603413 52.59606  50.69306  14.405601 50.3466\n 58.208164 47.945    62.625423       nan 53.999004 46.72098  48.532288\n 23.357365 41.40605  36.737125 52.45171  46.86581        nan 46.04463\n       nan       nan 46.474888 51.66863  57.87637 ]\nPosterior variance: [9.0909088e-01 9.0909088e-01 1.0000000e+01 6.9396249e-03 2.5900025e-03\n 9.0909088e-01 1.0000000e+01 2.3201857e-02 1.4084508e-01 1.9681165e-03\n 9.9009894e-02 3.0211482e-02 1.6553551e-03 1.5847862e-02 1.6393442e-01\n 2.4624476e-03 1.8832393e-02 4.7619051e-01 1.6638935e-02 1.6337200e-03\n 3.8314175e-02 1.8832393e-02 5.2356020e-02 2.7847395e-03 1.0000000e+01\n 6.2853550e-03 1.4084508e-01 4.7619051e-01 1.9642506e-03 4.4228216e-03\n 2.8563268e-03 1.4084508e-01 1.7211704e-02 1.0000000e+01 3.2258067e-01\n 1.0000000e+01 1.0000000e+01 5.5248618e-02 9.7943190e-03 6.1690314e-03]\nPosterior means: [53.988277 50.508675       nan 44.321648 68.625496 50.27725        nan\n 48.62351  48.94468  75.74877  47.956207 49.129364 84.6398   55.975628\n 48.37488  30.706238 46.60981  52.47244  50.69191  14.411416 50.34527\n 58.192707 47.95576  62.621906       nan 53.99649  46.767162 48.602177\n 23.362598 41.40985  36.740913 52.417175 46.871204       nan 46.172222\n       nan       nan 46.494366 51.666996 57.87151 ]\nResampled means: [54.785324 50.362576       nan 44.331    68.62418  49.09445        nan\n 48.60738  48.81573  75.7471   47.928257 49.175262 84.6419   55.950485\n 48.471313 30.706743 46.609196 53.070488 50.673042 14.408944 50.355675\n 58.185616 47.99141  62.622765       nan 53.995655 46.570087 48.61542\n 23.363451 41.41095  36.748146 52.456608 46.838444       nan 45.713333\n       nan       nan 46.55952  51.664837 57.874477]\nChosen means: [54.785324 50.362576 50.256966 44.331    68.62418  49.09445  51.656677\n 48.60738  48.81573  75.7471   47.928257 49.175262 84.6419   55.950485\n 48.471313 30.706743 46.609196 53.070488 50.673042 14.408944 50.355675\n 58.185616 47.99141  62.622765 51.98892  53.995655 46.570087 48.61542\n 23.363451 41.41095  36.748146 52.456608 46.838444 48.25525  45.713333\n 51.90899  51.840244 46.55952  51.664837 57.874477]\nCategory counts: [  0   0   0 144 387   2   0  42   0 511   4  27 600  74  15 388  66   1\n  55 612  28  70  32 347   0 169   4   3 504 239 360   9  51   0   6   1\n   0  16  87 146]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 1.443125e+02 3.873125e+02\n 2.312500e+00 3.125000e-01 4.231250e+01 3.125000e-01 5.113125e+02\n 4.312500e+00 2.731250e+01 6.003125e+02 7.431250e+01 1.531250e+01\n 3.883125e+02 6.631250e+01 1.312500e+00 5.531250e+01 6.123125e+02\n 2.831250e+01 7.031250e+01 3.231250e+01 3.473125e+02 3.125000e-01\n 1.693125e+02 4.312500e+00 3.312500e+00 5.043125e+02 2.393125e+02\n 3.603125e+02 9.312500e+00 5.131250e+01 3.125000e-01 6.312500e+00\n 1.312500e+00 3.125000e-01 1.631250e+01 8.731250e+01 1.463125e+02]\nNew probs: [5.5022940e-07 3.0267096e-04 6.3524590e-07 2.5555480e-02 7.6980419e-02\n 3.8213024e-04 2.2580285e-04 9.7997943e-03 1.5557227e-04 1.0730188e-01\n 1.2567432e-03 6.2593408e-03 1.2909627e-01 1.3676280e-02 2.3612233e-03\n 7.6512471e-02 1.2365999e-02 4.3235955e-04 1.1849331e-02 1.1437607e-01\n 5.3246575e-03 1.4966677e-02 7.7714068e-03 6.8627082e-02 2.5861489e-05\n 3.4210093e-02 1.0923138e-03 9.7642234e-04 9.8569468e-02 4.6043143e-02\n 7.4139550e-02 2.2035085e-03 8.7487129e-03 5.3070381e-04 6.9546833e-04\n 2.7074464e-04 3.6979185e-05 2.6738311e-03 1.5866427e-02 2.8336383e-02]\n</pre> <pre>Category counts: [  0   0   0 144 387   2   0  42   0 511   4  27 600  74  15 388  66   1\n  55 612  28  70  32 347   0 169   4   3 504 239 360   9  51   0   6   1\n   0  16  87 146]\nCurrent means: [      nan       nan       nan 44.282413 68.63991  48.861847       nan\n 48.586407       nan 75.79298  49.593864 49.029007 84.676704 56.07272\n 48.902897 30.439249 46.74512  52.309032 50.762714 14.405591 50.465843\n 58.44057  47.96418  62.70321        nan 53.952553 47.039585 48.471508\n 23.323893 41.30079  36.492466 52.45295  46.961685       nan 46.240482\n 50.837963       nan 46.528984 51.62077  58.051525]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 6.9396249e-03 2.5833119e-03\n 4.7619051e-01 1.0000000e+01 2.3752971e-02 1.0000000e+01 1.9565641e-03\n 2.4390244e-01 3.6900368e-02 1.6663891e-03 1.3495277e-02 6.6225164e-02\n 2.5766555e-03 1.5128593e-02 9.0909088e-01 1.8148821e-02 1.6337200e-03\n 3.5587188e-02 1.4265335e-02 3.1152649e-02 2.8810140e-03 1.0000000e+01\n 5.9136604e-03 2.4390244e-01 3.2258067e-01 1.9837334e-03 4.1823504e-03\n 2.7770062e-03 1.0989010e-01 1.9569471e-02 1.0000000e+01 1.6393442e-01\n 9.0909088e-01 1.0000000e+01 6.2111799e-02 1.1481057e-02 6.8446267e-03]\nPosterior means: [       nan        nan        nan 44.28638   68.63509   48.916042\n        nan 48.589764         nan 75.787926  49.60377   49.032593\n 84.67092   56.064526  48.910164  30.444288  46.750042  52.099125\n 50.76133   14.4114065 50.464188  58.428528  47.970524  62.699547\n        nan 53.950214  47.111794  48.520817  23.329184  41.30443\n 36.496216  52.425995  46.967632         nan 46.302113  50.761787\n        nan 46.550545  51.618904  58.046013 ]\nResampled means: [      nan       nan       nan 44.293953 68.63869  48.450962       nan\n 48.620197       nan 75.78464  49.74146  49.001488 84.67144  56.06142\n 48.852425 30.443342 46.73002  53.002316 50.779625 14.410494 50.44956\n 58.400265 47.98959  62.696518       nan 53.962646 47.183113 48.478283\n 23.329332 41.30088  36.49556  52.604538 46.95366        nan 46.496227\n 50.840443       nan 46.470547 51.62973  58.052776]\nChosen means: [54.785324 50.362576 50.256966 44.293953 68.63869  48.450962 51.656677\n 48.620197 48.81573  75.78464  49.74146  49.001488 84.67144  56.06142\n 48.852425 30.443342 46.73002  53.002316 50.779625 14.410494 50.44956\n 58.400265 47.98959  62.696518 51.98892  53.962646 47.183113 48.478283\n 23.329332 41.30088  36.49556  52.604538 46.95366  48.25525  46.496227\n 50.840443 51.840244 46.470547 51.62973  58.052776]\nCategory counts: [  0   1   0 140 400   0   0  49   1 506  10  28 593  90   9 388  64   2\n  70 608  28  77  34 328   0 162   8   5 503 245 357  12  44   1   6   4\n   0  14  71 142]\nNew alpha: [3.125000e-01 1.312500e+00 3.125000e-01 1.403125e+02 4.003125e+02\n 3.125000e-01 3.125000e-01 4.931250e+01 1.312500e+00 5.063125e+02\n 1.031250e+01 2.831250e+01 5.933125e+02 9.031250e+01 9.312500e+00\n 3.883125e+02 6.431250e+01 2.312500e+00 7.031250e+01 6.083125e+02\n 2.831250e+01 7.731250e+01 3.431250e+01 3.283125e+02 3.125000e-01\n 1.623125e+02 8.312500e+00 5.312500e+00 5.033125e+02 2.453125e+02\n 3.573125e+02 1.231250e+01 4.431250e+01 1.312500e+00 6.312500e+00\n 4.312500e+00 3.125000e-01 1.431250e+01 7.131250e+01 1.423125e+02]\nNew probs: [8.65303155e-05 1.07864886e-04 1.22180239e-07 2.94431131e-02\n 8.23498666e-02 2.63679802e-04 3.16001979e-06 8.67303368e-03\n 1.01903963e-04 1.08072788e-01 1.54459744e-03 4.93239472e-03\n 1.20051794e-01 1.65823121e-02 1.99768948e-03 7.67828226e-02\n 1.30979568e-02 2.48043216e-04 1.19200209e-02 1.22810282e-01\n 5.37806470e-03 1.58272851e-02 6.96109375e-03 5.89160807e-02\n 1.11304384e-04 3.57171856e-02 1.55356701e-03 1.25211233e-03\n 9.67356339e-02 4.75406460e-02 7.14384615e-02 2.03004666e-03\n 1.02129392e-02 9.92069719e-04 1.98390405e-03 5.62202302e-04\n 1.15492078e-07 2.21090531e-03 1.44404713e-02 2.70660110e-02]\nCategory counts: [  0   1   0 140 400   0   0  49   1 506  10  28 593  90   9 388  64   2\n  70 608  28  77  34 328   0 162   8   5 503 245 357  12  44   1   6   4\n   0  14  71 142]\nCurrent means: [      nan 48.677013       nan 44.258507 68.74148        nan       nan\n 48.541954 48.374683 75.93782  49.58125  48.93934  84.729935 56.051\n 48.688225 30.354279 46.465496 52.450592 50.728645 14.375331 50.836857\n 58.902634 47.92643  62.81496        nan 53.957386 46.950874 47.955303\n 23.25353  41.18593  36.40016  52.650166 47.088028 48.392933 46.459087\n 50.88317        nan 46.087627 51.77462  58.270557]\nPosterior variance: [1.00000000e+01 9.09090877e-01 1.00000000e+01 7.13775866e-03\n 2.49937503e-03 1.00000000e+01 1.00000000e+01 2.03665998e-02\n 9.09090877e-01 1.97589397e-03 9.90098938e-02 3.55871879e-02\n 1.68605638e-03 1.10987797e-02 1.09890103e-01 2.57665548e-03\n 1.56006245e-02 4.76190507e-01 1.42653352e-02 1.64446642e-03\n 3.55871879e-02 1.29701691e-02 2.93255150e-02 3.04785115e-03\n 1.00000000e+01 6.16903137e-03 1.23456784e-01 1.96078435e-01\n 1.98767637e-03 4.07996727e-03 2.80033611e-03 8.26446265e-02\n 2.26757377e-02 9.09090877e-01 1.63934425e-01 2.43902445e-01\n 1.00000000e+01 7.09219873e-02 1.40646975e-02 7.03729736e-03]\nPosterior means: [      nan 48.797287       nan 44.262608 68.73679        nan       nan\n 48.54492  48.52244  75.93269  49.585392 48.943115 84.72408  56.04428\n 48.70264  30.359339 46.47101  52.333897 50.727604 14.381188 50.83388\n 58.891083 47.932514 62.811054       nan 53.954945 46.988518 47.995396\n 23.258848 41.189526 36.40397  52.628265 47.094635 48.539032 46.51713\n 50.861633       nan 46.115376 51.772125 58.26474 ]\nResampled means: [      nan 48.72506        nan 44.268845 68.73629        nan       nan\n 48.53182  47.863136 75.929985 49.62591  48.951447 84.72761  56.028175\n 48.478615 30.357594 46.456688 51.997906 50.72084  14.38114  50.798203\n 58.890335 47.908867 62.812965       nan 53.94899  46.9622   47.79916\n 23.260815 41.192127 36.40196  52.73834  47.065575 48.437866 46.705574\n 50.72838        nan 46.1161   51.78049  58.25899 ]\nChosen means: [54.785324 48.72506  50.256966 44.268845 68.73629  48.450962 51.656677\n 48.53182  47.863136 75.929985 49.62591  48.951447 84.72761  56.028175\n 48.478615 30.357594 46.456688 51.997906 50.72084  14.38114  50.798203\n 58.890335 47.908867 62.812965 51.98892  53.94899  46.9622   47.79916\n 23.260815 41.192127 36.40196  52.73834  47.065575 48.437866 46.705574\n 50.72838  51.840244 46.1161   51.78049  58.25899 ]\nCategory counts: [  0   0   0 162 406   1   0  48   0 510   5  27 580 103   7 392  60   1\n  75 606  22  94  25 306   1 157   5   6 499 250 345  14  55   4  12   5\n   0   5  75 137]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 1.623125e+02 4.063125e+02\n 1.312500e+00 3.125000e-01 4.831250e+01 3.125000e-01 5.103125e+02\n 5.312500e+00 2.731250e+01 5.803125e+02 1.033125e+02 7.312500e+00\n 3.923125e+02 6.031250e+01 1.312500e+00 7.531250e+01 6.063125e+02\n 2.231250e+01 9.431250e+01 2.531250e+01 3.063125e+02 1.312500e+00\n 1.573125e+02 5.312500e+00 6.312500e+00 4.993125e+02 2.503125e+02\n 3.453125e+02 1.431250e+01 5.531250e+01 4.312500e+00 1.231250e+01\n 5.312500e+00 3.125000e-01 5.312500e+00 7.531250e+01 1.373125e+02]\nNew probs: [4.9959187e-08 6.5020358e-09 4.1206917e-08 3.4166224e-02 8.0338001e-02\n 4.9619435e-04 3.2374068e-05 9.3826829e-03 5.1125240e-05 1.1110215e-01\n 1.1903753e-03 4.9133571e-03 1.1225811e-01 1.6696367e-02 1.7124411e-03\n 8.4942862e-02 1.3559049e-02 7.4127027e-05 1.3177190e-02 1.1769284e-01\n 3.4338483e-03 2.2185523e-02 5.3121876e-03 5.9541505e-02 1.5263635e-05\n 3.3372790e-02 7.3269889e-04 1.5462791e-03 9.6982867e-02 5.0306659e-02\n 6.5639980e-02 2.5012041e-03 1.0402044e-02 1.1461738e-03 1.6975332e-03\n 9.7154395e-04 1.3496401e-04 9.8722242e-04 1.6126288e-02 2.5177965e-02]\nCategory counts: [  0   0   0 162 406   1   0  48   0 510   5  27 580 103   7 392  60   1\n  75 606  22  94  25 306   1 157   5   6 499 250 345  14  55   4  12   5\n   0   5  75 137]\nCurrent means: [      nan       nan       nan 44.26426  68.8368   50.236065       nan\n 48.477955       nan 76.11437  50.150463 49.20505  84.828384 56.16548\n 48.921223 30.281796 46.72175  52.76932  50.819344 14.362005 50.695602\n 59.386803 47.876183 63.042503 54.09299  53.920414 46.611393 48.271236\n 23.192081 40.98868  36.290417 52.727264 47.082947 48.519367 46.799526\n 50.996468       nan 45.89495  51.883    58.425987]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 6.1690314e-03 2.4624476e-03\n 9.0909088e-01 1.0000000e+01 2.0790022e-02 1.0000000e+01 1.9603998e-03\n 1.9607843e-01 3.6900368e-02 1.7238408e-03 9.6993214e-03 1.4084508e-01\n 2.5503698e-03 1.6638935e-02 9.0909088e-01 1.3315580e-02 1.6498928e-03\n 4.5248870e-02 1.0626993e-02 3.9840639e-02 3.2669061e-03 9.0909088e-01\n 6.3653723e-03 1.9607843e-01 1.6393442e-01 2.0036064e-03 3.9984006e-03\n 2.8977108e-03 7.0921987e-02 1.8148821e-02 2.4390244e-01 8.2644626e-02\n 1.9607843e-01 1.0000000e+01 1.9607843e-01 1.3315580e-02 7.2939456e-03]\nPosterior means: [      nan       nan       nan 44.2678   68.83216  50.214607       nan\n 48.48112        nan 76.10925  50.147514 49.207985 84.82238  56.159504\n 48.936413 30.286825 46.727203 52.517567 50.818253 14.367885 50.692455\n 59.376827 47.884647 63.038242 53.720905 53.917915 46.677837 48.299576\n 23.197453 40.992287 36.29439  52.707924 47.08824  48.55548  46.825977\n 50.97693        nan 45.97544  51.880493 58.41984 ]\nResampled means: [      nan       nan       nan 44.26481  68.83198  49.345673       nan\n 48.470993       nan 76.10928  49.913254 49.12682  84.82178  56.149105\n 48.968414 30.286142 46.716167 50.389515 50.818615 14.368418 50.73428\n 59.379944 47.861496 63.034462 55.074005 53.91642  46.808556 48.34504\n 23.195333 40.988327 36.29487  52.71478  47.09159  48.283222 46.81593\n 50.776936       nan 45.950222 51.898148 58.426094]\nChosen means: [54.785324 48.72506  50.256966 44.26481  68.83198  49.345673 51.656677\n 48.470993 47.863136 76.10928  49.913254 49.12682  84.82178  56.149105\n 48.968414 30.286142 46.716167 50.389515 50.818615 14.368418 50.73428\n 59.379944 47.861496 63.034462 55.074005 53.91642  46.808556 48.34504\n 23.195333 40.988327 36.29487  52.71478  47.09159  48.283222 46.81593\n 50.776936 51.840244 45.950222 51.898148 58.426094]\nCategory counts: [  0   0   0 173 419   2   0  49   0 515   8  18 560 115  14 407  73   0\n  52 595  19 125  21 278   0 154   3   7 494 260 330  15  41  10   9   5\n   0   4 101 124]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 1.733125e+02 4.193125e+02\n 2.312500e+00 3.125000e-01 4.931250e+01 3.125000e-01 5.153125e+02\n 8.312500e+00 1.831250e+01 5.603125e+02 1.153125e+02 1.431250e+01\n 4.073125e+02 7.331250e+01 3.125000e-01 5.231250e+01 5.953125e+02\n 1.931250e+01 1.253125e+02 2.131250e+01 2.783125e+02 3.125000e-01\n 1.543125e+02 3.312500e+00 7.312500e+00 4.943125e+02 2.603125e+02\n 3.303125e+02 1.531250e+01 4.131250e+01 1.031250e+01 9.312500e+00\n 5.312500e+00 3.125000e-01 4.312500e+00 1.013125e+02 1.243125e+02]\nNew probs: [5.74024625e-06 3.15531332e-04 3.57416830e-05 3.27434428e-02\n 9.07265991e-02 3.86525528e-04 1.71038373e-05 1.20109795e-02\n 1.76165292e-06 9.75727513e-02 6.40950922e-04 3.28610628e-03\n 1.02545552e-01 2.14902870e-02 2.21244828e-03 9.25975740e-02\n 1.68150216e-02 2.54406160e-07 9.86529328e-03 1.24923728e-01\n 2.85546109e-03 2.24736109e-02 3.63580673e-03 5.67867346e-02\n 9.43873383e-05 3.03390864e-02 8.42640293e-04 1.25434971e-03\n 9.44193080e-02 5.65310977e-02 6.39531091e-02 2.26973579e-03\n 8.35783035e-03 2.63884431e-03 1.12579623e-03 9.93307563e-04\n 2.08154244e-10 9.51728667e-04 1.52432276e-02 2.70403661e-02]\nCategory counts: [  0   0   0 173 419   2   0  49   0 515   8  18 560 115  14 407  73   0\n  52 595  19 125  21 278   0 154   3   7 494 260 330  15  41  10   9   5\n   0   4 101 124]\nCurrent means: [      nan       nan       nan 44.21294  68.98006  48.244038       nan\n 48.57217        nan 76.39027  49.95064  49.21184  84.984344 56.27346\n 49.05907  30.137125 46.812183       nan 50.955772 14.281114 50.466686\n 59.81618  47.545956 63.32053        nan 53.97194  46.39124  48.35056\n 22.977398 40.802254 36.16964  52.023277 47.162632 48.391277 47.299946\n 50.282494       nan 47.178276 51.764126 58.43635 ]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 5.7770074e-03 2.3860654e-03\n 4.7619051e-01 1.0000000e+01 2.0366600e-02 1.0000000e+01 1.9413707e-03\n 1.2345678e-01 5.5248618e-02 1.7853956e-03 8.6880978e-03 7.0921987e-02\n 2.4563989e-03 1.3679891e-02 1.0000000e+01 1.9193858e-02 1.6803899e-03\n 5.2356020e-02 7.9936050e-03 4.7393363e-02 3.5958288e-03 1.0000000e+01\n 6.4892923e-03 3.2258067e-01 1.4084508e-01 2.0238818e-03 3.8446751e-03\n 3.0293849e-03 6.6225164e-02 2.4330901e-02 9.9009894e-02 1.0989010e-01\n 1.9607843e-01 1.0000000e+01 2.4390244e-01 9.8911971e-03 8.0580181e-03]\nPosterior means: [      nan       nan       nan 44.216286 68.975525 48.327652       nan\n 48.575077       nan 76.38514  49.951252 49.216198 84.978096 56.268013\n 49.065746 30.142004 46.816547       nan 50.953938 14.287116 50.464245\n 59.80833  47.557583 63.31574        nan 53.969364 46.507652 48.373787\n 22.982866 40.80579  36.17383  52.00988  47.169537 48.4072   47.32962\n 50.276955       nan 47.2471   51.762383 58.429554]\nResampled means: [      nan       nan       nan 44.21199  68.977234 47.88106        nan\n 48.5615         nan 76.38218  50.009598 49.313316 84.9772   56.26208\n 49.079582 30.141233 46.814335       nan 50.998215 14.288943 50.601223\n 59.812035 47.580566 63.31232        nan 53.96958  46.331688 48.26493\n 22.9811   40.804367 36.18016  52.03869  47.169132 48.48611  47.36634\n 50.199207       nan 47.075287 51.74989  58.438393]\nChosen means: [54.785324 48.72506  50.256966 44.21199  68.977234 47.88106  51.656677\n 48.5615   47.863136 76.38218  50.009598 49.313316 84.9772   56.26208\n 49.079582 30.141233 46.814335 50.389515 50.998215 14.288943 50.601223\n 59.812035 47.580566 63.31232  55.074005 53.96958  46.331688 48.26493\n 22.9811   40.804367 36.18016  52.03869  47.169132 48.48611  47.36634\n 50.199207 51.840244 47.075287 51.74989  58.438393]\nCategory counts: [  0   1   0 186 445   0   0  54   0 507   5  22 532 122  12 426  79   0\n  64 580  12 128  22 273   1 144   3  10 487 254 321  22  43  12   4   7\n   0   3  83 136]\nNew alpha: [3.125000e-01 1.312500e+00 3.125000e-01 1.863125e+02 4.453125e+02\n 3.125000e-01 3.125000e-01 5.431250e+01 3.125000e-01 5.073125e+02\n 5.312500e+00 2.231250e+01 5.323125e+02 1.223125e+02 1.231250e+01\n 4.263125e+02 7.931250e+01 3.125000e-01 6.431250e+01 5.803125e+02\n 1.231250e+01 1.283125e+02 2.231250e+01 2.733125e+02 1.312500e+00\n 1.443125e+02 3.312500e+00 1.031250e+01 4.873125e+02 2.543125e+02\n 3.213125e+02 2.231250e+01 4.331250e+01 1.231250e+01 4.312500e+00\n 7.312500e+00 3.125000e-01 3.312500e+00 8.331250e+01 1.363125e+02]\nNew probs: [2.80429322e-06 1.81943644e-04 9.73285933e-06 3.57217528e-02\n 8.98862183e-02 1.03743474e-07 7.27040315e-05 1.15019511e-02\n 3.98485921e-04 1.03315681e-01 7.49598083e-04 2.75973906e-03\n 1.06978238e-01 2.51188092e-02 1.92470034e-03 8.18696171e-02\n 1.42404065e-02 6.34031494e-06 1.47428839e-02 1.16038099e-01\n 2.59371079e-03 2.93435864e-02 3.47060990e-03 5.51026911e-02\n 3.09708557e-04 2.99854819e-02 4.79521579e-04 2.44962028e-03\n 9.40896422e-02 5.23621067e-02 6.60327673e-02 3.30943568e-03\n 9.25188418e-03 3.47817922e-03 6.33124553e-04 1.21755700e-03\n 9.23704574e-05 3.65160522e-04 1.41465487e-02 2.57662330e-02]\nCategory counts: [  0   1   0 186 445   0   0  54   0 507   5  22 532 122  12 426  79   0\n  64 580  12 128  22 273   1 144   3  10 487 254 321  22  43  12   4   7\n   0   3  83 136]\nCurrent means: [      nan 48.224773       nan 44.049427 69.33452        nan       nan\n 48.586113       nan 76.88929  50.48852  49.562607 85.22019  56.170803\n 48.88553  29.94153  46.61958        nan 50.90063  14.166531 51.151443\n 60.042255 47.853283 63.549507 54.080788 53.989964 46.77264  48.215797\n 22.67945  40.57391  36.037235 52.287537 46.94919  48.87729  47.423836\n 50.096897       nan 47.220306 51.81302  58.468895]\nPosterior variance: [1.00000000e+01 9.09090877e-01 1.00000000e+01 5.37345512e-03\n 2.24668602e-03 1.00000000e+01 1.00000000e+01 1.84842888e-02\n 1.00000000e+01 1.97199755e-03 1.96078435e-01 4.52488698e-02\n 1.87934609e-03 8.19000788e-03 8.26446265e-02 2.34686700e-03\n 1.26422253e-02 1.00000000e+01 1.56006245e-02 1.72384083e-03\n 8.26446265e-02 7.80640077e-03 4.52488698e-02 3.66166234e-03\n 9.09090877e-01 6.93962490e-03 3.22580665e-01 9.90098938e-02\n 2.05296651e-03 3.93545860e-03 3.11429449e-03 4.52488698e-02\n 2.32018568e-02 8.26446265e-02 2.43902445e-01 1.40845075e-01\n 1.00000000e+01 3.22580665e-01 1.20336944e-02 7.34753814e-03]\nPosterior means: [      nan 48.38616        nan 44.052624 69.33017        nan       nan\n 48.58873        nan 76.88399  50.478943 49.564583 85.21358  56.16575\n 48.89474  29.946238 46.62385        nan 50.899223 14.172708 51.14193\n 60.034416 47.862995 63.544544 53.70981  53.987194 46.87675  48.23346\n 22.685059 40.57762  36.04158  52.277184 46.95627  48.88657  47.48667\n 50.09553        nan 47.309975 51.81084  58.462673]\nResampled means: [      nan 48.517757       nan 44.050083 69.32965        nan       nan\n 48.58809        nan 76.8844   50.43929  49.565693 85.21376  56.158504\n 48.87032  29.946695 46.607834       nan 50.90474  14.172882 51.01044\n 60.040585 47.870296 63.54323  52.854137 53.988655 46.597153 48.191963\n 22.686249 40.57778  36.043633 52.25087  46.963257 48.82206  47.190994\n 50.283943       nan 47.756027 51.807682 58.463276]\nChosen means: [54.785324 48.517757 50.256966 44.050083 69.32965  47.88106  51.656677\n 48.58809  47.863136 76.8844   50.43929  49.565693 85.21376  56.158504\n 48.87032  29.946695 46.607834 50.389515 50.90474  14.172882 51.01044\n 60.040585 47.870296 63.54323  52.854137 53.988655 46.597153 48.191963\n 22.686249 40.57778  36.043633 52.25087  46.963257 48.82206  47.190994\n 50.283943 51.840244 47.756027 51.807682 58.463276]\nCategory counts: [  0   0   0 187 441   0   0  63   4 504   7   6 502 122   8 453  75   0\n  77 562  19 154  20 293   1 171   1  10 475 267 305  14  49  15   2   7\n   0   2  67 117]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 1.873125e+02 4.413125e+02\n 3.125000e-01 3.125000e-01 6.331250e+01 4.312500e+00 5.043125e+02\n 7.312500e+00 6.312500e+00 5.023125e+02 1.223125e+02 8.312500e+00\n 4.533125e+02 7.531250e+01 3.125000e-01 7.731250e+01 5.623125e+02\n 1.931250e+01 1.543125e+02 2.031250e+01 2.933125e+02 1.312500e+00\n 1.713125e+02 1.312500e+00 1.031250e+01 4.753125e+02 2.673125e+02\n 3.053125e+02 1.431250e+01 4.931250e+01 1.531250e+01 2.312500e+00\n 7.312500e+00 3.125000e-01 2.312500e+00 6.731250e+01 1.173125e+02]\nNew probs: [7.09650340e-06 1.83198736e-05 1.14397597e-04 3.48398425e-02\n 8.63475204e-02 1.40617340e-05 7.28057756e-08 1.07375896e-02\n 1.21475896e-03 1.04338519e-01 1.15982268e-03 8.00847600e-04\n 1.06258914e-01 2.34691277e-02 2.06163968e-03 9.13328081e-02\n 1.59997996e-02 1.57997158e-04 1.35226781e-02 1.05034478e-01\n 3.22956499e-03 2.88475659e-02 4.06102045e-03 6.06048405e-02\n 4.57637972e-04 3.27504836e-02 2.46213749e-05 1.79761450e-03\n 9.41155851e-02 5.56505769e-02 6.28353655e-02 1.77561468e-03\n 8.04054551e-03 2.61475542e-03 1.49044383e-03 1.19895034e-03\n 1.66104379e-04 9.06577450e-04 1.62409842e-02 2.57608406e-02]\nCategory counts: [  0   0   0 187 441   0   0  63   4 504   7   6 502 122   8 453  75   0\n  77 562  19 154  20 293   1 171   1  10 475 267 305  14  49  15   2   7\n   0   2  67 117]\nCurrent means: [      nan       nan       nan 43.923756 69.83612        nan       nan\n 48.509785 47.89221  77.3927   49.40949  48.98676  85.48665  56.266586\n 48.468006 29.686811 46.63543        nan 50.841892 14.025418 50.821384\n 60.23792  47.7693   64.026245 52.705917 53.939037 46.64162  48.106464\n 22.28939  40.37375  35.860214 52.296173 46.936718 48.68881  47.494473\n 50.590763       nan 48.45289  51.737633 58.453934]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 5.3447355e-03 2.2670596e-03\n 1.0000000e+01 1.0000000e+01 1.5847862e-02 2.4390244e-01 1.9837334e-03\n 1.4084508e-01 1.6393442e-01 1.9916352e-03 8.1900079e-03 1.2345678e-01\n 2.2070182e-03 1.3315580e-02 1.0000000e+01 1.2970169e-02 1.7790430e-03\n 5.2356020e-02 6.4892923e-03 4.9751244e-02 3.4118048e-03 9.0909088e-01\n 5.8445353e-03 9.0909088e-01 9.9009894e-02 2.1048200e-03 3.7439161e-03\n 3.2776138e-03 7.0921987e-02 2.0366600e-02 6.6225164e-02 4.7619051e-01\n 1.4084508e-01 1.0000000e+01 4.7619051e-01 1.4903130e-02 8.5397102e-03]\nPosterior means: [      nan       nan       nan 43.927006 69.83163        nan       nan\n 48.512146 47.94362  77.38726  49.417805 49.00337  85.479576 56.26145\n 48.48692  29.691296 46.639908       nan 50.840797 14.031817 50.817085\n 60.231277 47.780396 64.02146  52.459927 53.936737 46.94693  48.12521\n 22.295223 40.377354 35.86485  52.27989  46.942955 48.697495 47.61378\n 50.58244        nan 48.526558 51.735046 58.446716]\nResampled means: [      nan       nan       nan 43.926414 69.83049        nan       nan\n 48.483227 47.84457  77.38935  49.23747  49.086277 85.48034  56.252865\n 48.56688  29.686947 46.626675       nan 50.84313  14.033538 50.946392\n 60.22171  47.765667 64.02113  52.524822 53.93591  46.681522 48.02826\n 22.294777 40.375267 35.86552  52.230457 46.93764  48.691357 48.026367\n 50.673035       nan 49.046455 51.741783 58.453773]\nChosen means: [54.785324 48.517757 50.256966 43.926414 69.83049  47.88106  51.656677\n 48.483227 47.84457  77.38935  49.23747  49.086277 85.48034  56.252865\n 48.56688  29.686947 46.626675 50.389515 50.84313  14.033538 50.946392\n 60.22171  47.765667 64.02113  52.524822 53.93591  46.681522 48.02826\n 22.294777 40.375267 35.86552  52.230457 46.93764  48.691357 48.026367\n 50.673035 51.840244 49.046455 51.741783 58.453773]\nCategory counts: [  0   0   0 203 400   0   0  44  13 500  12   8 500 130  12 455  81   2\n  75 527  15 146  19 333   3 155   0   5 485 263 313  10  47  12   4   4\n   0   8  87 129]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 2.033125e+02 4.003125e+02\n 3.125000e-01 3.125000e-01 4.431250e+01 1.331250e+01 5.003125e+02\n 1.231250e+01 8.312500e+00 5.003125e+02 1.303125e+02 1.231250e+01\n 4.553125e+02 8.131250e+01 2.312500e+00 7.531250e+01 5.273125e+02\n 1.531250e+01 1.463125e+02 1.931250e+01 3.333125e+02 3.312500e+00\n 1.553125e+02 3.125000e-01 5.312500e+00 4.853125e+02 2.633125e+02\n 3.133125e+02 1.031250e+01 4.731250e+01 1.231250e+01 4.312500e+00\n 4.312500e+00 3.125000e-01 8.312500e+00 8.731250e+01 1.293125e+02]\nNew probs: [4.9949818e-13 1.4943035e-07 6.6575382e-08 4.1952372e-02 8.5524663e-02\n 5.1285926e-04 6.4538490e-09 7.7770674e-03 3.7312214e-03 1.0055984e-01\n 2.7382523e-03 2.0116167e-03 1.0221276e-01 2.4806760e-02 1.7664705e-03\n 9.6070290e-02 1.6504044e-02 4.3739934e-04 1.8986214e-02 9.9815868e-02\n 2.7849060e-03 2.8823545e-02 3.4886771e-03 6.0641844e-02 8.4518711e-04\n 3.2398392e-02 3.7109229e-04 9.1513933e-04 1.0045167e-01 5.1131386e-02\n 5.7238638e-02 1.6108911e-03 9.6134301e-03 1.2128513e-03 2.8153794e-04\n 6.0813525e-04 6.9596456e-07 1.8938705e-03 1.6075103e-02 2.4205066e-02]\nCategory counts: [  0   0   0 203 400   0   0  44  13 500  12   8 500 130  12 455  81   2\n  75 527  15 146  19 333   3 155   0   5 485 263 313  10  47  12   4   4\n   0   8  87 129]\nCurrent means: [      nan       nan       nan 43.779316 70.27104        nan       nan\n 48.43407  47.824604 77.4616   49.09482  49.27849  85.505264 56.342457\n 48.087402 29.345306 46.639038 50.407413 50.927307 13.741527 50.573505\n 60.41481  47.504684 64.4172   51.702713 53.98133        nan 47.838524\n 21.79433  40.10696  35.507427 52.560955 47.005585 48.646725 48.266712\n 50.46382        nan 49.48398  51.709164 58.534435]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 4.9236827e-03 2.4993750e-03\n 1.0000000e+01 1.0000000e+01 2.2675738e-02 7.6335877e-02 1.9996001e-03\n 8.2644626e-02 1.2345678e-01 1.9996001e-03 7.6863947e-03 8.2644626e-02\n 2.1973192e-03 1.2330457e-02 4.7619051e-01 1.3315580e-02 1.8971734e-03\n 6.6225164e-02 6.8446267e-03 5.2356020e-02 3.0021013e-03 3.2258067e-01\n 6.4474531e-03 1.0000000e+01 1.9607843e-01 2.0614306e-03 3.8008362e-03\n 3.1938676e-03 9.9009894e-02 2.1231424e-02 8.2644626e-02 2.4390244e-01\n 2.4390244e-01 1.0000000e+01 1.2345678e-01 1.1481057e-02 7.7459328e-03]\nPosterior means: [      nan       nan       nan 43.782383 70.265976       nan       nan\n 48.437626 47.84121  77.45611  49.102303 49.287395 85.49816  56.33758\n 48.10321  29.349844 46.643185 50.388012 50.92607  13.748405 50.56971\n 60.40768  47.51775  64.412865 51.64779  53.978764       nan 47.880905\n 21.800146 40.11072  35.512054 52.535595 47.01194  48.65791  48.30899\n 50.45251        nan 49.49035  51.707195 58.52783 ]\nResampled means: [      nan       nan       nan 43.783382 70.26693        nan       nan\n 48.45525  47.874165 77.45567  49.03182  49.291134 85.49722  56.334892\n 48.101994 29.352394 46.643364 50.022465 50.92127  13.748005 50.57531\n 60.399902 47.525337 64.41111  51.772026 53.974533       nan 47.608585\n 21.801092 40.106842 35.51583  52.671757 46.97978  48.666348 48.26028\n 50.652542       nan 49.535423 51.70479  58.538273]\nChosen means: [54.785324 48.517757 50.256966 43.783382 70.26693  47.88106  51.656677\n 48.45525  47.874165 77.45567  49.03182  49.291134 85.49722  56.334892\n 48.101994 29.352394 46.643364 50.022465 50.92127  13.748005 50.57531\n 60.399902 47.525337 64.41111  51.772026 53.974533 46.681522 47.608585\n 21.801092 40.106842 35.51583  52.671757 46.97978  48.666348 48.26028\n 50.652542 51.840244 49.535423 51.70479  58.538273]\nCategory counts: [  0   0   0 214 381   3   0  44  22 496  13   9 499 141   8 442  86   2\n  84 501  16 167  16 338   6 153   1   3 499 272 314   8  52   3   1   4\n   0   7  70 125]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 2.143125e+02 3.813125e+02\n 3.312500e+00 3.125000e-01 4.431250e+01 2.231250e+01 4.963125e+02\n 1.331250e+01 9.312500e+00 4.993125e+02 1.413125e+02 8.312500e+00\n 4.423125e+02 8.631250e+01 2.312500e+00 8.431250e+01 5.013125e+02\n 1.631250e+01 1.673125e+02 1.631250e+01 3.383125e+02 6.312500e+00\n 1.533125e+02 1.312500e+00 3.312500e+00 4.993125e+02 2.723125e+02\n 3.143125e+02 8.312500e+00 5.231250e+01 3.312500e+00 1.312500e+00\n 4.312500e+00 3.125000e-01 7.312500e+00 7.031250e+01 1.253125e+02]\nNew probs: [5.5907103e-06 6.2187646e-05 6.8134519e-05 5.0386868e-02 7.2158583e-02\n 2.2513649e-04 2.0889744e-04 6.7379922e-03 4.0450436e-03 9.8244518e-02\n 3.5109459e-03 2.5528576e-03 9.8781064e-02 2.8623892e-02 1.2686024e-03\n 8.6860046e-02 1.4883361e-02 5.6168222e-04 1.4298958e-02 1.0015413e-01\n 3.1936751e-03 3.4063544e-02 3.6306994e-03 6.6614963e-02 9.0740394e-04\n 3.2160938e-02 1.2642148e-04 7.3079928e-04 9.9814013e-02 5.3025357e-02\n 6.3014135e-02 1.5182944e-03 1.1054564e-02 8.1649300e-04 8.0712831e-05\n 2.7004543e-03 9.1134338e-05 1.9154303e-03 1.4909351e-02 2.5993394e-02]\nCategory counts: [  0   0   0 214 381   3   0  44  22 496  13   9 499 141   8 442  86   2\n  84 501  16 167  16 338   6 153   1   3 499 272 314   8  52   3   1   4\n   0   7  70 125]\nCurrent means: [      nan       nan       nan 43.608616 70.54277  48.219234       nan\n 48.420963 48.1218   77.50534  49.01728  49.216335 85.51192  56.252937\n 48.62314  29.072632 46.672    50.668213 50.92214  13.509985 50.57722\n 60.67238  47.786343 64.70131  52.178806 53.86198  46.042553 48.840153\n 21.503424 39.84333  35.060436 52.554306 46.82122  48.492023 48.865356\n 51.927937       nan 49.3791   51.670097 58.465515]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 4.6707145e-03 2.6239832e-03\n 3.2258067e-01 1.0000000e+01 2.2675738e-02 4.5248870e-02 2.0157227e-03\n 7.6335877e-02 1.0989010e-01 2.0036064e-03 7.0871720e-03 1.2345678e-01\n 2.2619315e-03 1.1614402e-02 4.7619051e-01 1.1890607e-02 1.9956096e-03\n 6.2111799e-02 5.9844404e-03 6.2111799e-02 2.9577047e-03 1.6393442e-01\n 6.5316786e-03 9.0909088e-01 3.2258067e-01 2.0036064e-03 3.6751193e-03\n 3.1836994e-03 1.2345678e-01 1.9193858e-02 3.2258067e-01 9.0909088e-01\n 2.4390244e-01 1.0000000e+01 1.4084508e-01 1.4265335e-02 7.9936050e-03]\nPosterior means: [      nan       nan       nan 43.611603 70.537384 48.27668        nan\n 48.42455  48.1303   77.499794 49.02478  49.22495  85.50481  56.2485\n 48.640137 29.077368 46.675865 50.63639  50.92104  13.517267 50.573635\n 60.665993 47.80009  64.69696  52.143085 53.85946  46.402325 48.877567\n 21.509132 39.84706  35.065193 52.52277  46.827324 48.54067  48.96851\n 51.880917       nan 49.387844 51.667713 58.458744]\nResampled means: [      nan       nan       nan 43.617302 70.53343  48.199238       nan\n 48.38795  48.209743 77.495544 48.96209  49.337048 85.50465  56.244225\n 48.624043 29.078491 46.678635 50.71531  50.905518 13.516542 50.530838\n 60.665016 47.87274  64.70049  52.353455 53.86357  46.48974  48.60417\n 21.50624  39.845234 35.066395 52.49889  46.828976 48.101315 50.099075\n 52.108906       nan 49.385258 51.663452 58.459953]\nChosen means: [54.785324 48.517757 50.256966 43.617302 70.53343  48.199238 51.656677\n 48.38795  48.209743 77.495544 48.96209  49.337048 85.50465  56.244225\n 48.624043 29.078491 46.678635 50.71531  50.905518 13.516542 50.530838\n 60.665016 47.87274  64.70049  52.353455 53.86357  46.48974  48.60417\n 21.50624  39.845234 35.066395 52.49889  46.828976 48.101315 50.099075\n 52.108906 51.840244 49.385258 51.663452 58.459953]\nCategory counts: [  0   0   0 240 384   0   1  30  29 487  23  11 501 144   7 406  80   6\n  63 501  25 192  21 312   3 143   0   2 495 264 344   3  49   3   0  17\n   0  11  72 131]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 2.403125e+02 3.843125e+02\n 3.125000e-01 1.312500e+00 3.031250e+01 2.931250e+01 4.873125e+02\n 2.331250e+01 1.131250e+01 5.013125e+02 1.443125e+02 7.312500e+00\n 4.063125e+02 8.031250e+01 6.312500e+00 6.331250e+01 5.013125e+02\n 2.531250e+01 1.923125e+02 2.131250e+01 3.123125e+02 3.312500e+00\n 1.433125e+02 3.125000e-01 2.312500e+00 4.953125e+02 2.643125e+02\n 3.443125e+02 3.312500e+00 4.931250e+01 3.312500e+00 3.125000e-01\n 1.731250e+01 3.125000e-01 1.131250e+01 7.231250e+01 1.313125e+02]\nNew probs: [3.25791291e-07 2.44681132e-06 3.21692496e-05 4.82545905e-02\n 8.52804482e-02 5.00553824e-06 6.75006362e-04 7.38058286e-03\n 5.84495906e-03 9.83613282e-02 6.14352431e-03 2.03070883e-03\n 1.01843886e-01 2.72703320e-02 1.30591972e-03 8.15449506e-02\n 1.78689770e-02 7.00326636e-04 1.33592850e-02 1.00683339e-01\n 4.42511821e-03 3.87859121e-02 4.76672594e-03 6.19496927e-02\n 6.13622251e-04 2.65059173e-02 4.54518304e-05 6.45950553e-04\n 9.37550068e-02 4.88904938e-02 6.89235106e-02 4.20088589e-04\n 8.45643878e-03 5.90828247e-04 1.62154901e-10 2.93524005e-03\n 2.89764248e-05 2.52371025e-03 1.14084650e-02 2.57404931e-02]\nCategory counts: [  0   0   0 240 384   0   1  30  29 487  23  11 501 144   7 406  80   6\n  63 501  25 192  21 312   3 143   0   2 495 264 344   3  49   3   0  17\n   0  11  72 131]\nCurrent means: [      nan       nan       nan 43.60038  70.63417        nan 50.72257\n 48.519573 48.361176 77.536606 49.13186  49.153957 85.49552  56.21049\n 48.2487   28.704706 46.71353  51.225296 50.92542  13.50986  50.689518\n 61.03122  47.996216 64.96309  52.849346 53.903244       nan 48.337147\n 21.47621  39.56952  34.681812 52.485977 46.724266 47.1351         nan\n 51.934          nan 49.618458 51.730812 58.41097 ]\nPosterior variance: [1.00000000e+01 1.00000000e+01 1.00000000e+01 4.16493136e-03\n 2.60348874e-03 1.00000000e+01 9.09090877e-01 3.32225896e-02\n 3.43642607e-02 2.05296651e-03 4.32900414e-02 9.00900885e-02\n 1.99560961e-03 6.93962490e-03 1.40845075e-01 2.46244762e-03\n 1.24843949e-02 1.63934425e-01 1.58478618e-02 1.99560961e-03\n 3.98406386e-02 5.20562194e-03 4.73933630e-02 3.20410123e-03\n 3.22580665e-01 6.98811980e-03 1.00000000e+01 4.76190507e-01\n 2.01979396e-03 3.78644443e-03 2.90613179e-03 3.22580665e-01\n 2.03665998e-02 3.22580665e-01 1.00000000e+01 5.84795326e-02\n 1.00000000e+01 9.00900885e-02 1.38696255e-02 7.62776472e-03]\nPosterior means: [      nan       nan       nan 43.603046 70.6288         nan 50.656883\n 48.52449  48.366806 77.53095  49.13562  49.16158  85.48844  56.20618\n 48.273365 28.70995  46.717636 51.205208 50.923954 13.517142 50.68677\n 61.02548  48.00571  64.9583   52.757435 53.900513       nan 48.41633\n 21.481972 39.573467 34.686264 52.405785 46.730938 47.22752        nan\n 51.922688       nan 49.621895 51.72841  58.404556]\nResampled means: [      nan       nan       nan 43.605865 70.62407        nan 48.865368\n 48.527267 48.334667 77.531235 49.14076  49.17936  85.48706  56.189693\n 48.351044 28.709566 46.733047 51.08135  50.917896 13.517806 50.63491\n 61.030266 48.03768  64.95566  52.91168  53.90314        nan 48.68394\n 21.480673 39.572712 34.68572  52.473522 46.724045 47.082855       nan\n 51.952805       nan 49.749027 51.72708  58.402912]\nChosen means: [54.785324 48.517757 50.256966 43.605865 70.62407  48.199238 48.865368\n 48.527267 48.334667 77.531235 49.14076  49.17936  85.48706  56.189693\n 48.351044 28.709566 46.733047 51.08135  50.917896 13.517806 50.63491\n 61.030266 48.03768  64.95566  52.91168  53.90314  46.48974  48.68394\n 21.480673 39.572712 34.68572  52.473522 46.724045 47.082855 50.099075\n 51.952805 51.840244 49.749027 51.72708  58.402912]\n</pre> <pre>Category counts: [  0   0   0 238 388   0   3  40  34 483  23  13 499 143   6 383  91   4\n  65 501  26 218  18 279   6 145   1   4 495 260 363   4  35   7   0  11\n   0  10  63 141]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 2.383125e+02 3.883125e+02\n 3.125000e-01 3.312500e+00 4.031250e+01 3.431250e+01 4.833125e+02\n 2.331250e+01 1.331250e+01 4.993125e+02 1.433125e+02 6.312500e+00\n 3.833125e+02 9.131250e+01 4.312500e+00 6.531250e+01 5.013125e+02\n 2.631250e+01 2.183125e+02 1.831250e+01 2.793125e+02 6.312500e+00\n 1.453125e+02 1.312500e+00 4.312500e+00 4.953125e+02 2.603125e+02\n 3.633125e+02 4.312500e+00 3.531250e+01 7.312500e+00 3.125000e-01\n 1.131250e+01 3.125000e-01 1.031250e+01 6.331250e+01 1.413125e+02]\nNew probs: [3.4635155e-05 2.8219031e-06 9.7406832e-05 4.7968537e-02 8.0768242e-02\n 3.0099840e-05 5.5328326e-04 7.6829586e-03 6.3383202e-03 9.1058545e-02\n 4.5402637e-03 2.1751572e-03 1.0221734e-01 2.4443300e-02 1.2629405e-03\n 7.9693839e-02 1.9294446e-02 1.3136385e-03 1.1645088e-02 9.8354578e-02\n 3.7392078e-03 4.5674659e-02 4.1715070e-03 5.4907314e-02 1.2310073e-03\n 2.8054828e-02 5.8268208e-05 7.8204012e-04 9.6263528e-02 5.3029835e-02\n 7.9439983e-02 2.0693346e-04 6.9700917e-03 1.0893432e-03 9.5995591e-04\n 1.6939358e-03 1.3488378e-05 2.0413008e-03 1.1068141e-02 2.9129606e-02]\nCategory counts: [  0   0   0 238 388   0   3  40  34 483  23  13 499 143   6 383  91   4\n  65 501  26 218  18 279   6 145   1   4 495 260 363   4  35   7   0  11\n   0  10  63 141]\nCurrent means: [      nan       nan       nan 43.47454  70.70658        nan 49.58031\n 48.555317 48.17765  77.59376  49.263752 49.05468  85.51204  56.19612\n 47.46115  28.490477 46.593445 51.40793  51.00254  13.509324 50.930656\n 61.38901  47.733627 65.25762  52.106464 53.848    48.241695 49.778008\n 21.477484 39.46377  34.501064 52.70704  46.481365 47.326714       nan\n 51.93296        nan 49.75189  51.796474 58.47957 ]\nPosterior variance: [1.0000000e+01 1.0000000e+01 1.0000000e+01 4.1999160e-03 2.5766555e-03\n 1.0000000e+01 3.2258067e-01 2.4937658e-02 2.9325515e-02 2.0699648e-03\n 4.3290041e-02 7.6335877e-02 2.0036064e-03 6.9881198e-03 1.6393442e-01\n 2.6102844e-03 1.0976949e-02 2.4390244e-01 1.5360983e-02 1.9956096e-03\n 3.8314175e-02 4.5850524e-03 5.5248618e-02 3.5829451e-03 1.6393442e-01\n 6.8917987e-03 9.0909088e-01 2.4390244e-01 2.0197940e-03 3.8446751e-03\n 2.7540622e-03 2.4390244e-01 2.8490029e-02 1.4084508e-01 1.0000000e+01\n 9.0090089e-02 1.0000000e+01 9.9009894e-02 1.5847862e-02 7.0871720e-03]\nPosterior means: [      nan       nan       nan 43.47728  70.70124        nan 49.59385\n 48.558918 48.183    77.58805  49.26694  49.061893 85.50493  56.191788\n 47.50277  28.49609  46.597183 51.373592 51.001    13.516606 50.92709\n 61.383793 47.74615  65.25215  52.07193  53.845345 48.401543 49.783424\n 21.483246 39.467823 34.505333 52.641014 46.49139  47.364365       nan\n 51.915546       nan 49.75434  51.79363  58.473557]\nResampled means: [       nan        nan        nan 43.47686   70.7027           nan\n 49.827885  48.59339   48.160206  77.58965   49.281433  49.194508\n 85.50393   56.186115  47.539604  28.497272  46.58078   51.442986\n 50.97834   13.5168495 50.928905  61.39363   47.71127   65.249214\n 52.279945  53.8373    49.04661   49.99173   21.482107  39.46611\n 34.50521   52.413742  46.48437   47.582157         nan 51.891113\n        nan 49.85986   51.811466  58.478897 ]\nChosen means: [54.785324  48.517757  50.256966  43.47686   70.7027    48.199238\n 49.827885  48.59339   48.160206  77.58965   49.281433  49.194508\n 85.50393   56.186115  47.539604  28.497272  46.58078   51.442986\n 50.97834   13.5168495 50.928905  61.39363   47.71127   65.249214\n 52.279945  53.8373    49.04661   49.99173   21.482107  39.46611\n 34.50521   52.413742  46.48437   47.582157  50.099075  51.891113\n 51.840244  49.85986   51.811466  58.478897 ]\nCategory counts: [  0   0   0 239 391   1   4  42  35 477  30  10 500 146  12 387  95   8\n  56 500  17 242  17 264   7 143   0   7 486 261 362   1  31   4   4   9\n   0  10  73 129]\nNew alpha: [3.125000e-01 3.125000e-01 3.125000e-01 2.393125e+02 3.913125e+02\n 1.312500e+00 4.312500e+00 4.231250e+01 3.531250e+01 4.773125e+02\n 3.031250e+01 1.031250e+01 5.003125e+02 1.463125e+02 1.231250e+01\n 3.873125e+02 9.531250e+01 8.312500e+00 5.631250e+01 5.003125e+02\n 1.731250e+01 2.423125e+02 1.731250e+01 2.643125e+02 7.312500e+00\n 1.433125e+02 3.125000e-01 7.312500e+00 4.863125e+02 2.613125e+02\n 3.623125e+02 1.312500e+00 3.131250e+01 4.312500e+00 4.312500e+00\n 9.312500e+00 3.125000e-01 1.031250e+01 7.331250e+01 1.293125e+02]\nNew probs: [3.0511830e-05 3.0462563e-04 1.4171136e-06 4.5171276e-02 7.0965245e-02\n 8.0417485e-05 8.3926145e-04 8.4822774e-03 6.2672999e-03 8.8894308e-02\n 5.7717143e-03 2.9643842e-03 1.0190670e-01 2.5496799e-02 1.8141996e-03\n 8.0302268e-02 1.6538337e-02 1.2334916e-03 1.1002403e-02 1.0415430e-01\n 2.3678588e-03 4.6080895e-02 3.9656893e-03 5.5080600e-02 5.3894758e-04\n 3.1780507e-02 6.1700397e-05 2.6583569e-03 9.8780356e-02 5.4839574e-02\n 7.3202141e-02 2.1951186e-05 6.7179217e-03 1.3174739e-03 5.2931532e-04\n 1.8078473e-03 4.1486866e-05 2.6748958e-03 1.9057769e-02 2.6253749e-02]\nCategory counts: [  0   0   0 239 391   1   4  42  35 477  30  10 500 146  12 387  95   8\n  56 500  17 242  17 264   7 143   0   7 486 261 362   1  31   4   4   9\n   0  10  73 129]\nCurrent means: [      nan       nan       nan 43.374447 70.75929  50.715942 48.891457\n 48.759766 47.982426 77.62051  49.14893  49.33862  85.505264 56.312946\n 47.39384  28.344835 46.528946 51.624245 51.096897 13.500216 50.6362\n 61.483482 47.80845  65.45166  51.58659  53.951187       nan 50.28064\n 21.402939 39.35617  34.404587 50.810223 46.44624  47.691452 50.625137\n 51.745853       nan 49.89942  51.78929  58.292572]\nPosterior variance: [1.00000000e+01 1.00000000e+01 1.00000000e+01 4.18235036e-03\n 2.55689071e-03 9.09090877e-01 2.43902445e-01 2.37529706e-02\n 2.84900293e-02 2.09599664e-03 3.32225896e-02 9.90098938e-02\n 1.99960009e-03 6.84462674e-03 8.26446265e-02 2.58331187e-03\n 1.05152475e-02 1.23456784e-01 1.78253129e-02 1.99960009e-03\n 5.84795326e-02 4.13052458e-03 5.84795326e-02 3.78644443e-03\n 1.40845075e-01 6.98811980e-03 1.00000000e+01 1.40845075e-01\n 2.05718982e-03 3.82995023e-03 2.76166806e-03 9.09090877e-01\n 3.21543403e-02 2.43902445e-01 2.43902445e-01 1.09890103e-01\n 1.00000000e+01 9.90098938e-02 1.36798909e-02 7.74593279e-03]\nPosterior means: [      nan       nan       nan 43.37722  70.75398  50.65086  48.918495\n 48.76271  47.988174 77.61472  49.151756 49.345165 85.49816  56.308624\n 47.415382 28.350431 46.532597 51.60419  51.09494  13.507514 50.63248\n 61.478737 47.821266 65.4458   51.56424  53.94842        nan 50.276684\n 21.408821 39.360245 34.408894 50.73657  46.457664 47.74776  50.609894\n 51.72667        nan 49.900414 51.786846 58.28615 ]\nResampled means: [      nan       nan       nan 43.38024  70.75457  50.042118 48.812843\n 48.755695 47.99515  77.613914 49.18082  49.475952 85.49539  56.309254\n 47.584938 28.352097 46.52681  51.462982 51.084747 13.509335 50.724102\n 61.485626 47.914547 65.44864  51.461273 53.940235       nan 50.01835\n 21.40581  39.35029  34.405388 50.99337  46.43412  47.726963 50.618633\n 51.7432         nan 50.12273  51.793316 58.286106]\nChosen means: [54.785324 48.517757 50.256966 43.38024  70.75457  50.042118 48.812843\n 48.755695 47.99515  77.613914 49.18082  49.475952 85.49539  56.309254\n 47.584938 28.352097 46.52681  51.462982 51.084747 13.509335 50.724102\n 61.485626 47.914547 65.44864  51.461273 53.940235 49.04661  50.01835\n 21.40581  39.35029  34.405388 50.99337  46.43412  47.726963 50.618633\n 51.7432   51.840244 50.12273  51.793316 58.286106]\nCategory counts: [  0   1   0 233 378   0   5  34  43 476  35  10 499 124  10 394  88   4\n  53 499  13 245  13 270   3 158   0  10 482 279 346   0  35   6   2   8\n   0  16  81 147]\nNew alpha: [3.125000e-01 1.312500e+00 3.125000e-01 2.333125e+02 3.783125e+02\n 3.125000e-01 5.312500e+00 3.431250e+01 4.331250e+01 4.763125e+02\n 3.531250e+01 1.031250e+01 4.993125e+02 1.243125e+02 1.031250e+01\n 3.943125e+02 8.831250e+01 4.312500e+00 5.331250e+01 4.993125e+02\n 1.331250e+01 2.453125e+02 1.331250e+01 2.703125e+02 3.312500e+00\n 1.583125e+02 3.125000e-01 1.031250e+01 4.823125e+02 2.793125e+02\n 3.463125e+02 3.125000e-01 3.531250e+01 6.312500e+00 2.312500e+00\n 8.312500e+00 3.125000e-01 1.631250e+01 8.131250e+01 1.473125e+02]\nNew probs: [1.40619022e-06 4.74512752e-04 1.59367308e-04 4.18216661e-02\n 7.45415688e-02 8.56891447e-09 1.27747236e-03 8.71269405e-03\n 6.82738516e-03 9.82691646e-02 9.60949808e-03 2.10076664e-03\n 1.02366745e-01 2.44948957e-02 2.10511149e-03 7.59700239e-02\n 1.73265487e-02 2.41883346e-04 1.25310859e-02 1.01850301e-01\n 2.18132068e-03 4.61555123e-02 1.54434261e-03 5.70653155e-02\n 7.88126781e-04 2.88868379e-02 4.25003564e-05 3.09779611e-03\n 9.96703357e-02 5.31056225e-02 6.63886443e-02 6.45245236e-05\n 7.87119474e-03 8.89332674e-04 7.88078294e-04 2.39211298e-03\n 8.10353754e-07 3.60024697e-03 1.64868105e-02 2.82984544e-02]\nCategory counts: [  0   1   0 233 378   0   5  34  43 476  35  10 499 124  10 394  88   4\n  53 499  13 245  13 270   3 158   0  10 482 279 346   0  35   6   2   8\n   0  16  81 147]\nCurrent means: [      nan 50.236065       nan 43.39756  70.87479        nan 48.07522\n 48.63822  47.90253  77.64544  49.04522  49.846497 85.512146 56.357567\n 47.0487   28.314146 46.558346 51.39997  51.20949  13.493739 50.945477\n 61.570293 48.086613 65.683044 50.867588 53.947598       nan 49.707726\n 21.356606 39.257587 34.328957       nan 46.18873  47.523926 50.42093\n 51.829453       nan 49.840355 51.712902 58.300385]\nPosterior variance: [1.0000000e+01 9.0909088e-01 1.0000000e+01 4.2900043e-03 2.6448029e-03\n 1.0000000e+01 1.9607843e-01 2.9325515e-02 2.3201857e-02 2.1003990e-03\n 2.8490029e-02 9.9009894e-02 2.0036064e-03 8.0580181e-03 9.9009894e-02\n 2.5374270e-03 1.1350738e-02 2.4390244e-01 1.8832393e-02 2.0036064e-03\n 7.6335877e-02 4.0799673e-03 7.6335877e-02 3.7023325e-03 3.2258067e-01\n 6.3251103e-03 1.0000000e+01 9.9009894e-02 2.0742584e-03 3.5829451e-03\n 2.8893382e-03 1.0000000e+01 2.8490029e-02 1.6393442e-01 4.7619051e-01\n 1.2345678e-01 1.0000000e+01 6.2111799e-02 1.2330457e-02 6.7980965e-03]\nPosterior means: [      nan 50.214607       nan 43.400394 70.86926        nan 48.11296\n 48.64222  47.907402 77.63963  49.04794  49.84801  85.505035 56.352444\n 47.077915 28.319649 46.562252 51.365826 51.207214 13.501054 50.93826\n 61.565575 48.101215 65.67723  50.839603 53.9451         nan 49.710617\n 21.362549 39.261436 34.333485       nan 46.199585 47.564514 50.400883\n 51.806866       nan 49.841347 51.710793 58.294743]\nResampled means: [      nan 50.79293        nan 43.41106  70.86659        nan 48.36641\n 48.603058 47.89624  77.6405   49.02691  49.77925  85.50556  56.33808\n 47.03825  28.322071 46.58067  51.29116  51.208427 13.500567 50.91504\n 61.563812 48.169834 65.67794  50.623196 53.947845       nan 49.530136\n 21.361748 39.25987  34.331394       nan 46.282684 47.61319  50.91863\n 51.804142       nan 49.788506 51.721226 58.29742 ]\nChosen means: [54.785324 50.79293  50.256966 43.41106  70.86659  50.042118 48.36641\n 48.603058 47.89624  77.6405   49.02691  49.77925  85.50556  56.33808\n 47.03825  28.322071 46.58067  51.29116  51.208427 13.500567 50.91504\n 61.563812 48.169834 65.67794  50.623196 53.947845 49.04661  49.530136\n 21.361748 39.25987  34.331394 50.99337  46.282684 47.61319  50.91863\n 51.804142 51.840244 49.788506 51.721226 58.29742 ]\nCategory counts: [  1   3   0 228 370   0   6  35  22 471  42  11 500 133   5 392  99   1\n  67 500   6 244   5 282   2 142   1  10 481 280 346   1  47   7   2  11\n   0  21  81 145]\nNew alpha: [1.312500e+00 3.312500e+00 3.125000e-01 2.283125e+02 3.703125e+02\n 3.125000e-01 6.312500e+00 3.531250e+01 2.231250e+01 4.713125e+02\n 4.231250e+01 1.131250e+01 5.003125e+02 1.333125e+02 5.312500e+00\n 3.923125e+02 9.931250e+01 1.312500e+00 6.731250e+01 5.003125e+02\n 6.312500e+00 2.443125e+02 5.312500e+00 2.823125e+02 2.312500e+00\n 1.423125e+02 1.312500e+00 1.031250e+01 4.813125e+02 2.803125e+02\n 3.463125e+02 1.312500e+00 4.731250e+01 7.312500e+00 2.312500e+00\n 1.131250e+01 3.125000e-01 2.131250e+01 8.131250e+01 1.453125e+02]\nNew probs: [2.43410395e-04 8.36246647e-04 2.96382859e-06 3.98644246e-02\n 7.71295577e-02 5.21818947e-05 1.31754111e-03 6.53072307e-03\n 4.90837870e-03 9.14968848e-02 8.20402708e-03 2.33462709e-03\n 9.96722803e-02 2.56021693e-02 9.38482583e-04 7.28163421e-02\n 2.28638500e-02 1.11488385e-04 1.31668076e-02 1.06436193e-01\n 8.38166045e-04 5.27748279e-02 9.27140820e-04 5.74082471e-02\n 3.52992822e-04 2.55355388e-02 1.35743539e-04 3.36077926e-03\n 9.80808809e-02 4.85726595e-02 6.85935766e-02 1.80162606e-04\n 9.87344421e-03 1.06530020e-03 1.56489783e-04 2.36505014e-03\n 3.21519019e-06 5.24714775e-03 1.60640068e-02 3.39357927e-02]\nCategory counts: [  1   3   0 228 370   0   6  35  22 471  42  11 500 133   5 392  99   1\n  67 500   6 244   5 282   2 142   1  10 481 280 346   1  47   7   2  11\n   0  21  81 145]\nCurrent means: [56.810204 50.38672        nan 43.361927 71.00005        nan 47.912918\n 48.528225 48.190784 77.665436 49.01738  50.496964 85.50273  56.169945\n 46.57189  28.296785 46.609184 52.158653 51.08198  13.500216 50.1309\n 61.572556 47.8621   65.794136 51.897995 54.058453 50.233772 49.086586\n 21.370134 39.22235  34.297623 50.55954  46.289062 47.340183 51.437\n 51.93249        nan 49.616375 51.789425 58.392456]\nPosterior variance: [9.0909088e-01 3.2258067e-01 1.0000000e+01 4.3840418e-03 2.7019724e-03\n 1.0000000e+01 1.6393442e-01 2.8490029e-02 4.5248870e-02 2.1226916e-03\n 2.3752971e-02 9.0090089e-02 1.9996001e-03 7.5131478e-03 1.9607843e-01\n 2.5503698e-03 1.0090818e-02 9.0909088e-01 1.4903130e-02 1.9996001e-03\n 1.6393442e-01 4.0966817e-03 1.9607843e-01 3.5448421e-03 4.7619051e-01\n 7.0372974e-03 9.0909088e-01 9.9009894e-02 2.0785700e-03 3.5701534e-03\n 2.8893382e-03 9.0909088e-01 2.1231424e-02 1.4084508e-01 4.7619051e-01\n 9.0090089e-02 1.0000000e+01 4.7393363e-02 1.2330457e-02 6.8917987e-03]\nPosterior means: [56.191097 50.374245       nan 43.364838 70.994385       nan 47.94713\n 48.532417 48.19897  77.65956  49.01971  50.49249  85.49563  56.16531\n 46.63911  28.30232  46.612602 51.962414 51.08037  13.507514 50.128754\n 61.56782  47.90402  65.788536 51.807613 54.0556   50.212524 49.095627\n 21.376085 39.226196 34.30216  50.508675 46.29694  47.377644 51.368572\n 51.91508        nan 49.61819  51.787224 58.38667 ]\nResampled means: [58.114204 50.720947       nan 43.3703   70.99138        nan 48.09725\n 48.512405 48.24592  77.66202  49.01609  50.587467 85.49391  56.167393\n 46.613876 28.300611 46.611897 52.33351  51.09628  13.509682 50.307884\n 61.5702   48.084915 65.78104  52.36102  54.05704  50.195206 49.23861\n 21.373919 39.228596 34.29975  50.328022 46.296467 47.315247 51.49997\n 51.87056        nan 49.583504 51.80251  58.386223]\nChosen means: [58.114204 50.720947 50.256966 43.3703   70.99138  50.042118 48.09725\n 48.512405 48.24592  77.66202  49.01609  50.587467 85.49391  56.167393\n 46.613876 28.300611 46.611897 52.33351  51.09628  13.509682 50.307884\n 61.5702   48.084915 65.78104  52.36102  54.05704  50.195206 49.23861\n 21.373919 39.228596 34.29975  50.328022 46.296467 47.315247 51.49997\n 51.87056  51.840244 49.583504 51.80251  58.386223]\nCategory counts: [  3   4   0 225 376   0   8  37  25 454  43  15 500 130   6 385 107   0\n  61 500   8 248   3 288   1 142   0  14 483 279 348   0  45   6   1  11\n   0  26  72 146]\nNew alpha: [3.312500e+00 4.312500e+00 3.125000e-01 2.253125e+02 3.763125e+02\n 3.125000e-01 8.312500e+00 3.731250e+01 2.531250e+01 4.543125e+02\n 4.331250e+01 1.531250e+01 5.003125e+02 1.303125e+02 6.312500e+00\n 3.853125e+02 1.073125e+02 3.125000e-01 6.131250e+01 5.003125e+02\n 8.312500e+00 2.483125e+02 3.312500e+00 2.883125e+02 1.312500e+00\n 1.423125e+02 3.125000e-01 1.431250e+01 4.833125e+02 2.793125e+02\n 3.483125e+02 3.125000e-01 4.531250e+01 6.312500e+00 1.312500e+00\n 1.131250e+01 3.125000e-01 2.631250e+01 7.231250e+01 1.463125e+02]\nNew probs: [7.64535682e-04 6.84952596e-04 8.33735066e-06 4.51777205e-02\n 6.92145228e-02 1.20078628e-04 1.35255919e-03 6.58890605e-03\n 5.10420324e-03 9.34272185e-02 8.56507663e-03 4.58392967e-03\n 1.01584263e-01 2.62229331e-02 9.48858040e-04 8.16225633e-02\n 1.99896358e-02 4.38088546e-06 9.96625796e-03 9.54719558e-02\n 1.85955619e-03 5.08441553e-02 7.35376496e-04 6.06439821e-02\n 3.89003952e-04 2.65437216e-02 1.46178121e-04 2.13034637e-03\n 9.83637720e-02 5.36287911e-02 7.42327273e-02 2.08196838e-09\n 7.70636369e-03 1.95974112e-03 3.38508602e-04 2.39500613e-03\n 5.35312392e-05 5.77141158e-03 1.38524445e-02 2.70023812e-02]\nCategory counts: [  3   4   0 225 376   0   8  37  25 454  43  15 500 130   6 385 107   0\n  61 500   8 248   3 288   1 142   0  14 483 279 348   0  45   6   1  11\n   0  26  72 146]\nCurrent means: [57.7401   51.00595        nan 43.284855 71.22886        nan 47.59931\n 48.559303 48.208828 77.78626  49.237362 50.383904 85.505264 56.219234\n 45.831852 28.284973 46.411674       nan 51.488842 13.500216 50.520035\n 61.618484 48.65305  65.93817  54.100525 53.96384        nan 48.975773\n 21.382845 39.170288 34.22587        nan 46.43498  47.27304  52.271763\n 52.18589        nan 49.89228  51.831333 58.372726]\nPosterior variance: [3.22580665e-01 2.43902445e-01 1.00000000e+01 4.44246968e-03\n 2.65886728e-03 1.00000000e+01 1.23456784e-01 2.69541796e-02\n 3.98406386e-02 2.20215810e-03 2.32018568e-02 6.62251636e-02\n 1.99960009e-03 7.68639473e-03 1.63934425e-01 2.59672804e-03\n 9.33706854e-03 1.00000000e+01 1.63666122e-02 1.99960009e-03\n 1.23456784e-01 4.03063279e-03 3.22580665e-01 3.47101688e-03\n 9.09090877e-01 7.03729736e-03 1.00000000e+01 7.09219873e-02\n 2.06996477e-03 3.58294509e-03 2.87273759e-03 1.00000000e+01\n 2.21729502e-02 1.63934425e-01 9.09090877e-01 9.00900885e-02\n 1.00000000e+01 3.83141749e-02 1.38696255e-02 6.84462674e-03]\nPosterior means: [57.49042  50.98142        nan 43.287838 71.22322        nan 47.62895\n 48.563183 48.215965 77.78014  49.239136 50.381363 85.49816  56.214455\n 45.90018  28.290611 46.415024       nan 51.486404 13.507514 50.513615\n 61.613804 48.696503 65.93264  53.727753 53.961052       nan 48.98304\n 21.388767 39.174168 34.2304         nan 46.442883 47.31774  52.065243\n 52.1662         nan 49.892693 51.82879  58.366997]\n</pre> <pre>Resampled means: [56.998425 50.860054       nan 43.280018 71.223854       nan 47.59126\n 48.565487 48.17486  77.77967  49.211807 50.30891  85.49646  56.19884\n 45.90986  28.292175 46.40763        nan 51.469707 13.507615 50.51911\n 61.618145 47.864704 65.92994  54.092648 53.954678       nan 49.02344\n 21.387598 39.172733 34.228004       nan 46.412285 47.197365 52.49266\n 52.111626       nan 49.916267 51.839035 58.371906]\nChosen means: [56.998425 50.860054 50.256966 43.280018 71.223854 50.042118 47.59126\n 48.565487 48.17486  77.77967  49.211807 50.30891  85.49646  56.19884\n 45.90986  28.292175 46.40763  52.33351  51.469707 13.507615 50.51911\n 61.618145 47.864704 65.92994  54.092648 53.954678 50.195206 49.02344\n 21.387598 39.172733 34.228004 50.328022 46.412285 47.197365 52.49266\n 52.111626 51.840244 49.916267 51.839035 58.371906]\nCategory counts: [  6   3   0 225 377   0   4  33  31 439  40  20 499 137   5 389 108   0\n  49 499  16 248   5 301   5 138   1   8 482 292 336   0  41  12   1  11\n   1  30  67 141]\nNew alpha: [6.312500e+00 3.312500e+00 3.125000e-01 2.253125e+02 3.773125e+02\n 3.125000e-01 4.312500e+00 3.331250e+01 3.131250e+01 4.393125e+02\n 4.031250e+01 2.031250e+01 4.993125e+02 1.373125e+02 5.312500e+00\n 3.893125e+02 1.083125e+02 3.125000e-01 4.931250e+01 4.993125e+02\n 1.631250e+01 2.483125e+02 5.312500e+00 3.013125e+02 5.312500e+00\n 1.383125e+02 1.312500e+00 8.312500e+00 4.823125e+02 2.923125e+02\n 3.363125e+02 3.125000e-01 4.131250e+01 1.231250e+01 1.312500e+00\n 1.131250e+01 1.312500e+00 3.031250e+01 6.731250e+01 1.413125e+02]\nNew probs: [9.6678309e-04 3.9508296e-04 7.9507205e-05 4.7341980e-02 7.0675232e-02\n 1.2246127e-05 1.8909659e-03 5.2449168e-03 5.9702019e-03 8.6146668e-02\n 8.2107345e-03 3.0261944e-03 9.6346140e-02 2.8209724e-02 1.1206995e-03\n 7.7825800e-02 1.8845478e-02 1.4001704e-05 1.1767292e-02 1.0611850e-01\n 3.6945294e-03 5.2061781e-02 8.6982851e-04 6.5180548e-02 5.7372311e-04\n 2.8886149e-02 2.6476305e-04 1.2921882e-03 9.7034678e-02 5.0628766e-02\n 6.5972216e-02 3.1672324e-05 7.4010836e-03 2.7125934e-03 7.9258360e-05\n 1.6530894e-03 3.7436999e-04 5.7086516e-03 1.5905878e-02 2.9466076e-02]\nCategory counts: [  6   3   0 225 377   0   4  33  31 439  40  20 499 137   5 389 108   0\n  49 499  16 248   5 301   5 138   1   8 482 292 336   0  41  12   1  11\n   1  30  67 141]\nCurrent means: [56.644165 50.74463        nan 43.336224 71.47404        nan 47.2819\n 48.63369  48.129745 77.92116  49.187626 50.595375 85.512215 56.344917\n 45.034966 28.281582 46.500076       nan 51.368046 13.493734 50.587547\n 61.64951  48.19483  66.07235  54.69274  53.908894 48.90726  49.114185\n 21.35795  39.102074 34.173122       nan 46.354282 47.69212  54.222744\n 52.231934 50.904926 50.140934 51.950993 58.36867 ]\nPosterior variance: [1.6393442e-01 3.2258067e-01 1.0000000e+01 4.4424697e-03 2.6518165e-03\n 1.0000000e+01 2.4390244e-01 3.0211482e-02 3.2154340e-02 2.2773854e-03\n 2.4937658e-02 4.9751244e-02 2.0036064e-03 7.2939456e-03 1.9607843e-01\n 2.5700333e-03 9.2506940e-03 1.0000000e+01 2.0366600e-02 2.0036064e-03\n 6.2111799e-02 4.0306328e-03 1.9607843e-01 3.3211557e-03 1.9607843e-01\n 7.2411294e-03 9.0909088e-01 1.2345678e-01 2.0742584e-03 3.4234850e-03\n 2.9753048e-03 1.0000000e+01 2.4330901e-02 8.2644626e-02 9.0909088e-01\n 9.0090089e-02 9.0909088e-01 3.3222590e-02 1.4903130e-02 7.0871720e-03]\nPosterior means: [56.53524  50.72061        nan 43.339184 71.468346       nan 47.348194\n 48.63782  48.135757 77.914795 49.18965  50.59241  85.505104 56.34029\n 45.13232  28.287163 46.503315       nan 51.365257 13.501049 50.583897\n 61.644814 48.23023  66.06701  54.600727 53.906063 49.006603 49.125122\n 21.363894 39.105804 34.17783        nan 46.363155 47.711197 53.83886\n 52.211826 50.822662 50.140465 51.948086 58.362736]\nResampled means: [56.413307 49.977238       nan 43.33512  71.472855       nan 47.28313\n 48.580143 48.18255  77.91594  49.162903 50.57166  85.5054   56.34658\n 45.05403  28.291475 46.491127       nan 51.389687 13.496277 50.63157\n 61.646587 48.401085 66.06896  54.81875  53.897095 49.89351  49.084488\n 21.367321 39.110237 34.179935       nan 46.38851  47.673836 53.050323\n 52.16431  50.84937  50.19548  51.93234  58.357082]\nChosen means: [56.413307 49.977238 50.256966 43.33512  71.472855 50.042118 47.28313\n 48.580143 48.18255  77.91594  49.162903 50.57166  85.5054   56.34658\n 45.05403  28.291475 46.491127 52.33351  51.389687 13.496277 50.63157\n 61.646587 48.401085 66.06896  54.81875  53.897095 49.89351  49.084488\n 21.367321 39.110237 34.179935 50.328022 46.38851  47.673836 53.050323\n 52.16431  50.84937  50.19548  51.93234  58.357082]\nCategory counts: [  6   2   0 240 368   0   6  34  36 421  47  18 497 134   4 390  90   0\n  52 501  18 243   0 330   5 145   2   3 481 291 328   0  40  17   0   8\n   0  31  69 143]\nNew alpha: [6.312500e+00 2.312500e+00 3.125000e-01 2.403125e+02 3.683125e+02\n 3.125000e-01 6.312500e+00 3.431250e+01 3.631250e+01 4.213125e+02\n 4.731250e+01 1.831250e+01 4.973125e+02 1.343125e+02 4.312500e+00\n 3.903125e+02 9.031250e+01 3.125000e-01 5.231250e+01 5.013125e+02\n 1.831250e+01 2.433125e+02 3.125000e-01 3.303125e+02 5.312500e+00\n 1.453125e+02 2.312500e+00 3.312500e+00 4.813125e+02 2.913125e+02\n 3.283125e+02 3.125000e-01 4.031250e+01 1.731250e+01 3.125000e-01\n 8.312500e+00 3.125000e-01 3.131250e+01 6.931250e+01 1.433125e+02]\nNew probs: [6.9975248e-04 4.3831341e-04 3.5504536e-06 4.5476526e-02 7.3145725e-02\n 5.2649548e-06 1.8559048e-03 7.2659533e-03 7.9242876e-03 8.5444398e-02\n 8.3000055e-03 4.0000053e-03 1.0537560e-01 2.0916697e-02 1.1278843e-03\n 7.4036710e-02 1.8907314e-02 2.3818222e-07 9.2008635e-03 9.8215386e-02\n 2.9103516e-03 5.2052073e-02 5.0148341e-05 6.5897077e-02 7.7747792e-04\n 3.1257123e-02 5.5249786e-04 5.6265097e-04 9.6693248e-02 5.8242794e-02\n 6.5721817e-02 1.1008720e-04 9.1893012e-03 3.5699783e-03 2.0397223e-05\n 1.0119251e-03 1.2229506e-07 6.9588767e-03 1.2810428e-02 2.9271001e-02]\nCategory counts: [  6   2   0 240 368   0   6  34  36 421  47  18 497 134   4 390  90   0\n  52 501  18 243   0 330   5 145   2   3 481 291 328   0  40  17   0   8\n   0  31  69 143]\nCurrent means: [56.500458 50.115196       nan 43.33164  71.87142        nan 47.20531\n 48.50893  48.187103 78.10368  48.836655 50.78934  85.52416  56.34894\n 44.543365 28.30445  46.488434       nan 51.504326 13.510222 50.78502\n 61.683064       nan 66.292496 54.877052 53.935932 49.92327  49.860413\n 21.382296 38.99134  34.151592       nan 46.298588 47.83155        nan\n 52.664806       nan 50.380665 51.803783 58.491814]\nPosterior variance: [1.63934425e-01 4.76190507e-01 1.00000000e+01 4.16493136e-03\n 2.71665305e-03 1.00000000e+01 1.63934425e-01 2.93255150e-02\n 2.77008321e-02 2.37473287e-03 2.12314241e-02 5.52486181e-02\n 2.01166770e-03 7.45712128e-03 2.43902445e-01 2.56344513e-03\n 1.10987797e-02 1.00000000e+01 1.91938579e-02 1.99560961e-03\n 5.52486181e-02 4.11353353e-03 1.00000000e+01 3.02938488e-03\n 1.96078435e-01 6.89179869e-03 4.76190507e-01 3.22580665e-01\n 2.07856996e-03 3.43524548e-03 3.04785115e-03 1.00000000e+01\n 2.49376576e-02 5.84795326e-02 1.00000000e+01 1.23456784e-01\n 1.00000000e+01 3.21543403e-02 1.44717805e-02 6.98811980e-03]\nPosterior means: [56.39389  50.10971        nan 43.334415 71.86548        nan 47.25112\n 48.51331  48.192127 78.097    48.839123 50.78498  85.51701  56.344208\n 44.676456 28.31001  46.49233        nan 51.50144  13.517505 50.78068\n 61.67826        nan 66.28755  54.781425 53.933216 49.92692  49.864918\n 21.388245 38.995125 34.15642        nan 46.30782  47.84423        nan\n 52.63191        nan 50.37944  51.80117  58.485874]\nResampled means: [56.538956 50.352234       nan 43.333897 71.86322        nan 47.411263\n 48.54042  48.14466  78.09408  48.858044 50.823296 85.51424  56.33515\n 44.689514 28.309277 46.484627       nan 51.482563 13.516183 50.785717\n 61.67584        nan 66.2892   55.02598  53.935493 50.394512 49.27578\n 21.385967 38.98882  34.155155       nan 46.301254 47.837715       nan\n 52.549976       nan 50.301365 51.801132 58.49296 ]\nChosen means: [56.538956 50.352234 50.256966 43.333897 71.86322  50.042118 47.411263\n 48.54042  48.14466  78.09408  48.858044 50.823296 85.51424  56.33515\n 44.689514 28.309277 46.484627 52.33351  51.482563 13.516183 50.785717\n 61.67584  48.401085 66.2892   55.02598  53.935493 50.394512 49.27578\n 21.385967 38.98882  34.155155 50.328022 46.301254 47.837715 53.050323\n 52.549976 50.84937  50.301365 51.801132 58.49296 ]\nCategory counts: [  5   1   0 219 371   0   9  39  44 398  47  24 495 115   7 384 104   0\n  48 499  15 245   0 347   5 152   0   4 490 300 320   0  41   7   0   7\n   0  32  68 158]\nNew alpha: [5.312500e+00 1.312500e+00 3.125000e-01 2.193125e+02 3.713125e+02\n 3.125000e-01 9.312500e+00 3.931250e+01 4.431250e+01 3.983125e+02\n 4.731250e+01 2.431250e+01 4.953125e+02 1.153125e+02 7.312500e+00\n 3.843125e+02 1.043125e+02 3.125000e-01 4.831250e+01 4.993125e+02\n 1.531250e+01 2.453125e+02 3.125000e-01 3.473125e+02 5.312500e+00\n 1.523125e+02 3.125000e-01 4.312500e+00 4.903125e+02 3.003125e+02\n 3.203125e+02 3.125000e-01 4.131250e+01 7.312500e+00 3.125000e-01\n 7.312500e+00 3.125000e-01 3.231250e+01 6.831250e+01 1.583125e+02]\nNew probs: [6.14568940e-04 3.17575250e-05 1.77405236e-05 4.49006334e-02\n 7.04777539e-02 1.91179482e-04 1.27784652e-03 9.63288639e-03\n 9.42455884e-03 8.35168809e-02 1.02422405e-02 4.82206140e-03\n 9.31329876e-02 2.51863953e-02 1.13898702e-03 8.22103098e-02\n 1.60372816e-02 1.15551911e-05 7.99940433e-03 1.02275699e-01\n 2.85451766e-03 4.36407104e-02 4.31873445e-07 6.57288954e-02\n 9.39603371e-04 3.13387588e-02 6.17908197e-04 2.92181066e-04\n 9.78421345e-02 6.07657023e-02 6.40396178e-02 4.62569783e-12\n 7.83742033e-03 1.68831123e-03 1.84474970e-06 3.66242370e-04\n 1.79807048e-05 7.59147247e-03 1.32889207e-02 3.80041972e-02]\nCategory counts: [  5   1   0 219 371   0   9  39  44 398  47  24 495 115   7 384 104   0\n  48 499  15 245   0 347   5 152   0   4 490 300 320   0  41   7   0   7\n   0  32  68 158]\nCurrent means: [56.496956 50.22929        nan 43.21151  72.24021        nan 46.89808\n 48.515617 48.230076 78.34058  49.06494  51.18458  85.53952  56.319077\n 44.775166 28.373095 46.263325       nan 51.470585 13.493625 51.108376\n 61.73073        nan 66.48314  55.253593 53.959602       nan 49.19496\n 21.414316 38.95648  34.11257        nan 46.325092 47.695198       nan\n 52.575386       nan 50.305588 51.70619  58.51723 ]\nPosterior variance: [1.9607843e-01 9.0909088e-01 1.0000000e+01 4.5641260e-03 2.6946913e-03\n 1.0000000e+01 1.0989010e-01 2.5575448e-02 2.2675738e-02 2.5119316e-03\n 2.1231424e-02 4.1493773e-02 2.0197940e-03 8.6880978e-03 1.4084508e-01\n 2.6034887e-03 9.6061481e-03 1.0000000e+01 2.0790022e-02 2.0036064e-03\n 6.6225164e-02 4.0799673e-03 1.0000000e+01 2.8810140e-03 1.9607843e-01\n 6.5746219e-03 1.0000000e+01 2.4390244e-01 2.0403999e-03 3.3322226e-03\n 3.1240238e-03 1.0000000e+01 2.4330901e-02 1.4084508e-01 1.0000000e+01\n 1.4084508e-01 1.0000000e+01 3.1152649e-02 1.4684288e-02 6.3251103e-03]\nPosterior means: [56.369564 50.208447       nan 43.214607 72.23422        nan 46.932167\n 48.519413 48.234093 78.33346  49.066925 51.179665 85.53235  56.313587\n 44.84875  28.378725 46.266914       nan 51.46753  13.500939 51.101036\n 61.725945       nan 66.478386 55.15058  53.956997       nan 49.2146\n 21.420149 38.96016  34.117535       nan 46.334034 47.727657       nan\n 52.539112       nan 50.304634 51.703682 58.51184 ]\nResampled means: [56.6996   51.93298        nan 43.211132 72.23603        nan 46.89099\n 48.520893 48.21903  78.33005  49.09784  51.131424 85.5351   56.329945\n 44.904953 28.37801  46.26097        nan 51.476604 13.499941 51.23373\n 61.734695       nan 66.47641  54.963127 53.955772       nan 49.324627\n 21.419565 38.96012  34.119423       nan 46.35519  47.82891        nan\n 52.861965       nan 50.265095 51.71132  58.519913]\nChosen means: [56.6996   51.93298  50.256966 43.211132 72.23603  50.042118 46.89099\n 48.520893 48.21903  78.33005  49.09784  51.131424 85.5351   56.329945\n 44.904953 28.37801  46.26097  52.33351  51.476604 13.499941 51.23373\n 61.734695 48.401085 66.47641  54.963127 53.955772 50.394512 49.324627\n 21.419565 38.96012  34.119423 50.328022 46.35519  47.82891  53.050323\n 52.861965 50.84937  50.265095 51.71132  58.519913]\nCategory counts: [  3   0   0 232 371   3   7  38  51 389  56  23 490 118   6 384  89   0\n  38 500  18 248   0 347   7 162   3   1 487 309 313   0  42   8   0   4\n   0  20  69 164]\nNew alpha: [3.312500e+00 3.125000e-01 3.125000e-01 2.323125e+02 3.713125e+02\n 3.312500e+00 7.312500e+00 3.831250e+01 5.131250e+01 3.893125e+02\n 5.631250e+01 2.331250e+01 4.903125e+02 1.183125e+02 6.312500e+00\n 3.843125e+02 8.931250e+01 3.125000e-01 3.831250e+01 5.003125e+02\n 1.831250e+01 2.483125e+02 3.125000e-01 3.473125e+02 7.312500e+00\n 1.623125e+02 3.312500e+00 1.312500e+00 4.873125e+02 3.093125e+02\n 3.133125e+02 3.125000e-01 4.231250e+01 8.312500e+00 3.125000e-01\n 4.312500e+00 3.125000e-01 2.031250e+01 6.931250e+01 1.643125e+02]\nNew probs: [3.21530708e-04 7.39561801e-06 2.72797070e-06 4.66700606e-02\n 7.50734210e-02 7.27361417e-04 1.42301631e-03 7.55701726e-03\n 9.05699469e-03 7.75841475e-02 1.20473625e-02 3.73163773e-03\n 9.52834859e-02 2.66124364e-02 8.42647860e-04 8.24900120e-02\n 1.67048480e-02 2.21223573e-09 6.75999466e-03 1.05529778e-01\n 4.88654384e-03 5.01871258e-02 2.52224963e-05 6.41018823e-02\n 1.19888119e-03 3.09709180e-02 5.73549187e-04 5.05295407e-04\n 9.53043476e-02 5.55820763e-02 6.65273815e-02 1.36461892e-07\n 1.07151661e-02 1.90410821e-03 1.11817772e-05 8.39246262e-04\n 1.46143757e-05 4.24549496e-03 1.36724915e-02 3.03088091e-02]\nCategory counts: [  3   0   0 232 371   3   7  38  51 389  56  23 490 118   6 384  89   0\n  38 500  18 248   0 347   7 162   3   1 487 309 313   0  42   8   0   4\n   0  20  69 164]\nCurrent means: [56.463425       nan       nan 43.297874 72.458046 50.300884 47.326252\n 48.419506 48.334663 78.516174 49.055294 50.869194 85.57444  56.46686\n 45.699535 28.34627  46.348454       nan 51.530304 13.500216 50.93369\n 61.939987       nan 66.67784  55.421215 53.972534 51.78404  48.462624\n 21.410202 38.883747 34.012566       nan 46.306858 47.868473       nan\n 52.51217        nan 50.654495 51.716213 58.573505]\nPosterior variance: [3.22580665e-01 1.00000000e+01 1.00000000e+01 4.30848775e-03\n 2.69469130e-03 3.22580665e-01 1.40845075e-01 2.62467209e-02\n 1.95694715e-02 2.57003331e-03 1.78253129e-02 4.32900414e-02\n 2.04039994e-03 8.46740045e-03 1.63934425e-01 2.60348874e-03\n 1.12233451e-02 1.00000000e+01 2.62467209e-02 1.99960009e-03\n 5.52486181e-02 4.03063279e-03 1.00000000e+01 2.88101402e-03\n 1.40845075e-01 6.16903137e-03 3.22580665e-01 9.09090877e-01\n 2.05296651e-03 3.23519879e-03 3.19386763e-03 1.00000000e+01\n 2.37529706e-02 1.23456784e-01 1.00000000e+01 2.43902445e-01\n 1.00000000e+01 4.97512445e-02 1.44717805e-02 6.09384477e-03]\nPosterior means: [56.25493        nan       nan 43.30076  72.451996 50.29118  47.363907\n 48.423653 48.33792  78.50884  49.056976 50.865433 85.567184 56.461384\n 45.77003  28.351908 46.35255        nan 51.526287 13.507514 50.92853\n 61.935177       nan 66.673035 55.344856 53.970085 51.72649  48.602386\n 21.416073 38.887344 34.017673       nan 46.315628 47.894787       nan\n 52.450897       nan 50.651237 51.713726 58.56828 ]\nResampled means: [55.69862        nan       nan 43.31078  72.45312  50.153187 47.30779\n 48.41313  48.348423 78.5107   49.073395 50.96239  85.56707  56.455288\n 45.808617 28.351074 46.34356        nan 51.513474 13.507926 50.92774\n 61.93856        nan 66.67193  55.451523 53.97859  51.9743   48.747993\n 21.416214 38.89711  34.01872        nan 46.3242   47.696896       nan\n 52.15747        nan 50.69268  51.74144  58.575657]\nChosen means: [55.69862  51.93298  50.256966 43.31078  72.45312  50.153187 47.30779\n 48.41313  48.348423 78.5107   49.073395 50.96239  85.56707  56.455288\n 45.808617 28.351074 46.34356  52.33351  51.513474 13.507926 50.92774\n 61.93856  48.401085 66.67193  55.451523 53.97859  51.9743   48.747993\n 21.416214 38.89711  34.01872  50.328022 46.3242   47.696896 53.050323\n 52.15747  50.84937  50.69268  51.74144  58.575657]\nCategory counts: [  3   0   0 230 376   4   4  36  41 387  65  24 486 134   5 386  80   0\n  37 500  26 257   0 341   6 149   2   4 483 310 313   0  51  13   0   3\n   0  26  66 152]\nNew alpha: [3.312500e+00 3.125000e-01 3.125000e-01 2.303125e+02 3.763125e+02\n 4.312500e+00 4.312500e+00 3.631250e+01 4.131250e+01 3.873125e+02\n 6.531250e+01 2.431250e+01 4.863125e+02 1.343125e+02 5.312500e+00\n 3.863125e+02 8.031250e+01 3.125000e-01 3.731250e+01 5.003125e+02\n 2.631250e+01 2.573125e+02 3.125000e-01 3.413125e+02 6.312500e+00\n 1.493125e+02 2.312500e+00 4.312500e+00 4.833125e+02 3.103125e+02\n 3.133125e+02 3.125000e-01 5.131250e+01 1.331250e+01 3.125000e-01\n 3.312500e+00 3.125000e-01 2.631250e+01 6.631250e+01 1.523125e+02]\nNew probs: [1.5712998e-04 5.9312595e-05 9.6077425e-04 4.5810871e-02 7.0267960e-02\n 1.3380859e-03 1.1873130e-03 8.0016395e-03 7.8140050e-03 7.9072870e-02\n 1.2927977e-02 4.3127136e-03 9.7659752e-02 2.5753029e-02 2.1056326e-03\n 7.7587143e-02 2.0465082e-02 6.0544262e-06 7.3906374e-03 9.9876143e-02\n 4.6110875e-03 5.7262525e-02 8.3207913e-11 6.7918070e-02 9.0016040e-04\n 2.4404518e-02 6.4713042e-04 6.8338146e-04 1.0264046e-01 5.5605642e-02\n 5.8058236e-02 2.7662711e-05 1.0486388e-02 2.0336760e-03 6.3426887e-05\n 4.4843627e-04 2.0375089e-06 5.1144604e-03 1.3618460e-02 3.2720357e-02]\nCategory counts: [  3   0   0 230 376   4   4  36  41 387  65  24 486 134   5 386  80   0\n  37 500  26 257   0 341   6 149   2   4 483 310 313   0  51  13   0   3\n   0  26  66 152]\nCurrent means: [55.699356       nan       nan 43.28614  72.52094  49.366817 46.583244\n 48.471878 48.344746 78.60422  49.118282 51.061115 85.60139  56.44649\n 46.077045 28.29696  46.212807       nan 51.432087 13.500216 50.843338\n 61.999496       nan 66.72551  55.7066   53.97209  50.869637 48.41204\n 21.381493 38.872723 33.972393       nan 46.027866 48.089928       nan\n 52.655346       nan 50.942593 51.79045  58.68547 ]\nPosterior variance: [3.2258067e-01 1.0000000e+01 1.0000000e+01 4.3459362e-03 2.6588673e-03\n 2.4390244e-01 2.4390244e-01 2.7700832e-02 2.4330901e-02 2.5833119e-03\n 1.5360983e-02 4.1493773e-02 2.0571898e-03 7.4571213e-03 1.9607843e-01\n 2.5900025e-03 1.2484395e-02 1.0000000e+01 2.6954180e-02 1.9996001e-03\n 3.8314175e-02 3.8895370e-03 1.0000000e+01 2.9316915e-03 1.6393442e-01\n 6.7069079e-03 4.7619051e-01 2.4390244e-01 2.0699648e-03 3.2247661e-03\n 3.1938676e-03 1.0000000e+01 1.9569471e-02 7.6335877e-02 1.0000000e+01\n 3.2258067e-01 1.0000000e+01 3.8314175e-02 1.5128593e-02 6.5746219e-03]\nPosterior means: [55.515507       nan       nan 43.28906  72.51496  49.382263 46.66658\n 48.476112 48.348774 78.59683  49.119637 51.05671  85.594055 56.441685\n 46.153965 28.302582 46.217537       nan 51.428223 13.507514 50.840103\n 61.99483        nan 66.720604 55.613045 53.96943  50.828224 48.450775\n 21.387415 38.87631  33.977512       nan 46.03564  48.104507       nan\n 52.56969        nan 50.93898  51.78774  58.679756]\nResampled means: [55.55832        nan       nan 43.29375  72.52015  49.080017 47.034515\n 48.436176 48.336613 78.599724 49.10584  51.134254 85.59443  56.437504\n 46.074757 28.301647 46.231533       nan 51.429863 13.509312 50.86279\n 61.99022        nan 66.71808  55.671417 53.963844 50.187183 48.187138\n 21.388847 38.870457 33.9773         nan 46.057983 48.108913       nan\n 52.4439         nan 50.943485 51.781773 58.676426]\nChosen means: [55.55832  51.93298  50.256966 43.29375  72.52015  49.080017 47.034515\n 48.436176 48.336613 78.599724 49.10584  51.134254 85.59443  56.437504\n 46.074757 28.301647 46.231533 52.33351  51.429863 13.509312 50.86279\n 61.99022  48.401085 66.71808  55.671417 53.963844 50.187183 48.187138\n 21.388847 38.870457 33.9773   50.328022 46.057983 48.108913 53.050323\n 52.4439   50.84937  50.943485 51.781773 58.676426]\nCategory counts: [  0   1   6 222 373  12   3  37  45 392  49  11 483 149  10 387  96   0\n  46 500  23 265   0 333   6 139   5   2 479 310 314   0  51   8   0   4\n   0  18  78 143]\nNew alpha: [3.125000e-01 1.312500e+00 6.312500e+00 2.223125e+02 3.733125e+02\n 1.231250e+01 3.312500e+00 3.731250e+01 4.531250e+01 3.923125e+02\n 4.931250e+01 1.131250e+01 4.833125e+02 1.493125e+02 1.031250e+01\n 3.873125e+02 9.631250e+01 3.125000e-01 4.631250e+01 5.003125e+02\n 2.331250e+01 2.653125e+02 3.125000e-01 3.333125e+02 6.312500e+00\n 1.393125e+02 5.312500e+00 2.312500e+00 4.793125e+02 3.103125e+02\n 3.143125e+02 3.125000e-01 5.131250e+01 8.312500e+00 3.125000e-01\n 4.312500e+00 3.125000e-01 1.831250e+01 7.831250e+01 1.433125e+02]\nNew probs: [3.2528493e-07 9.7315654e-04 1.1290865e-03 4.4298220e-02 7.0935808e-02\n 2.1667371e-03 2.0861684e-03 7.7117491e-03 8.4956754e-03 8.3362542e-02\n 9.8740179e-03 2.7674797e-03 9.6617669e-02 3.1818144e-02 2.1889750e-03\n 7.6117858e-02 2.1244030e-02 8.5643493e-05 8.4548257e-03 9.9203341e-02\n 6.0355612e-03 5.6050289e-02 4.2908840e-05 6.5489225e-02 1.0757667e-03\n 2.7090246e-02 1.3810359e-03 5.6426169e-04 8.9673921e-02 5.9869163e-02\n 6.5467805e-02 2.6206519e-05 9.9636735e-03 1.9069095e-03 8.2215356e-06\n 1.1203089e-03 7.8103127e-05 1.6792762e-03 1.5977500e-02 2.6967999e-02]\nCategory counts: [  0   1   6 222 373  12   3  37  45 392  49  11 483 149  10 387  96   0\n  46 500  23 265   0 333   6 139   5   2 479 310 314   0  51   8   0   4\n   0  18  78 143]\nCurrent means: [      nan 52.479576 50.20181  43.18207  72.506905 48.715897 47.03783\n 48.443245 48.407345 78.61407  49.269127 50.725163 85.62419  56.47334\n 45.87961  28.233189 46.342896       nan 51.72117  13.500216 50.974247\n 62.095158       nan 66.79538  55.7232   54.03293  50.496967 48.19935\n 21.354717 38.834824 33.938824       nan 46.053265 47.856354       nan\n 51.94744        nan 50.96286  51.82108  58.7547  ]\nPosterior variance: [1.00000000e+01 9.09090877e-01 1.63934425e-01 4.50247619e-03\n 2.68024649e-03 8.26446265e-02 3.22580665e-01 2.69541796e-02\n 2.21729502e-02 2.55036983e-03 2.03665998e-02 9.00900885e-02\n 2.06996477e-03 6.70690788e-03 9.90098938e-02 2.58331187e-03\n 1.04058273e-02 1.00000000e+01 2.16919743e-02 1.99960009e-03\n 4.32900414e-02 3.77216144e-03 1.00000000e+01 3.00210132e-03\n 1.63934425e-01 7.18907220e-03 1.96078435e-01 4.76190507e-01\n 2.08724686e-03 3.22476611e-03 3.18369945e-03 1.00000000e+01\n 1.95694715e-02 1.23456784e-01 1.00000000e+01 2.43902445e-01\n 1.00000000e+01 5.52486181e-02 1.28040975e-02 6.98811980e-03]\nPosterior means: [      nan 52.25416  50.198498 43.185143 72.50088  48.726513 47.133385\n 48.447437 48.410873 78.606766 49.270615 50.71863  85.61682  56.468998\n 45.9204   28.238813 46.3467         nan 51.717434 13.507514 50.97003\n 62.09059        nan 66.79033  55.629375 54.030025 50.487225 48.28509\n 21.360697 38.838425 33.94394        nan 46.06099  47.882816       nan\n 51.899944       nan 50.957542 51.818752 58.748577]\nResampled means: [      nan 50.96344  49.924606 43.18251  72.50734  48.856636 46.910984\n 48.50051  48.411167 78.60345  49.269382 50.683773 85.61384  56.465603\n 45.987354 28.237478 46.335476       nan 51.74293  13.505055 50.95849\n 62.09311        nan 66.78845  55.56518  54.036335 50.64767  48.359955\n 21.35861  38.83482  33.94237        nan 46.04602  47.848473       nan\n 51.892124       nan 50.928    51.831867 58.736877]\nChosen means: [55.55832  50.96344  49.924606 43.18251  72.50734  48.856636 46.910984\n 48.50051  48.411167 78.60345  49.269382 50.683773 85.61384  56.465603\n 45.987354 28.237478 46.335476 52.33351  51.74293  13.505055 50.95849\n 62.09311  48.401085 66.78845  55.56518  54.036335 50.64767  48.359955\n 21.35861  38.83482  33.94237  50.328022 46.04602  47.848473 53.050323\n 51.892124 50.84937  50.928    51.831867 58.736877]\nCategory counts: [  0   4   3 228 374   9  10  28  49 399  36  11 476 151  12 396 102   0\n  32 501  43 265   0 323   6 141   5   3 472 314 304   0  45   6   0   6\n   0   6  89 151]\nNew alpha: [3.125000e-01 4.312500e+00 3.312500e+00 2.283125e+02 3.743125e+02\n 9.312500e+00 1.031250e+01 2.831250e+01 4.931250e+01 3.993125e+02\n 3.631250e+01 1.131250e+01 4.763125e+02 1.513125e+02 1.231250e+01\n 3.963125e+02 1.023125e+02 3.125000e-01 3.231250e+01 5.013125e+02\n 4.331250e+01 2.653125e+02 3.125000e-01 3.233125e+02 6.312500e+00\n 1.413125e+02 5.312500e+00 3.312500e+00 4.723125e+02 3.143125e+02\n 3.043125e+02 3.125000e-01 4.531250e+01 6.312500e+00 3.125000e-01\n 6.312500e+00 3.125000e-01 6.312500e+00 8.931250e+01 1.513125e+02]\nNew probs: [2.57881300e-10 5.03240270e-04 3.31567950e-04 4.79556844e-02\n 7.11394921e-02 1.65699259e-03 2.45738286e-03 7.80669786e-03\n 1.02805523e-02 7.49592036e-02 7.58781005e-03 1.95630430e-03\n 8.94299224e-02 3.39032486e-02 3.19130626e-03 8.29987526e-02\n 2.06685662e-02 4.83961600e-08 7.93622900e-03 1.06288001e-01\n 9.49819572e-03 5.31287715e-02 1.02058828e-09 6.87851533e-02\n 1.43412524e-03 2.48288196e-02 7.27112056e-04 3.23668879e-04\n 9.46266428e-02 5.98968044e-02 6.07028008e-02 1.54615498e-09\n 9.25386045e-03 1.26554887e-03 1.44748365e-05 1.10961159e-03\n 5.02293233e-07 6.97192270e-04 1.55954864e-02 2.70604603e-02]\nCategory counts: [  0   4   3 228 374   9  10  28  49 399  36  11 476 151  12 396 102   0\n  32 501  43 265   0 323   6 141   5   3 472 314 304   0  45   6   0   6\n   0   6  89 151]\nCurrent means: [      nan 50.956047 49.70046  43.172142 72.501564 48.88909  46.924988\n 48.792908 48.37741  78.681404 49.56804  50.47124  85.6736   56.462135\n 45.612247 28.205046 46.32508        nan 51.70361  13.509375 50.49199\n 62.22778        nan 66.83861  56.089733 53.993717 50.631584 48.47998\n 21.31609  38.769188 33.901714       nan 46.27619  48.01192        nan\n 51.830814       nan 51.23631  51.79422  58.901142]\nPosterior variance: [1.0000000e+01 2.4390244e-01 3.2258067e-01 4.3840418e-03 2.6730821e-03\n 1.0989010e-01 9.9009894e-02 3.5587188e-02 2.0366600e-02 2.5056377e-03\n 2.7700832e-02 9.0090089e-02 2.1003990e-03 6.6181333e-03 8.2644626e-02\n 2.5246150e-03 9.7943190e-03 1.0000000e+01 3.1152649e-02 1.9956096e-03\n 2.3201857e-02 3.7721614e-03 1.0000000e+01 3.0950170e-03 1.6393442e-01\n 7.0871720e-03 1.9607843e-01 3.2258067e-01 2.1181952e-03 3.1836994e-03\n 3.2883920e-03 1.0000000e+01 2.2172950e-02 1.6393442e-01 1.0000000e+01\n 1.6393442e-01 1.0000000e+01 1.6393442e-01 1.1223345e-02 6.6181333e-03]\nPosterior means: [      nan 50.93273  49.71012  43.175137 72.49555  48.901302 46.95543\n 48.797207 48.380714 78.67422  49.569237 50.466995 85.66611  56.457863\n 45.64851  28.210546 46.328682       nan 51.698303 13.516657 50.490852\n 62.223164       nan 66.8334   55.9899   53.990883 50.6192   48.529015\n 21.322166 38.772766 33.90701        nan 46.284447 48.04451        nan\n 51.8008         nan 51.216038 51.792206 58.895256]\nResampled means: [      nan 50.874355 49.63321  43.170856 72.497116 48.859585 46.91243\n 48.8109   48.366768 78.673065 49.519245 50.548435 85.66423  56.448452\n 45.701645 28.212738 46.32994        nan 51.690792 13.516532 50.507423\n 62.221493       nan 66.8344   55.620174 53.987232 50.653397 48.583878\n 21.321556 38.773106 33.90619        nan 46.273636 47.960907       nan\n 51.72347        nan 51.13991  51.80767  58.892677]\nChosen means: [55.55832  50.874355 49.63321  43.170856 72.497116 48.859585 46.91243\n 48.8109   48.366768 78.673065 49.519245 50.548435 85.66423  56.448452\n 45.701645 28.212738 46.32994  52.33351  51.690792 13.516532 50.507423\n 62.221493 48.401085 66.8344   55.620174 53.987232 50.653397 48.583878\n 21.321556 38.773106 33.90619  50.328022 46.273636 47.960907 53.050323\n 51.72347  50.84937  51.13991  51.80767  58.892677]\nCategory counts: [  0   3   2 232 372  12  12  41  51 405  37   5 469 149  16 387 106   0\n  41 499  47 264   0 317   6 145   5   2 478 314 301   0  27   5   0   9\n   0   2  76 163]\nNew alpha: [3.125000e-01 3.312500e+00 2.312500e+00 2.323125e+02 3.723125e+02\n 1.231250e+01 1.231250e+01 4.131250e+01 5.131250e+01 4.053125e+02\n 3.731250e+01 5.312500e+00 4.693125e+02 1.493125e+02 1.631250e+01\n 3.873125e+02 1.063125e+02 3.125000e-01 4.131250e+01 4.993125e+02\n 4.731250e+01 2.643125e+02 3.125000e-01 3.173125e+02 6.312500e+00\n 1.453125e+02 5.312500e+00 2.312500e+00 4.783125e+02 3.143125e+02\n 3.013125e+02 3.125000e-01 2.731250e+01 5.312500e+00 3.125000e-01\n 9.312500e+00 3.125000e-01 2.312500e+00 7.631250e+01 1.633125e+02]\nNew probs: [8.1305305e-05 4.0257012e-04 5.0905894e-04 4.7141381e-02 7.2554350e-02\n 2.4364113e-03 2.5781791e-03 8.3703976e-03 1.1224793e-02 7.7730633e-02\n 6.5033520e-03 1.3970666e-03 8.5700773e-02 3.0159622e-02 3.4838710e-03\n 8.0795169e-02 2.2048309e-02 1.6325093e-05 1.1585725e-02 9.9806443e-02\n 8.8787004e-03 5.1681787e-02 3.3559558e-05 6.4786375e-02 1.3162036e-03\n 3.1689201e-02 1.1220352e-03 2.2890967e-04 9.4658867e-02 5.9853178e-02\n 6.2190644e-02 5.2058695e-06 7.9280436e-03 1.6935377e-03 4.0660449e-04\n 2.6941481e-03 1.3713109e-04 7.9899881e-04 1.2348598e-02 3.3023015e-02]\nCategory counts: [  0   3   2 232 372  12  12  41  51 405  37   5 469 149  16 387 106   0\n  41 499  47 264   0 317   6 145   5   2 478 314 301   0  27   5   0   9\n   0   2  76 163]\nCurrent means: [      nan 51.911312 50.41223  43.05192  72.52873  48.137142 47.229504\n 48.536236 48.356457 78.752464 49.551575 49.539135 85.722    56.44187\n 45.265587 28.204626 46.276745       nan 51.797386 13.493208 50.562855\n 62.36721        nan 66.952324 55.918896 53.894665 50.594368 49.080856\n 21.330736 38.6635   33.78072        nan 46.168358 47.666508       nan\n 51.540085       nan 50.472237 51.8423   58.997887]\nPosterior variance: [1.0000000e+01 3.2258067e-01 4.7619051e-01 4.3084878e-03 2.6874496e-03\n 8.2644626e-02 8.2644626e-02 2.4330901e-02 1.9569471e-02 2.4685264e-03\n 2.6954180e-02 1.9607843e-01 2.1317415e-03 6.7069079e-03 6.2111799e-02\n 2.5833119e-03 9.4250711e-03 1.0000000e+01 2.4330901e-02 2.0036064e-03\n 2.1231424e-02 3.7864444e-03 1.0000000e+01 3.1535793e-03 1.6393442e-01\n 6.8917987e-03 1.9607843e-01 4.7619051e-01 2.0916127e-03 3.1836994e-03\n 3.3211557e-03 1.0000000e+01 3.6900368e-02 1.9607843e-01 1.0000000e+01\n 1.0989010e-01 1.0000000e+01 4.7619051e-01 1.3140605e-02 6.1312076e-03]\nPosterior means: [      nan 51.84966  50.3926   43.054913 72.522675 48.15254  47.252403\n 48.5398   48.359673 78.74537  49.55278  49.548172 85.71438  56.437553\n 45.294994 28.210258 46.28026        nan 51.793015 13.500524 50.56166\n 62.362526       nan 66.946976 55.82186  53.89198  50.582714 49.124622\n 21.336731 38.66711  33.786102       nan 46.1825   47.71226        nan\n 51.523163       nan 50.44975  51.83988  58.99237 ]\nResampled means: [      nan 51.534744 50.367676 43.063503 72.5228   48.17123  47.096645\n 48.48462  48.3405   78.745384 49.56497  49.211876 85.71338  56.42125\n 45.357674 28.214544 46.270405       nan 51.805775 13.501053 50.548805\n 62.356987       nan 66.948044 55.68625  53.89256  50.579517 48.546745\n 21.338078 38.665047 33.78727        nan 46.14147  47.862362       nan\n 51.391838       nan 50.849964 51.837383 58.992363]\nChosen means: [55.55832  51.534744 50.367676 43.063503 72.5228   48.17123  47.096645\n 48.48462  48.3405   78.745384 49.56497  49.211876 85.71338  56.42125\n 45.357674 28.214544 46.270405 52.33351  51.805775 13.501053 50.548805\n 62.356987 48.401085 66.948044 55.68625  53.89256  50.579517 48.546745\n 21.338078 38.665047 33.78727  50.328022 46.14147  47.862362 53.050323\n 51.391838 50.84937  50.849964 51.837383 58.992363]\n</pre> <pre>Category counts: [  1   4   1 221 371  13  12  46  43 420  28  11 454 138  22 391  98   0\n  51 500  49 266   0 304   8 138   6   3 476 327 286   0  37   8   4   8\n   1   4  64 186]\nNew alpha: [1.312500e+00 4.312500e+00 1.312500e+00 2.213125e+02 3.713125e+02\n 1.331250e+01 1.231250e+01 4.631250e+01 4.331250e+01 4.203125e+02\n 2.831250e+01 1.131250e+01 4.543125e+02 1.383125e+02 2.231250e+01\n 3.913125e+02 9.831250e+01 3.125000e-01 5.131250e+01 5.003125e+02\n 4.931250e+01 2.663125e+02 3.125000e-01 3.043125e+02 8.312500e+00\n 1.383125e+02 6.312500e+00 3.312500e+00 4.763125e+02 3.273125e+02\n 2.863125e+02 3.125000e-01 3.731250e+01 8.312500e+00 4.312500e+00\n 8.312500e+00 1.312500e+00 4.312500e+00 6.431250e+01 1.863125e+02]\nNew probs: [3.1579746e-04 1.2034466e-03 4.1637267e-04 4.5945607e-02 7.4523404e-02\n 2.8895517e-03 2.2052177e-03 9.3941744e-03 1.0714262e-02 8.2592189e-02\n 7.8994324e-03 1.5829586e-03 8.4145099e-02 2.9569726e-02 3.4492167e-03\n 7.5718820e-02 1.9744534e-02 2.7605086e-08 9.7978413e-03 9.6142828e-02\n 1.2056282e-02 5.2694034e-02 1.9718686e-04 5.7571933e-02 2.1654437e-03\n 3.0535659e-02 7.9975463e-04 7.8606291e-04 9.9203959e-02 6.2102456e-02\n 6.0355555e-02 4.7684560e-05 9.5267855e-03 1.7416665e-03 5.7588954e-04\n 1.1824957e-03 2.2736624e-04 2.4070423e-04 1.4205249e-02 3.5533451e-02]\nCategory counts: [  1   4   1 221 371  13  12  46  43 420  28  11 454 138  22 391  98   0\n  51 500  49 266   0 304   8 138   6   3 476 327 286   0  37   8   4   8\n   1   4  64 186]\nCurrent means: [54.09279  51.61155  48.970226 43.005524 72.53725  47.893913 46.940582\n 48.537926 48.31994  78.87615  49.646786 49.617313 85.83809  56.270332\n 45.264362 28.214853 46.154694       nan 51.607475 13.502434 50.60423\n 62.569286       nan 67.071045 55.979755 53.99313  50.358555 49.330902\n 21.332085 38.587025 33.70303        nan 46.171238 47.64765  53.32832\n 51.371994 52.137756 50.968536 51.909203 59.000492]\nPosterior variance: [9.09090877e-01 2.43902445e-01 9.09090877e-01 4.52284003e-03\n 2.69469130e-03 7.63358772e-02 8.26446265e-02 2.16919743e-02\n 2.32018568e-02 2.38038553e-03 3.55871879e-02 9.00900885e-02\n 2.20215810e-03 7.24112941e-03 4.52488698e-02 2.55689071e-03\n 1.01936804e-02 1.00000000e+01 1.95694715e-02 1.99960009e-03\n 2.03665998e-02 3.75798554e-03 1.00000000e+01 3.28839198e-03\n 1.23456784e-01 7.24112941e-03 1.63934425e-01 3.22580665e-01\n 2.10039900e-03 3.05716903e-03 3.49528133e-03 1.00000000e+01\n 2.69541796e-02 1.23456784e-01 2.43902445e-01 1.23456784e-01\n 9.09090877e-01 2.43902445e-01 1.56006245e-02 5.37345512e-03]\nPosterior means: [53.72072  51.572247 49.063847 43.008682 72.53118  47.90999  46.96587\n 48.541096 48.32384  78.86928  49.648045 49.62076  85.83019  56.265793\n 45.28579  28.220423 46.15861        nan 51.60433  13.509732 50.602997\n 62.564564       nan 67.06543  55.90593  53.99024  50.352673 49.35249\n 21.338106 38.59051  33.708725       nan 46.181557 47.676693 53.247143\n 51.355057 51.943417 50.944916 51.906223 58.99565 ]\nResampled means: [54.14435  51.51574  50.225204 43.007874 72.53262  47.823124 46.886475\n 48.541153 48.330833 78.868614 49.63132  49.610664 85.82778  56.26926\n 45.290638 28.221455 46.163025       nan 51.597992 13.510991 50.632824\n 62.560715       nan 67.06664  55.741848 53.995968 50.07911  49.990967\n 21.337154 38.590508 33.710644       nan 46.171986 47.92779  53.10456\n 51.400986 51.01554  51.334644 51.925224 58.998425]\nChosen means: [54.14435  51.51574  50.225204 43.007874 72.53262  47.823124 46.886475\n 48.541153 48.330833 78.868614 49.63132  49.610664 85.82778  56.26926\n 45.290638 28.221455 46.163025 52.33351  51.597992 13.510991 50.632824\n 62.560715 48.401085 67.06664  55.741848 53.995968 50.07911  49.990967\n 21.337154 38.590508 33.710644 50.328022 46.171986 47.92779  53.10456\n 51.400986 51.01554  51.334644 51.925224 58.998425]\nCategory counts: [  1   4   0 231 372  11   6  46  58 422  35   9 450 145  13 389  97   0\n  52 500  50 271   0 285   9 138   2   4 474 322 291   0  40   8   2   3\n   2   2  64 192]\nNew alpha: [1.312500e+00 4.312500e+00 3.125000e-01 2.313125e+02 3.723125e+02\n 1.131250e+01 6.312500e+00 4.631250e+01 5.831250e+01 4.223125e+02\n 3.531250e+01 9.312500e+00 4.503125e+02 1.453125e+02 1.331250e+01\n 3.893125e+02 9.731250e+01 3.125000e-01 5.231250e+01 5.003125e+02\n 5.031250e+01 2.713125e+02 3.125000e-01 2.853125e+02 9.312500e+00\n 1.383125e+02 2.312500e+00 4.312500e+00 4.743125e+02 3.223125e+02\n 2.913125e+02 3.125000e-01 4.031250e+01 8.312500e+00 2.312500e+00\n 3.312500e+00 2.312500e+00 2.312500e+00 6.431250e+01 1.923125e+02]\nNew probs: [4.2126368e-05 9.8625361e-04 1.2499719e-05 4.1902453e-02 8.0743097e-02\n 1.8524257e-03 5.6517887e-04 1.0637982e-02 1.1753799e-02 8.0684826e-02\n 5.9381477e-03 2.0840615e-03 9.5920287e-02 3.1153545e-02 2.5282053e-03\n 7.2370842e-02 2.0604830e-02 5.5813422e-07 9.7625107e-03 9.6488796e-02\n 1.0915829e-02 5.4293714e-02 2.1782221e-06 5.9997138e-02 1.5722847e-03\n 2.7767288e-02 1.6390810e-04 6.8291789e-04 9.1297016e-02 6.2877029e-02\n 5.7574734e-02 4.6575416e-05 8.9534540e-03 1.5097510e-03 7.5386581e-04\n 1.6484876e-03 2.8188503e-04 6.3175359e-04 1.3671605e-02 3.9325885e-02]\nCategory counts: [  1   4   0 231 372  11   6  46  58 422  35   9 450 145  13 389  97   0\n  52 500  50 271   0 285   9 138   2   4 474 322 291   0  40   8   2   3\n   2   2  64 192]\nCurrent means: [54.02542  51.64749        nan 42.99082  72.563896 47.444324 46.767925\n 48.67489  48.369507 78.9213   49.408688 49.616497 85.86962  56.292336\n 45.3683   28.168617 46.284916       nan 51.509068 13.500216 50.734047\n 62.833134       nan 67.251465 55.32457  54.090794 50.638016 50.518047\n 21.31873  38.559723 33.683533       nan 45.860584 48.328003 53.633514\n 51.70387  50.363575 50.463818 52.08528  59.235855]\nPosterior variance: [9.09090877e-01 2.43902445e-01 1.00000000e+01 4.32713097e-03\n 2.68744957e-03 9.00900885e-02 1.63934425e-01 2.16919743e-02\n 1.72117036e-02 2.36910675e-03 2.84900293e-02 1.09890103e-01\n 2.22172844e-03 6.89179869e-03 7.63358772e-02 2.57003331e-03\n 1.02986610e-02 1.00000000e+01 1.91938579e-02 1.99960009e-03\n 1.99600812e-02 3.68867558e-03 1.00000000e+01 3.50754103e-03\n 1.09890103e-01 7.24112941e-03 4.76190507e-01 2.43902445e-01\n 2.10925960e-03 3.10462574e-03 3.43524548e-03 1.00000000e+01\n 2.49376576e-02 1.23456784e-01 4.76190507e-01 3.22580665e-01\n 4.76190507e-01 4.76190507e-01 1.56006245e-02 5.20562194e-03]\nPosterior means: [53.659477 51.60731        nan 42.99386  72.55783  47.46735  46.820908\n 48.67776  48.372314 78.91445  49.41037  49.620712 85.86166  56.288\n 45.403656 28.174227 46.288746       nan 51.506172 13.507514 50.73258\n 62.828403       nan 67.24542  55.26606  54.087833 50.60763  50.505413\n 21.32478  38.563274 33.68914        nan 45.870907 48.348644 53.460487\n 51.648907 50.34626  50.44173  52.082027 59.23105 ]\nResampled means: [54.406803 51.028355       nan 42.99867  72.55519  47.427605 46.910137\n 48.67518  48.363815 78.91607  49.427994 49.508144 85.85906  56.28747\n 45.40935  28.169222 46.282005       nan 51.509872 13.506594 50.743168\n 62.833218       nan 67.24621  55.364838 54.091442 50.6718   50.62326\n 21.327293 38.56305  33.687893       nan 45.873043 48.326405 53.314575\n 52.164066 50.931313 50.03181  52.074623 59.226696]\nChosen means: [54.406803 51.028355 50.225204 42.99867  72.55519  47.427605 46.910137\n 48.67518  48.363815 78.91607  49.427994 49.508144 85.85906  56.28747\n 45.40935  28.169222 46.282005 52.33351  51.509872 13.506594 50.743168\n 62.833218 48.401085 67.24621  55.364838 54.091442 50.6718   50.62326\n 21.327293 38.56305  33.687893 50.328022 45.873043 48.326405 53.314575\n 52.164066 50.931313 50.03181  52.074623 59.226696]\nCategory counts: [  0   3   0 216 373   6   6  50  55 427  29   9 447 150   8 381 117   0\n  54 498  50 243   0 280  10 119   2   4 476 316 299   0  48   6   4  11\n   0   1  81 221]\nNew alpha: [3.125000e-01 3.312500e+00 3.125000e-01 2.163125e+02 3.733125e+02\n 6.312500e+00 6.312500e+00 5.031250e+01 5.531250e+01 4.273125e+02\n 2.931250e+01 9.312500e+00 4.473125e+02 1.503125e+02 8.312500e+00\n 3.813125e+02 1.173125e+02 3.125000e-01 5.431250e+01 4.983125e+02\n 5.031250e+01 2.433125e+02 3.125000e-01 2.803125e+02 1.031250e+01\n 1.193125e+02 2.312500e+00 4.312500e+00 4.763125e+02 3.163125e+02\n 2.993125e+02 3.125000e-01 4.831250e+01 6.312500e+00 4.312500e+00\n 1.131250e+01 3.125000e-01 1.312500e+00 8.131250e+01 2.213125e+02]\nNew probs: [1.0027899e-05 9.6206251e-04 3.8609134e-05 3.9303672e-02 7.3366627e-02\n 9.9128485e-04 1.9634638e-03 7.7797603e-03 1.1180174e-02 8.9896031e-02\n 6.8137515e-03 1.4427023e-03 8.6484656e-02 3.1512912e-02 1.2425840e-03\n 7.6020107e-02 2.2720758e-02 2.1456301e-06 1.1375427e-02 9.8592363e-02\n 1.2327971e-02 5.0267212e-02 1.3819245e-05 5.7189777e-02 1.5199475e-03\n 2.2993298e-02 3.4571928e-04 3.5937462e-04 9.2499122e-02 6.0978580e-02\n 6.6228621e-02 1.6300258e-04 8.7771183e-03 1.2976262e-03 9.9095120e-04\n 2.9385914e-03 4.6898512e-07 1.0067152e-04 1.5397001e-02 4.3912262e-02]\n</pre> Out[5]: <p>Plotting results</p> In\u00a0[6]: Copied! <pre># Prepare data for the animation\ndata_points = datapoints[\"datapoints\", \"obs\"].tolist()\nnp.random.seed(42)\njitter = np.random.uniform(-0.05, 0.05, size=len(data_points)).tolist()\nstd_dev = np.sqrt(OBS_VARIANCE) * 1.5\nall_cluster_assignments_list = [a.tolist() for a in all_cluster_assignment]\nall_posterior_means_list = [m.tolist() for m in all_posterior_means]\nall_posterior_weights_list = [w.tolist() for w in all_posterior_weights]\n\n# Define a consistent color palette to use throughout the visualization\ncolor_palette = \"\"\"\nconst plotColors = [\n    \"#4c78a8\", \"#f58518\", \"#e45756\", \"#72b7b2\", \"#54a24b\", \n    \"#eeca3b\", \"#b279a2\", \"#ff9da6\", \"#9d755d\", \"#bab0ac\",\n    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n    \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"\n];\n\"\"\"\n\n# Shared data initialization for all plot components\nframe_data_js = (\n    \"\"\"\n// Get current frame data\nconst frame = $state.frame;\nconst hoveredCluster = $state.hoveredCluster;\nconst means = \"\"\"\n    + str(all_posterior_means_list)\n    + \"\"\"[frame];\nconst weights = \"\"\"\n    + str(all_posterior_weights_list)\n    + \"\"\"[frame];\nconst assignments = \"\"\"\n    + str(all_cluster_assignments_list)\n    + \"\"\"[frame];\nconst stdDev = \"\"\"\n    + str(std_dev)\n    + \"\"\";\n\"\"\"\n)\n\n# Create a visualizer with animation\n(\n    Plot.initialState({\"frame\": 0, \"hoveredCluster\": None})\n    |\n    # Main visualization that updates based on the current frame\n    Plot.plot({\n        \"marks\": [\n            # 1. Data points with jitter - show all data points with optional highlighting\n            Plot.dot(\n                Plot.js(\n                    \"\"\"function() {\n                    \"\"\"\n                    + frame_data_js\n                    + \"\"\"\n                    const dataPoints = \"\"\"\n                    + str(data_points)\n                    + \"\"\";\n                    const jitter = \"\"\"\n                    + str(jitter)\n                    + \"\"\";\n                    \n                    \"\"\"\n                    + color_palette\n                    + \"\"\"\n                    \n                    // Return all points with hover-aware opacity\n                    return dataPoints.map((x, i) =&gt; {\n                        const clusterIdx = assignments[i];\n                        // If a cluster is hovered, reduce opacity of other clusters' points\n                        const isHovered = hoveredCluster !== null &amp;&amp; clusterIdx === hoveredCluster;\n                        const opacity = hoveredCluster === null ? 0.5 : (isHovered ? 0.7 : 0.15);\n                        return {\n                            x: x,\n                            y: jitter[i],\n                            color: plotColors[clusterIdx % 20],\n                            opacity: opacity\n                        };\n                    });\n                }()\"\"\"\n                ),\n                {\"x\": \"x\", \"y\": \"y\", \"fill\": \"color\", \"r\": 3, \"opacity\": \"opacity\"},\n            ),\n            # 2. Combined error bars (both horizontal lines and vertical caps)\n            Plot.line(\n                Plot.js(\n                    \"\"\"function() {\n                    \"\"\"\n                    + frame_data_js\n                    + \"\"\"\n                    const capSize = 0.04;  // Size of the vertical cap lines\n                    \n                    \"\"\"\n                    + color_palette\n                    + \"\"\"\n                    \n                    // We'll collect all line segments in a flat array\n                    const result = [];\n                    \n                    for (let i = 0; i &lt; means.length; i++) {\n                        // Only include error bars for clusters with weight &gt;= 0.01\n                        if (weights[i] &gt;= 0.01) {\n                            // Determine if this cluster is being hovered\n                            const isHovered = hoveredCluster === i;\n                            const opacity = hoveredCluster === null ? 0.7 : (isHovered ? 1.0 : 0.3);\n                            const strokeWidth = isHovered ? 4 : 3;\n                            const color = plotColors[i % 20];\n                            \n                            // Add horizontal line (error bar itself)\n                            result.push({x: means[i] - stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});\n                            result.push({x: means[i] + stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});\n                            \n                            // Add left cap (vertical line)\n                            result.push({x: means[i] - stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});\n                            result.push({x: means[i] - stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});\n                            \n                            // Add right cap (vertical line)\n                            result.push({x: means[i] + stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});\n                            result.push({x: means[i] + stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});\n                        }\n                    }\n                    return result;\n                }()\"\"\"\n                ),\n                {\n                    \"x\": \"x\",\n                    \"y\": \"y\",\n                    \"stroke\": \"color\",\n                    \"strokeWidth\": \"width\",\n                    \"opacity\": \"opacity\",\n                    \"z\": \"cluster\",\n                },\n            ),\n            # 3. Cluster means as stars\n            Plot.dot(\n                Plot.js(\n                    \"\"\"function() {\n                    \"\"\"\n                    + frame_data_js\n                    + \"\"\"\n                    \"\"\"\n                    + color_palette\n                    + \"\"\"\n                        \n                    // Create a simple array for each cluster mean\n                    return means.map((mean, i) =&gt; {\n                        // Only include means for clusters with sufficient weight\n                        if (weights[i] &gt;= 0.01) {\n                            const isHovered = hoveredCluster === i;\n                            return {\n                                x: mean,\n                                y: 0,\n                                cluster: i,\n                                color: plotColors[i % 20],\n                                opacity: isHovered ? 1.0 : 0.8\n                            };\n                        }\n                        return null;  // Skip low-weight clusters\n                    }).filter(d =&gt; d !== null);  // Remove null values\n                }()\"\"\"\n                ),\n                {\n                    \"x\": \"x\",\n                    \"y\": \"y\",\n                    \"fill\": \"color\",\n                    \"r\": 10,\n                    \"symbol\": \"star\",\n                    \"stroke\": \"black\",\n                    \"strokeWidth\": 2,\n                    \"opacity\": \"opacity\",\n                },\n            ),\n        ],\n        \"grid\": True,\n        \"marginTop\": 40,\n        \"marginRight\": 40,\n        \"marginBottom\": 40,\n        \"marginLeft\": 40,\n        \"style\": {\"height\": \"400px\"},\n        \"title\": Plot.js(\n            \"`Dirichlet Mixture Model - Iteration ${$state.frame} of \"\n            + str(len(all_posterior_means) - 1)\n            + \"`\"\n        ),\n        \"subtitle\": \"Cluster centers (\u2605) with standard deviation (\u2014) and data points (\u2022)\",\n    })\n    |\n    # Animation controls and legend with hover effects\n    Plot.html([\n        \"div\",\n        {\"className\": \"p-4\"},\n        [\n            \"div\",\n            {\"className\": \"mb-4\"},\n            Plot.Slider(\n                \"frame\",\n                init=0,\n                range=[0, len(all_posterior_means) - 1],\n                step=1,\n                label=\"Iteration\",\n                width=\"100%\",\n                fps=8,\n            ),\n        ],\n        [\n            \"div\",\n            {\"className\": \"mt-4\"},\n            Plot.js(\n                \"\"\"function() {\n                \"\"\"\n                + frame_data_js\n                + \"\"\"\n                // Count assignments in current frame\n                const counts = {};\n                assignments.forEach(a =&gt; { counts[a] = (counts[a] || 0) + 1; });\n                \n                \"\"\"\n                + color_palette\n                + \"\"\"\n                \n                // Sort clusters by weight, filter by minimum weight, and limit to top 10\n                const topClusters = Object.keys(weights)\n                    .map(i =&gt; ({ \n                        id: parseInt(i), \n                        weight: weights[i], \n                        count: counts[parseInt(i)] || 0 \n                    }))\n                    .filter(c =&gt; c.weight &gt;= 0.01)\n                    .sort((a, b) =&gt; b.weight - a.weight)\n                    .slice(0, 10);\n                \n                // Create placeholder rows for consistent height\n                const placeholders = Array(Math.max(0, 10 - topClusters.length))\n                    .fill(0)\n                    .map(() =&gt; [\"tr\", {\"className\": \"h-8\"}, [\"td\", {\"colSpan\": 3}, \"\"]]);\n                    \n                return [\n                    \"div\", {},\n                    [\"h3\", {}, `Top Clusters by Weight (Iteration ${frame})`],\n                    [\"div\", {\"style\": {\"height\": \"280px\", \"overflow\": \"auto\"}},\n                        [\"table\", {\"className\": \"w-full mt-2\"},\n                            [\"thead\", [\"tr\",\n                                [\"th\", {\"className\": \"text-left\"}, \"Cluster\"],\n                                [\"th\", {\"className\": \"text-left\"}, \"Weight\"],\n                                [\"th\", {\"className\": \"text-left\"}, \"Points\"]\n                            ]],\n                            [\"tbody\",\n                                ...topClusters.map(cluster =&gt; \n                                    [\"tr\", {\n                                        \"className\": \"h-8\",\n                                        \"style\": {\n                                            \"cursor\": \"pointer\",\n                                            \"backgroundColor\": $state.hoveredCluster === cluster.id ? \"#f0f0f0\" : \"transparent\"\n                                        },\n                                        \"onMouseEnter\": () =&gt; { $state.hoveredCluster = cluster.id; },\n                                        \"onMouseLeave\": () =&gt; { $state.hoveredCluster = null; }\n                                    },\n                                    [\"td\", {\"className\": \"py-1\"}, \n                                        [\"div\", {\"className\": \"flex items-center\"},\n                                            [\"div\", {\n                                                \"style\": {\n                                                    \"backgroundColor\": plotColors[cluster.id % 20],\n                                                    \"width\": \"24px\",\n                                                    \"height\": \"24px\",\n                                                    \"borderRadius\": \"4px\",\n                                                    \"border\": \"1px solid rgba(0,0,0,0.2)\",\n                                                    \"display\": \"inline-block\",\n                                                    \"marginRight\": \"8px\"\n                                                }\n                                            }],\n                                            `Cluster ${cluster.id}`\n                                        ]\n                                    ],\n                                    [\"td\", {\"className\": \"py-1\"}, cluster.weight.toFixed(4)],\n                                    [\"td\", {\"className\": \"py-1\"}, cluster.count]\n                                    ]\n                                ),\n                                ...placeholders\n                            ]\n                        ]\n                    ]\n                ];\n            }()\"\"\"\n            ),\n        ],\n    ])\n)\n</pre> # Prepare data for the animation data_points = datapoints[\"datapoints\", \"obs\"].tolist() np.random.seed(42) jitter = np.random.uniform(-0.05, 0.05, size=len(data_points)).tolist() std_dev = np.sqrt(OBS_VARIANCE) * 1.5 all_cluster_assignments_list = [a.tolist() for a in all_cluster_assignment] all_posterior_means_list = [m.tolist() for m in all_posterior_means] all_posterior_weights_list = [w.tolist() for w in all_posterior_weights]  # Define a consistent color palette to use throughout the visualization color_palette = \"\"\" const plotColors = [     \"#4c78a8\", \"#f58518\", \"#e45756\", \"#72b7b2\", \"#54a24b\",      \"#eeca3b\", \"#b279a2\", \"#ff9da6\", \"#9d755d\", \"#bab0ac\",     \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",     \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\" ]; \"\"\"  # Shared data initialization for all plot components frame_data_js = (     \"\"\" // Get current frame data const frame = $state.frame; const hoveredCluster = $state.hoveredCluster; const means = \"\"\"     + str(all_posterior_means_list)     + \"\"\"[frame]; const weights = \"\"\"     + str(all_posterior_weights_list)     + \"\"\"[frame]; const assignments = \"\"\"     + str(all_cluster_assignments_list)     + \"\"\"[frame]; const stdDev = \"\"\"     + str(std_dev)     + \"\"\"; \"\"\" )  # Create a visualizer with animation (     Plot.initialState({\"frame\": 0, \"hoveredCluster\": None})     |     # Main visualization that updates based on the current frame     Plot.plot({         \"marks\": [             # 1. Data points with jitter - show all data points with optional highlighting             Plot.dot(                 Plot.js(                     \"\"\"function() {                     \"\"\"                     + frame_data_js                     + \"\"\"                     const dataPoints = \"\"\"                     + str(data_points)                     + \"\"\";                     const jitter = \"\"\"                     + str(jitter)                     + \"\"\";                                          \"\"\"                     + color_palette                     + \"\"\"                                          // Return all points with hover-aware opacity                     return dataPoints.map((x, i) =&gt; {                         const clusterIdx = assignments[i];                         // If a cluster is hovered, reduce opacity of other clusters' points                         const isHovered = hoveredCluster !== null &amp;&amp; clusterIdx === hoveredCluster;                         const opacity = hoveredCluster === null ? 0.5 : (isHovered ? 0.7 : 0.15);                         return {                             x: x,                             y: jitter[i],                             color: plotColors[clusterIdx % 20],                             opacity: opacity                         };                     });                 }()\"\"\"                 ),                 {\"x\": \"x\", \"y\": \"y\", \"fill\": \"color\", \"r\": 3, \"opacity\": \"opacity\"},             ),             # 2. Combined error bars (both horizontal lines and vertical caps)             Plot.line(                 Plot.js(                     \"\"\"function() {                     \"\"\"                     + frame_data_js                     + \"\"\"                     const capSize = 0.04;  // Size of the vertical cap lines                                          \"\"\"                     + color_palette                     + \"\"\"                                          // We'll collect all line segments in a flat array                     const result = [];                                          for (let i = 0; i &lt; means.length; i++) {                         // Only include error bars for clusters with weight &gt;= 0.01                         if (weights[i] &gt;= 0.01) {                             // Determine if this cluster is being hovered                             const isHovered = hoveredCluster === i;                             const opacity = hoveredCluster === null ? 0.7 : (isHovered ? 1.0 : 0.3);                             const strokeWidth = isHovered ? 4 : 3;                             const color = plotColors[i % 20];                                                          // Add horizontal line (error bar itself)                             result.push({x: means[i] - stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});                             result.push({x: means[i] + stdDev, y: 0, cluster: i, color, opacity, width: strokeWidth});                                                          // Add left cap (vertical line)                             result.push({x: means[i] - stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});                             result.push({x: means[i] - stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});                                                          // Add right cap (vertical line)                             result.push({x: means[i] + stdDev, y: -capSize, cluster: i, color, opacity, width: strokeWidth});                             result.push({x: means[i] + stdDev, y: capSize, cluster: i, color, opacity, width: strokeWidth});                         }                     }                     return result;                 }()\"\"\"                 ),                 {                     \"x\": \"x\",                     \"y\": \"y\",                     \"stroke\": \"color\",                     \"strokeWidth\": \"width\",                     \"opacity\": \"opacity\",                     \"z\": \"cluster\",                 },             ),             # 3. Cluster means as stars             Plot.dot(                 Plot.js(                     \"\"\"function() {                     \"\"\"                     + frame_data_js                     + \"\"\"                     \"\"\"                     + color_palette                     + \"\"\"                                              // Create a simple array for each cluster mean                     return means.map((mean, i) =&gt; {                         // Only include means for clusters with sufficient weight                         if (weights[i] &gt;= 0.01) {                             const isHovered = hoveredCluster === i;                             return {                                 x: mean,                                 y: 0,                                 cluster: i,                                 color: plotColors[i % 20],                                 opacity: isHovered ? 1.0 : 0.8                             };                         }                         return null;  // Skip low-weight clusters                     }).filter(d =&gt; d !== null);  // Remove null values                 }()\"\"\"                 ),                 {                     \"x\": \"x\",                     \"y\": \"y\",                     \"fill\": \"color\",                     \"r\": 10,                     \"symbol\": \"star\",                     \"stroke\": \"black\",                     \"strokeWidth\": 2,                     \"opacity\": \"opacity\",                 },             ),         ],         \"grid\": True,         \"marginTop\": 40,         \"marginRight\": 40,         \"marginBottom\": 40,         \"marginLeft\": 40,         \"style\": {\"height\": \"400px\"},         \"title\": Plot.js(             \"`Dirichlet Mixture Model - Iteration ${$state.frame} of \"             + str(len(all_posterior_means) - 1)             + \"`\"         ),         \"subtitle\": \"Cluster centers (\u2605) with standard deviation (\u2014) and data points (\u2022)\",     })     |     # Animation controls and legend with hover effects     Plot.html([         \"div\",         {\"className\": \"p-4\"},         [             \"div\",             {\"className\": \"mb-4\"},             Plot.Slider(                 \"frame\",                 init=0,                 range=[0, len(all_posterior_means) - 1],                 step=1,                 label=\"Iteration\",                 width=\"100%\",                 fps=8,             ),         ],         [             \"div\",             {\"className\": \"mt-4\"},             Plot.js(                 \"\"\"function() {                 \"\"\"                 + frame_data_js                 + \"\"\"                 // Count assignments in current frame                 const counts = {};                 assignments.forEach(a =&gt; { counts[a] = (counts[a] || 0) + 1; });                                  \"\"\"                 + color_palette                 + \"\"\"                                  // Sort clusters by weight, filter by minimum weight, and limit to top 10                 const topClusters = Object.keys(weights)                     .map(i =&gt; ({                          id: parseInt(i),                          weight: weights[i],                          count: counts[parseInt(i)] || 0                      }))                     .filter(c =&gt; c.weight &gt;= 0.01)                     .sort((a, b) =&gt; b.weight - a.weight)                     .slice(0, 10);                                  // Create placeholder rows for consistent height                 const placeholders = Array(Math.max(0, 10 - topClusters.length))                     .fill(0)                     .map(() =&gt; [\"tr\", {\"className\": \"h-8\"}, [\"td\", {\"colSpan\": 3}, \"\"]]);                                      return [                     \"div\", {},                     [\"h3\", {}, `Top Clusters by Weight (Iteration ${frame})`],                     [\"div\", {\"style\": {\"height\": \"280px\", \"overflow\": \"auto\"}},                         [\"table\", {\"className\": \"w-full mt-2\"},                             [\"thead\", [\"tr\",                                 [\"th\", {\"className\": \"text-left\"}, \"Cluster\"],                                 [\"th\", {\"className\": \"text-left\"}, \"Weight\"],                                 [\"th\", {\"className\": \"text-left\"}, \"Points\"]                             ]],                             [\"tbody\",                                 ...topClusters.map(cluster =&gt;                                      [\"tr\", {                                         \"className\": \"h-8\",                                         \"style\": {                                             \"cursor\": \"pointer\",                                             \"backgroundColor\": $state.hoveredCluster === cluster.id ? \"#f0f0f0\" : \"transparent\"                                         },                                         \"onMouseEnter\": () =&gt; { $state.hoveredCluster = cluster.id; },                                         \"onMouseLeave\": () =&gt; { $state.hoveredCluster = null; }                                     },                                     [\"td\", {\"className\": \"py-1\"},                                          [\"div\", {\"className\": \"flex items-center\"},                                             [\"div\", {                                                 \"style\": {                                                     \"backgroundColor\": plotColors[cluster.id % 20],                                                     \"width\": \"24px\",                                                     \"height\": \"24px\",                                                     \"borderRadius\": \"4px\",                                                     \"border\": \"1px solid rgba(0,0,0,0.2)\",                                                     \"display\": \"inline-block\",                                                     \"marginRight\": \"8px\"                                                 }                                             }],                                             `Cluster ${cluster.id}`                                         ]                                     ],                                     [\"td\", {\"className\": \"py-1\"}, cluster.weight.toFixed(4)],                                     [\"td\", {\"className\": \"py-1\"}, cluster.count]                                     ]                                 ),                                 ...placeholders                             ]                         ]                     ]                 ];             }()\"\"\"             ),         ],     ]) ) Out[6]: <p>For the interested reader, here's some exercises to try out to make this model better:</p> <ol> <li>Extend the model to infer the variance of the clusters by putting an inverse_gamma prior replacing the <code>OBS_VARIANCE</code> hyperparameter and doing block-Gibbs on it using the normal-inverse-gamma conjugacy</li> <li>Try a better initialization of the datapoint assignment: pick a point a use something like k-means and assign all the surrounding points to the same initial cluster. Iterate on all the points until they all have some initial cluster.</li> <li>Improve inference using SMC via data annealing: subssample 1/100 of the data and run inference on this, then run inference again on 1/10 of the data starting with the inferred choices for cluster means and weights from the previous trace, and finally repeat for the whole data.</li> </ol> <p>Note that the model is still expected to get stuck in local minima (the clustering at the borders isn't great), and one way to improve upon it would be to use a split-merge move, via reversible-jump MCMC.</p>"},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html#block-gibbs-on-dirichlet-mixture-model","title":"Block-Gibbs on Dirichlet Mixture Model\u00b6","text":""},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html#clustering-points-on-the-real-line","title":"Clustering Points on the Real Line\u00b6","text":"<p>The goal here is to cluster datapoints on the real line. To do so, we model a fixed number of clusters, each as a 1D-Gaussian with fixed variance, and we want to infer their means.</p>"},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html#model-description","title":"Model Description\u00b6","text":"<p>The \"model of the world\" postulates:</p> <ul> <li>A fixed number of 1D Gaussians</li> <li>Each Gaussian is assigned a weight, representing the proportion of points assigned to each cluster</li> <li>Each datapoint is assigned to a cluster</li> </ul>"},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html#generative-process","title":"Generative Process\u00b6","text":"<p>We turn this into a generative model as follows:</p> <ul> <li>We have a fixed prior mean and variance for where the cluster centers might be</li> <li>We sample a mean for each cluster</li> <li>We sample an initial weight per cluster (sum of weights is 1)</li> <li>For each datapoint:<ul> <li>We sample a cluster assignment proportional to the cluster weights</li> <li>We sample the datapoint noisily around the mean of the cluster</li> </ul> </li> </ul>"},{"location":"cookbook/inactive/update/7_application_dirichlet_mixture_model.html#implementation-details","title":"Implementation Details\u00b6","text":"<p>We, the modelers, get to choose how this process is implemented.</p> <ul> <li>We choose distributions for each sampling step in a way that makes inference tractable.</li> <li>More precisely, we choose conjugate pairs so that we can do inference via Gibbs sampling.<ul> <li>Gibbs sampling is an MCMC method that samples an initial trace, and then updates the traced choices we want to infer over time.</li> <li>To update a choice, Gibbs sampling samples from a conditional distribution, which is tractable with conjugate relationships.</li> </ul> </li> </ul>"},{"location":"library/combinators.html","title":"Combinators: structured patterns of composition","text":"<p>While the programmatic <code>genjax.StaticGenerativeFunction</code> language is powerful, its restrictions can be limiting. Combinators are a way to express common patterns of composition in a more concise way, and to gain access to effects which are common in JAX (like <code>jax.vmap</code>) for generative computations.</p> <p>Each of the combinators below is implemented as a method on <code>genjax.GenerativeFunction</code> and as a standalone decorator.</p> <p>You should strongly prefer the method form. Here's an example of the <code>vmap</code> combinator created by the <code>genjax.GenerativeFunction.vmap</code> method:</p> <p>Here is the <code>vmap</code> combinator used as a method. <code>square_many</code> below accepts an array and returns an array:</p> <pre><code>import jax, genjax\n\n@genjax.gen\ndef square(x):\n    return x * x\n\nsquare_many = square.vmap()\n</code></pre> <p>Here is <code>square_many</code> defined with <code>genjax.vmap</code>, the decorator version of the <code>vmap</code> method:</p> <pre><code>@genjax.vmap()\n@genjax.gen\ndef square_many_decorator(x):\n    return x * x\n</code></pre> <p>Warning</p> <p>We do not recommend this style, since the original building block generative function won't be available by itself. Please prefer using the combinator methods, or the transformation style shown below.</p> <p>If you insist on using the decorator form, you can preserve the original function like this:</p> <pre><code>@genjax.gen\ndef square(x):\n    return x * x\n\n# Use the decorator as a transformation:\nsquare_many_better = genjax.vmap()(square)\n</code></pre>"},{"location":"library/combinators.html#vmap-like-combinators","title":"<code>vmap</code>-like Combinators","text":""},{"location":"library/combinators.html#genjax.vmap","title":"genjax.vmap","text":"<pre><code>vmap(\n    *, in_axes: InAxes = 0\n) -&gt; Callable[[GenerativeFunction[R]], Vmap[R]]\n</code></pre> <p>Returns a decorator that wraps a <code>GenerativeFunction</code> and returns a new <code>GenerativeFunction</code> that performs a vectorized map over the argument specified by <code>in_axes</code>. Traced values are nested under an index, and the retval is vectorized.</p> <p>Parameters:</p> Name Type Description Default <code>InAxes</code> <p>Selector specifying which input arguments (or index into them) should be vectorized. <code>in_axes</code> must match (or prefix) the <code>Pytree</code> type of the argument tuple for the underlying <code>gen_fn</code>. Defaults to 0, i.e., the first argument. See this link for more detail.</p> <code>0</code> <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[R]], Vmap[R]]</code> <p>A decorator that converts a <code>genjax.GenerativeFunction</code> into a new <code>genjax.GenerativeFunction</code> that accepts an argument of one-higher dimension at the position specified by <code>in_axes</code>.</p> <p>Examples:</p> <pre><code>import jax, genjax\nimport jax.numpy as jnp\n\n\n@genjax.vmap(in_axes=0)\n@genjax.gen\ndef vmapped_model(x):\n    v = genjax.normal(x, 1.0) @ \"v\"\n    return genjax.normal(v, 0.01) @ \"q\"\n\n\nkey = jax.random.key(314159)\narr = jnp.ones(100)\n\n# `vmapped_model` accepts an array of numbers:\ntr = jax.jit(vmapped_model.simulate)(key, (arr,))\n\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/vmap.py</code> <pre><code>def vmap(*, in_axes: InAxes = 0) -&gt; Callable[[GenerativeFunction[R]], Vmap[R]]:\n    \"\"\"\n    Returns a decorator that wraps a [`GenerativeFunction`][genjax.GenerativeFunction] and returns a new `GenerativeFunction` that performs a vectorized map over the argument specified by `in_axes`. Traced values are nested under an index, and the retval is vectorized.\n\n    Args:\n        in_axes: Selector specifying which input arguments (or index into them) should be vectorized. `in_axes` must match (or prefix) the `Pytree` type of the argument tuple for the underlying `gen_fn`. Defaults to 0, i.e., the first argument. See [this link](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees) for more detail.\n\n    Returns:\n        A decorator that converts a [`genjax.GenerativeFunction`][] into a new [`genjax.GenerativeFunction`][] that accepts an argument of one-higher dimension at the position specified by `in_axes`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"vmap\"\n        import jax, genjax\n        import jax.numpy as jnp\n\n\n        @genjax.vmap(in_axes=0)\n        @genjax.gen\n        def vmapped_model(x):\n            v = genjax.normal(x, 1.0) @ \"v\"\n            return genjax.normal(v, 0.01) @ \"q\"\n\n\n        key = jax.random.key(314159)\n        arr = jnp.ones(100)\n\n        # `vmapped_model` accepts an array of numbers:\n        tr = jax.jit(vmapped_model.simulate)(key, (arr,))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(gen_fn: GenerativeFunction[R]) -&gt; Vmap[R]:\n        return Vmap(gen_fn, in_axes)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.vmap(in_axes)","title":"<code>in_axes</code>","text":""},{"location":"library/combinators.html#genjax.repeat","title":"genjax.repeat","text":"<pre><code>repeat(\n    *, n: int\n) -&gt; Callable[\n    [GenerativeFunction[R]], GenerativeFunction[R]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> <code>gen_fn</code> of type <code>a -&gt; b</code> and returns a new <code>GenerativeFunction</code> of type <code>a -&gt; [b]</code> that samples from <code>gen_fn</code>n<code>times, returning a vector of</code>n` results.</p> <p>The values traced by each call <code>gen_fn</code> will be nested under an integer index that matches the loop iteration index that generated it.</p> <p>This combinator is useful for creating multiple samples from the same generative model in a batched manner.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The number of times to sample from the generative function.</p> required <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[R]], GenerativeFunction[R]]</code> <p>A new <code>genjax.GenerativeFunction</code> that samples from the original function <code>n</code> times.</p> <p>Examples:</p> <pre><code>import genjax, jax\n\n\n@genjax.repeat(n=10)\n@genjax.gen\ndef normal_draws(mean):\n    return genjax.normal(mean, 1.0) @ \"x\"\n\n\nkey = jax.random.key(314159)\n\n# Generate 10 draws from a normal distribution with mean 2.0\ntr = jax.jit(normal_draws.simulate)(key, (2.0,))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/repeat.py</code> <pre><code>def repeat(*, n: int) -&gt; Callable[[GenerativeFunction[R]], GenerativeFunction[R]]:\n    \"\"\"\n    Returns a decorator that wraps a [`genjax.GenerativeFunction`][] `gen_fn` of type `a -&gt; b` and returns a new `GenerativeFunction` of type `a -&gt; [b]` that samples from `gen_fn `n` times, returning a vector of `n` results.\n\n    The values traced by each call `gen_fn` will be nested under an integer index that matches the loop iteration index that generated it.\n\n    This combinator is useful for creating multiple samples from the same generative model in a batched manner.\n\n    Args:\n        n: The number of times to sample from the generative function.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] that samples from the original function `n` times.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"repeat\"\n        import genjax, jax\n\n\n        @genjax.repeat(n=10)\n        @genjax.gen\n        def normal_draws(mean):\n            return genjax.normal(mean, 1.0) @ \"x\"\n\n\n        key = jax.random.key(314159)\n\n        # Generate 10 draws from a normal distribution with mean 2.0\n        tr = jax.jit(normal_draws.simulate)(key, (2.0,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(gen_fn: GenerativeFunction[R]) -&gt; GenerativeFunction[R]:\n        return RepeatCombinator(gen_fn, n=n)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.repeat(n)","title":"<code>n</code>","text":""},{"location":"library/combinators.html#scan-like-combinators","title":"<code>scan</code>-like Combinators","text":""},{"location":"library/combinators.html#genjax.scan","title":"genjax.scan","text":"<pre><code>scan(*, n: int | None = None) -&gt; Callable[\n    [GenerativeFunction[tuple[Carry, Y]]],\n    GenerativeFunction[tuple[Carry, Y]],\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; (c, b)</code>and returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; (c, [b])</code> where.</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> <li><code>b</code> may be a primitive, an array type or a pytree (container) type with array leaves.</li> </ul> <p>The values traced by each call to the original generative function will be nested under an integer index that matches the loop iteration index that generated it.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>When the type of <code>xs</code> in the snippet below (denoted <code>[a]</code> above) is an array type or None, and the type of <code>ys</code> in the snippet below (denoted <code>[b]</code> above) is an array type, the semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def scan(f, init, xs, length=None):\n    if xs is None:\n        xs = [None] * length\n    carry = init\n    ys = []\n    for x in xs:\n        carry, y = f(carry, x)\n        ys.append(y)\n    return carry, np.stack(ys)\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>ys</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays. <code>None</code> is actually a special case of this, as it represents an empty pytree.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int | None</code> <p>optional integer specifying the number of loop iterations, which (if supplied) must agree with the sizes of leading axes of the arrays in the returned function's second argument. If supplied then the returned generative function can take <code>None</code> as its second argument.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[tuple[Carry, Y]]], GenerativeFunction[tuple[Carry, Y]]]</code> <p>A new <code>genjax.GenerativeFunction</code> that takes a loop-carried value and a new input, and returns a new loop-carried value along with either <code>None</code> or an output to be collected into the second return value.</p> <p>Examples:</p> <p>Scan for 1000 iterations with no array input: <pre><code>import jax\nimport genjax\n\n\n@genjax.scan(n=1000)\n@genjax.gen\ndef random_walk(prev, _):\n    x = genjax.normal(prev, 1.0) @ \"x\"\n    return x, None\n\n\ninit = 0.5\nkey = jax.random.key(314159)\n\ntr = jax.jit(random_walk.simulate)(key, (init, None))\nprint(tr.render_html())\n</code></pre> </p> <p>Scan across an input array: <pre><code>import jax.numpy as jnp\n\n\n@genjax.scan()\n@genjax.gen\ndef add_and_square_all(sum, x):\n    new_sum = sum + x\n    return new_sum, sum * sum\n\n\ninit = 0.0\nxs = jnp.ones(10)\n\ntr = jax.jit(add_and_square_all.simulate)(key, (init, xs))\n\n# The retval has the final carry and an array of all `sum*sum` returned.\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def scan(\n    *, n: int | None = None\n) -&gt; Callable[\n    [GenerativeFunction[tuple[Carry, Y]]], GenerativeFunction[tuple[Carry, Y]]\n]:\n    \"\"\"Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type\n    `(c, a) -&gt; (c, b)`and returns a new [`genjax.GenerativeFunction`][] of type\n    `(c, [a]) -&gt; (c, [b])` where.\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n    - `b` may be a primitive, an array type or a pytree (container) type with array leaves.\n\n    The values traced by each call to the original generative function will be nested under an integer index that matches the loop iteration index that generated it.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    When the type of `xs` in the snippet below (denoted `[a]` above) is an array type or None, and the type of `ys` in the snippet below (denoted `[b]` above) is an array type, the semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def scan(f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n    ```\n\n    Unlike that Python version, both `xs` and `ys` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays. `None` is actually a special case of this, as it represents an empty pytree.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: optional integer specifying the number of loop iterations, which (if supplied) must agree with the sizes of leading axes of the arrays in the returned function's second argument. If supplied then the returned generative function can take `None` as its second argument.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] that takes a loop-carried value and a new input, and returns a new loop-carried value along with either `None` or an output to be collected into the second return value.\n\n    Examples:\n        Scan for 1000 iterations with no array input:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.scan(n=1000)\n        @genjax.gen\n        def random_walk(prev, _):\n            x = genjax.normal(prev, 1.0) @ \"x\"\n            return x, None\n\n\n        init = 0.5\n        key = jax.random.key(314159)\n\n        tr = jax.jit(random_walk.simulate)(key, (init, None))\n        print(tr.render_html())\n        ```\n\n        Scan across an input array:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax.numpy as jnp\n\n\n        @genjax.scan()\n        @genjax.gen\n        def add_and_square_all(sum, x):\n            new_sum = sum + x\n            return new_sum, sum * sum\n\n\n        init = 0.0\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add_and_square_all.simulate)(key, (init, xs))\n\n        # The retval has the final carry and an array of all `sum*sum` returned.\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[tuple[Carry, Y]]):\n        return Scan[Carry, Y](f, length=n)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.scan(n)","title":"<code>n</code>","text":""},{"location":"library/combinators.html#genjax.accumulate","title":"genjax.accumulate","text":"<pre><code>accumulate() -&gt; Callable[\n    [GenerativeFunction[Carry]],\n    GenerativeFunction[Carry],\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; [c]</code> where.</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>[c]</code> is an array of all loop-carried values seen during iteration (including the first)</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> </ul> <p>All traced values are nested under an index.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation (note the similarity to <code>itertools.accumulate</code>):</p> <pre><code>def accumulate(f, init, xs):\n    carry = init\n    carries = [init]\n    for x in xs:\n        carry = f(carry, x)\n        carries.append(carry)\n    return carries\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>carries</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Examples:</p> <p>accumulate a running total: <pre><code>import jax\nimport genjax\nimport jax.numpy as jnp\n\n\n@genjax.accumulate()\n@genjax.gen\ndef add(sum, x):\n    new_sum = sum + x\n    return new_sum\n\n\ninit = 0.0\nkey = jax.random.key(314159)\nxs = jnp.ones(10)\n\ntr = jax.jit(add.simulate)(key, (init, xs))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def accumulate() -&gt; Callable[[GenerativeFunction[Carry]], GenerativeFunction[Carry]]:\n    \"\"\"Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type\n    `(c, a) -&gt; c` and returns a new [`genjax.GenerativeFunction`][] of type\n    `(c, [a]) -&gt; [c]` where.\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `[c]` is an array of all loop-carried values seen during iteration (including the first)\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n    All traced values are nested under an index.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`itertools.accumulate`](https://docs.python.org/3/library/itertools.html#itertools.accumulate)):\n\n    ```python\n    def accumulate(f, init, xs):\n        carry = init\n        carries = [init]\n        for x in xs:\n            carry = f(carry, x)\n            carries.append(carry)\n        return carries\n    ```\n\n    Unlike that Python version, both `xs` and `carries` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Examples:\n        accumulate a running total:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n        import jax.numpy as jnp\n\n\n        @genjax.accumulate()\n        @genjax.gen\n        def add(sum, x):\n            new_sum = sum + x\n            return new_sum\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add.simulate)(key, (init, xs))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[Carry]) -&gt; GenerativeFunction[Carry]:\n        return (\n            f.map(lambda ret: (ret, ret))\n            .scan()\n            .dimap(pre=lambda *args: args, post=prepend_initial_acc)\n        )\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.reduce","title":"genjax.reduce","text":"<pre><code>reduce() -&gt; Callable[\n    [GenerativeFunction[Carry]],\n    GenerativeFunction[Carry],\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; c</code> where.</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> </ul> <p>All traced values are nested under an index.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation (note the similarity to <code>functools.reduce</code>):</p> <pre><code>def reduce(f, init, xs):\n    carry = init\n    for x in xs:\n        carry = f(carry, x)\n    return carry\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>carry</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Examples:</p> <p>sum an array of numbers: <pre><code>import jax\nimport genjax\nimport jax.numpy as jnp\n\n\n@genjax.reduce()\n@genjax.gen\ndef add(sum, x):\n    new_sum = sum + x\n    return new_sum\n\n\ninit = 0.0\nkey = jax.random.key(314159)\nxs = jnp.ones(10)\n\ntr = jax.jit(add.simulate)(key, (init, xs))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def reduce() -&gt; Callable[[GenerativeFunction[Carry]], GenerativeFunction[Carry]]:\n    \"\"\"Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type\n    `(c, a) -&gt; c` and returns a new [`genjax.GenerativeFunction`][] of type\n    `(c, [a]) -&gt; c` where.\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n    All traced values are nested under an index.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`functools.reduce`](https://docs.python.org/3/library/itertools.html#functools.reduce)):\n\n    ```python\n    def reduce(f, init, xs):\n        carry = init\n        for x in xs:\n            carry = f(carry, x)\n        return carry\n    ```\n\n    Unlike that Python version, both `xs` and `carry` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Examples:\n        sum an array of numbers:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n        import jax.numpy as jnp\n\n\n        @genjax.reduce()\n        @genjax.gen\n        def add(sum, x):\n            new_sum = sum + x\n            return new_sum\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add.simulate)(key, (init, xs))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[Carry]) -&gt; GenerativeFunction[Carry]:\n        def pre(ret: Carry):\n            return ret, None\n\n        def post(ret: tuple[Carry, None]):\n            return ret[0]\n\n        return f.map(pre).scan().map(post)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.iterate","title":"genjax.iterate","text":"<pre><code>iterate(\n    *, n: int\n) -&gt; Callable[\n    [GenerativeFunction[Y]], GenerativeFunction[Y]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; [a]</code> where.</p> <ul> <li><code>a</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>[a]</code> is an array of all <code>a</code>, <code>f(a)</code>, <code>f(f(a))</code> etc. values seen during iteration.</li> </ul> <p>All traced values are nested under an index.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def iterate(f, n, init):\n    input = init\n    seen = [init]\n    for _ in range(n):\n        input = f(input)\n        seen.append(input)\n    return seen\n</code></pre> <p><code>init</code> may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.</p> <p>The iterated value <code>a</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>a</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>the number of iterations to run.</p> required <p>Examples:</p> <p>iterative addition, returning all intermediate sums: <pre><code>import jax\nimport genjax\n\n\n@genjax.iterate(n=100)\n@genjax.gen\ndef inc(x):\n    return x + 1\n\n\ninit = 0.0\nkey = jax.random.key(314159)\n\ntr = jax.jit(inc.simulate)(key, (init,))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def iterate(*, n: int) -&gt; Callable[[GenerativeFunction[Y]], GenerativeFunction[Y]]:\n    \"\"\"Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type\n    `a -&gt; a` and returns a new [`genjax.GenerativeFunction`][] of type `a -&gt;\n    [a]` where.\n\n    - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `[a]` is an array of all `a`, `f(a)`, `f(f(a))` etc. values seen during iteration.\n\n    All traced values are nested under an index.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def iterate(f, n, init):\n        input = init\n        seen = [init]\n        for _ in range(n):\n            input = f(input)\n            seen.append(input)\n        return seen\n    ```\n\n    `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n    The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: the number of iterations to run.\n\n    Examples:\n        iterative addition, returning all intermediate sums:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.iterate(n=100)\n        @genjax.gen\n        def inc(x):\n            return x + 1\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n\n        tr = jax.jit(inc.simulate)(key, (init,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[Y]) -&gt; GenerativeFunction[Y]:\n        # strip off the JAX-supplied `None` on the way in, accumulate `ret` on the way out.\n        return (\n            f.dimap(\n                pre=lambda *args: args[:-1],\n                post=lambda _args, _xformed, ret: (ret, ret),\n            )\n            .scan(n=n)\n            .dimap(pre=lambda *args: (*args, None), post=prepend_initial_acc)\n        )\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.iterate(n)","title":"<code>n</code>","text":""},{"location":"library/combinators.html#genjax.iterate_final","title":"genjax.iterate_final","text":"<pre><code>iterate_final(\n    *, n: int\n) -&gt; Callable[\n    [GenerativeFunction[Y]], GenerativeFunction[Y]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> where.</p> <ul> <li><code>a</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li>the original function is invoked <code>n</code> times with each input coming from the previous invocation's output, so that the new function returns \\(f^n(a)\\)</li> </ul> <p>All traced values are nested under an index.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def iterate_final(f, n, init):\n    ret = init\n    for _ in range(n):\n        ret = f(ret)\n    return ret\n</code></pre> <p><code>init</code> may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.</p> <p>The iterated value <code>a</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>a</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>the number of iterations to run.</p> required <p>Examples:</p> <p>iterative addition: <pre><code>import jax\nimport genjax\n\n\n@genjax.iterate_final(n=100)\n@genjax.gen\ndef inc(x):\n    return x + 1\n\n\ninit = 0.0\nkey = jax.random.key(314159)\n\ntr = jax.jit(inc.simulate)(key, (init,))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def iterate_final(\n    *, n: int\n) -&gt; Callable[[GenerativeFunction[Y]], GenerativeFunction[Y]]:\n    \"\"\"Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type\n    `a -&gt; a` and returns a new [`genjax.GenerativeFunction`][] of type `a -&gt; a`\n    where.\n\n    - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - the original function is invoked `n` times with each input coming from the previous invocation's output, so that the new function returns $f^n(a)$\n\n    All traced values are nested under an index.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def iterate_final(f, n, init):\n        ret = init\n        for _ in range(n):\n            ret = f(ret)\n        return ret\n    ```\n\n    `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n    The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: the number of iterations to run.\n\n    Examples:\n        iterative addition:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.iterate_final(n=100)\n        @genjax.gen\n        def inc(x):\n            return x + 1\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n\n        tr = jax.jit(inc.simulate)(key, (init,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[Y]) -&gt; GenerativeFunction[Y]:\n        # strip off the JAX-supplied `None` on the way in, no accumulation on the way out.\n        def pre_post(_, _xformed, ret: Y):\n            return ret, None\n\n        def post_post(_, _xformed, ret: tuple[Y, None]):\n            return ret[0]\n\n        return (\n            f.dimap(pre=lambda *args: args[:-1], post=pre_post)\n            .scan(n=n)\n            .dimap(pre=lambda *args: (*args, None), post=post_post)\n        )\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.iterate_final(n)","title":"<code>n</code>","text":""},{"location":"library/combinators.html#genjax.masked_iterate","title":"genjax.masked_iterate","text":"<pre><code>masked_iterate() -&gt; (\n    Callable[\n        [GenerativeFunction[Y]], GenerativeFunction[Y]\n    ]\n)\n</code></pre> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a list of values of type <code>a</code>.</p> <p>The original function is modified to accept an additional argument <code>mask</code>, which is a boolean value indicating whether the operation should be masked or not. The function returns a Masked list of results of the original operation with the input [mask] as mask.</p> <p>All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.</p> Example <pre><code>import jax\nimport genjax\n\nmasks = jnp.array([True, False, True])\n\n\n# Create a kernel generative function\n@genjax.gen\ndef step(x):\n    _ = (\n        genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n        @ \"rats\"\n    )\n    return x\n\n\n# Create a model using masked_iterate\nmodel = genjax.masked_iterate()(step)\n\n# Simulate from the model\nkey = jax.random.key(0)\nmask_steps = jnp.arange(10) &lt; 5\ntr = model.simulate(key, (0.0, mask_steps))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def masked_iterate() -&gt; Callable[[GenerativeFunction[Y]], GenerativeFunction[Y]]:\n    \"\"\"\n    Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a list of values of type `a`.\n\n    The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns a Masked list of results of the original operation with the input [mask] as mask.\n\n    All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n        masks = jnp.array([True, False, True])\n\n\n        # Create a kernel generative function\n        @genjax.gen\n        def step(x):\n            _ = (\n                genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                @ \"rats\"\n            )\n            return x\n\n\n        # Create a model using masked_iterate\n        model = genjax.masked_iterate()(step)\n\n        # Simulate from the model\n        key = jax.random.key(0)\n        mask_steps = jnp.arange(10) &lt; 5\n        tr = model.simulate(key, (0.0, mask_steps))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(step: GenerativeFunction[Y]) -&gt; GenerativeFunction[Y]:\n        def pre(state, flag: Flag):\n            return flag, state\n\n        def post(_unused_args, _xformed, masked_retval: Mask[Y]):\n            v = masked_retval.value\n            return v, v\n\n        # scan_step: (a, bool) -&gt; a\n        scan_step = step.mask().dimap(pre=pre, post=post)\n        return scan_step.scan().dimap(pre=lambda *args: args, post=prepend_initial_acc)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.masked_iterate_final","title":"genjax.masked_iterate_final","text":"<pre><code>masked_iterate_final() -&gt; (\n    Callable[\n        [GenerativeFunction[Y]], GenerativeFunction[Y]\n    ]\n)\n</code></pre> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a value of type <code>a</code>.</p> <p>The original function is modified to accept an additional argument <code>mask</code>, which is a boolean value indicating whether the operation should be masked or not. The function returns the result of the original operation if <code>mask</code> is <code>True</code>, and the original input if <code>mask</code> is <code>False</code>.</p> <p>All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.</p> Example <pre><code>import jax\nimport genjax\n\nmasks = jnp.array([True, False, True])\n\n\n# Create a kernel generative function\n@genjax.gen\ndef step(x):\n    _ = (\n        genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n        @ \"rats\"\n    )\n    return x\n\n\n# Create a model using masked_iterate_final\nmodel = genjax.masked_iterate_final()(step)\n\n# Simulate from the model\nkey = jax.random.key(0)\nmask_steps = jnp.arange(10) &lt; 5\ntr = model.simulate(key, (0.0, mask_steps))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/scan.py</code> <pre><code>def masked_iterate_final() -&gt; Callable[[GenerativeFunction[Y]], GenerativeFunction[Y]]:\n    \"\"\"\n    Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a value of type `a`.\n\n    The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns the result of the original operation if `mask` is `True`, and the original input if `mask` is `False`.\n\n    All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n        masks = jnp.array([True, False, True])\n\n\n        # Create a kernel generative function\n        @genjax.gen\n        def step(x):\n            _ = (\n                genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                @ \"rats\"\n            )\n            return x\n\n\n        # Create a model using masked_iterate_final\n        model = genjax.masked_iterate_final()(step)\n\n        # Simulate from the model\n        key = jax.random.key(0)\n        mask_steps = jnp.arange(10) &lt; 5\n        tr = model.simulate(key, (0.0, mask_steps))\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def decorator(step: GenerativeFunction[Y]) -&gt; GenerativeFunction[Y]:\n        def pre(state, flag: Flag):\n            return flag, state\n\n        def post(_unused_args, _xformed, masked_retval: Mask[Y]):\n            return masked_retval.value, None\n\n        # scan_step: (a, bool) -&gt; a\n        scan_step = step.mask().dimap(pre=pre, post=post)\n        return scan_step.scan().map(lambda ret: ret[0])\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#control-flow-combinators","title":"Control Flow Combinators","text":""},{"location":"library/combinators.html#genjax.or_else","title":"genjax.or_else","text":"<pre><code>or_else(\n    if_gen_fn: GenerativeFunction[R],\n    else_gen_fn: GenerativeFunction[R],\n) -&gt; GenerativeFunction[R]\n</code></pre> <p>Given two <code>genjax.GenerativeFunction</code>s <code>if_gen_fn</code> and <code>else_gen_fn</code>, returns a new <code>genjax.GenerativeFunction</code> that accepts</p> <ul> <li>a boolean argument</li> <li>an argument tuple for <code>if_gen_fn</code></li> <li>an argument tuple for the supplied <code>else_gen_fn</code></li> </ul> <p>and acts like <code>if_gen_fn</code> when the boolean is <code>True</code> or <code>else_gen_fn</code> otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>called when the boolean argument is <code>False</code>.</p> required <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A <code>genjax.GenerativeFunction</code> modified for conditional execution.</p> <p>Examples:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\n\n\n@genjax.gen\ndef if_model(x):\n    return genjax.normal(x, 1.0) @ \"if_value\"\n\n\n@genjax.gen\ndef else_model(x):\n    return genjax.normal(x, 5.0) @ \"else_value\"\n\n\nor_else_model = genjax.or_else(if_model, else_model)\n\n\n@genjax.gen\ndef model(toss: bool):\n    # Note that `or_else_model` takes a new boolean predicate in\n    # addition to argument tuples for each branch.\n    return or_else_model(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\nkey = jax.random.key(314159)\n\ntr = jax.jit(model.simulate)(key, (True,))\n\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/or_else.py</code> <pre><code>def or_else(\n    if_gen_fn: GenerativeFunction[R],\n    else_gen_fn: GenerativeFunction[R],\n) -&gt; GenerativeFunction[R]:\n    \"\"\"\n    Given two [`genjax.GenerativeFunction`][]s `if_gen_fn` and `else_gen_fn`, returns a new [`genjax.GenerativeFunction`][] that accepts\n\n    - a boolean argument\n    - an argument tuple for `if_gen_fn`\n    - an argument tuple for the supplied `else_gen_fn`\n\n    and acts like `if_gen_fn` when the boolean is `True` or `else_gen_fn` otherwise.\n\n    Args:\n        else_gen_fn: called when the boolean argument is `False`.\n\n    Returns:\n        A [`genjax.GenerativeFunction`][] modified for conditional execution.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"or_else\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n\n\n        @genjax.gen\n        def if_model(x):\n            return genjax.normal(x, 1.0) @ \"if_value\"\n\n\n        @genjax.gen\n        def else_model(x):\n            return genjax.normal(x, 5.0) @ \"else_value\"\n\n\n        or_else_model = genjax.or_else(if_model, else_model)\n\n\n        @genjax.gen\n        def model(toss: bool):\n            # Note that `or_else_model` takes a new boolean predicate in\n            # addition to argument tuples for each branch.\n            return or_else_model(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\n        key = jax.random.key(314159)\n\n        tr = jax.jit(model.simulate)(key, (True,))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n\n    def argument_mapping(\n        b: ScalarFlag, if_args: tuple[Any, ...], else_args: tuple[Any, ...]\n    ):\n        # Note that `True` maps to 0 to select the \"if\" branch, `False` to 1.\n        idx = jnp.array(jnp.logical_not(b), dtype=int)\n        return (idx, if_args, else_args)\n\n    return if_gen_fn.switch(else_gen_fn).contramap(argument_mapping)\n</code></pre>"},{"location":"library/combinators.html#genjax.or_else(else_gen_fn)","title":"<code>else_gen_fn</code>","text":""},{"location":"library/combinators.html#genjax.switch","title":"genjax.switch","text":"<pre><code>switch(*gen_fns: GenerativeFunction[R]) -&gt; Switch[R]\n</code></pre> <p>Given <code>n</code> <code>genjax.GenerativeFunction</code> inputs, returns a <code>genjax.GenerativeFunction</code> that accepts <code>n+1</code> arguments:</p> <ul> <li>an index in the range \\([0, n)\\)</li> <li>a tuple of arguments for each of the input generative functions (<code>n</code> total tuples)</li> </ul> <p>and executes the generative function at the supplied index with its provided arguments.</p> <p>If <code>index</code> is out of bounds, <code>index</code> is clamped to within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>generative functions that the <code>Switch</code> will select from.</p> <code>()</code> <p>Examples:</p> <p>Create a <code>Switch</code> via the <code>genjax.switch</code> method: <pre><code>import jax, genjax\n\n\n@genjax.gen\ndef branch_1():\n    x = genjax.normal(0.0, 1.0) @ \"x1\"\n\n\n@genjax.gen\ndef branch_2():\n    x = genjax.bernoulli(probs=0.3) @ \"x2\"\n\n\nswitch = genjax.switch(branch_1, branch_2)\n\nkey = jax.random.key(314159)\njitted = jax.jit(switch.simulate)\n\n# Select `branch_2` by providing 1:\ntr = jitted(key, (1, (), ()))\n\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/switch.py</code> <pre><code>def switch(\n    *gen_fns: GenerativeFunction[R],\n) -&gt; Switch[R]:\n    \"\"\"\n    Given `n` [`genjax.GenerativeFunction`][] inputs, returns a [`genjax.GenerativeFunction`][] that accepts `n+1` arguments:\n\n    - an index in the range $[0, n)$\n    - a tuple of arguments for each of the input generative functions (`n` total tuples)\n\n    and executes the generative function at the supplied index with its provided arguments.\n\n    If `index` is out of bounds, `index` is clamped to within bounds.\n\n    Args:\n        gen_fns: generative functions that the `Switch` will select from.\n\n    Examples:\n        Create a `Switch` via the [`genjax.switch`][] method:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"switch\"\n        import jax, genjax\n\n\n        @genjax.gen\n        def branch_1():\n            x = genjax.normal(0.0, 1.0) @ \"x1\"\n\n\n        @genjax.gen\n        def branch_2():\n            x = genjax.bernoulli(probs=0.3) @ \"x2\"\n\n\n        switch = genjax.switch(branch_1, branch_2)\n\n        key = jax.random.key(314159)\n        jitted = jax.jit(switch.simulate)\n\n        # Select `branch_2` by providing 1:\n        tr = jitted(key, (1, (), ()))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n    return Switch[R](gen_fns)\n</code></pre>"},{"location":"library/combinators.html#genjax.switch(gen_fns)","title":"<code>gen_fns</code>","text":""},{"location":"library/combinators.html#argument-and-return-transformations","title":"Argument and Return Transformations","text":""},{"location":"library/combinators.html#genjax.dimap","title":"genjax.dimap","text":"<pre><code>dimap(\n    *,\n    pre: Callable[..., ArgTuple] = lambda *args: args,\n    post: Callable[\n        [tuple[Any, ...], ArgTuple, R], S\n    ] = lambda _, _xformed, retval: retval\n) -&gt; Callable[\n    [GenerativeFunction[R]], Dimap[ArgTuple, R, S]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> and applies pre- and post-processing functions to its arguments and return value.</p> <p>Info</p> <p>Prefer <code>genjax.map</code> if you only need to transform the return value, or <code>genjax.contramap</code> if you need to transform the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[..., ArgTuple]</code> <p>A callable that preprocesses the arguments before passing them to the wrapped function. Note that <code>pre</code> must return a tuple of arguments, not a bare argument. Default is the identity function.</p> <code>lambda *args: args</code> <code>Callable[[tuple[Any, ...], ArgTuple, R], S]</code> <p>A callable that postprocesses the return value of the wrapped function. Default is the identity function.</p> <code>lambda _, _xformed, retval: retval</code> <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[R]], Dimap[ArgTuple, R, S]]</code> <p>A decorator that takes a <code>genjax.GenerativeFunction</code> and returns a new <code>genjax.GenerativeFunction</code> with the same behavior but with the arguments and return value transformed according to <code>pre</code> and <code>post</code>.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define pre- and post-processing functions\ndef pre_process(x, y):\n    return (x + 1, y * 2)\n\n\ndef post_process(args, xformed, retval):\n    return retval**2\n\n\n# Apply dimap to a generative function\n@genjax.dimap(pre=pre_process, post=post_process)\n@genjax.gen\ndef dimap_model(x, y):\n    return genjax.normal(x, y) @ \"z\"\n\n\n# Use the dimap model\nkey = jax.random.key(0)\ntrace = dimap_model.simulate(key, (2.0, 3.0))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/dimap.py</code> <pre><code>def dimap(\n    *,\n    pre: Callable[..., ArgTuple] = lambda *args: args,\n    post: Callable[[tuple[Any, ...], ArgTuple, R], S] = lambda _,\n    _xformed,\n    retval: retval,\n) -&gt; Callable[[GenerativeFunction[R]], Dimap[ArgTuple, R, S]]:\n    \"\"\"\n    Returns a decorator that wraps a [`genjax.GenerativeFunction`][] and applies pre- and post-processing functions to its arguments and return value.\n\n    !!! info\n        Prefer [`genjax.map`][] if you only need to transform the return value, or [`genjax.contramap`][] if you need to transform the arguments.\n\n    Args:\n        pre: A callable that preprocesses the arguments before passing them to the wrapped function. Note that `pre` must return a _tuple_ of arguments, not a bare argument. Default is the identity function.\n        post: A callable that postprocesses the return value of the wrapped function. Default is the identity function.\n\n    Returns:\n        A decorator that takes a [`genjax.GenerativeFunction`][] and returns a new [`genjax.GenerativeFunction`][] with the same behavior but with the arguments and return value transformed according to `pre` and `post`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"dimap\"\n        import jax, genjax\n\n\n        # Define pre- and post-processing functions\n        def pre_process(x, y):\n            return (x + 1, y * 2)\n\n\n        def post_process(args, xformed, retval):\n            return retval**2\n\n\n        # Apply dimap to a generative function\n        @genjax.dimap(pre=pre_process, post=post_process)\n        @genjax.gen\n        def dimap_model(x, y):\n            return genjax.normal(x, y) @ \"z\"\n\n\n        # Use the dimap model\n        key = jax.random.key(0)\n        trace = dimap_model.simulate(key, (2.0, 3.0))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n\n    def decorator(f: GenerativeFunction[R]) -&gt; Dimap[ArgTuple, R, S]:\n        return Dimap(f, pre, post)\n\n    return decorator\n</code></pre>"},{"location":"library/combinators.html#genjax.dimap(pre)","title":"<code>pre</code>","text":""},{"location":"library/combinators.html#genjax.dimap(post)","title":"<code>post</code>","text":""},{"location":"library/combinators.html#genjax.map","title":"genjax.map","text":"<pre><code>map(\n    f: Callable[[R], S]\n) -&gt; Callable[\n    [GenerativeFunction[R]], Dimap[tuple[Any, ...], R, S]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> and applies a post-processing function to its return value.</p> <p>This is a specialized version of <code>genjax.dimap</code> where only the post-processing function is applied.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[[R], S]</code> <p>A callable that postprocesses the return value of the wrapped function.</p> required <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[R]], Dimap[tuple[Any, ...], R, S]]</code> <p>A decorator that takes a <code>genjax.GenerativeFunction</code> and returns a new <code>genjax.GenerativeFunction</code> with the same behavior but with the return value transformed according to <code>f</code>.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define a post-processing function\ndef square(x):\n    return x**2\n\n\n# Apply map to a generative function\n@genjax.map(square)\n@genjax.gen\ndef map_model(x):\n    return genjax.normal(x, 1.0) @ \"z\"\n\n\n# Use the map model\nkey = jax.random.key(0)\ntrace = map_model.simulate(key, (2.0,))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/dimap.py</code> <pre><code>def map(\n    f: Callable[[R], S],\n) -&gt; Callable[[GenerativeFunction[R]], Dimap[tuple[Any, ...], R, S]]:\n    \"\"\"\n    Returns a decorator that wraps a [`genjax.GenerativeFunction`][] and applies a post-processing function to its return value.\n\n    This is a specialized version of [`genjax.dimap`][] where only the post-processing function is applied.\n\n    Args:\n        f: A callable that postprocesses the return value of the wrapped function.\n\n    Returns:\n        A decorator that takes a [`genjax.GenerativeFunction`][] and returns a new [`genjax.GenerativeFunction`][] with the same behavior but with the return value transformed according to `f`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"map\"\n        import jax, genjax\n\n\n        # Define a post-processing function\n        def square(x):\n            return x**2\n\n\n        # Apply map to a generative function\n        @genjax.map(square)\n        @genjax.gen\n        def map_model(x):\n            return genjax.normal(x, 1.0) @ \"z\"\n\n\n        # Use the map model\n        key = jax.random.key(0)\n        trace = map_model.simulate(key, (2.0,))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n\n    def post(_args, _xformed, x: R) -&gt; S:\n        return f(x)\n\n    return dimap(pre=lambda *args: args, post=post)\n</code></pre>"},{"location":"library/combinators.html#genjax.map(f)","title":"<code>f</code>","text":""},{"location":"library/combinators.html#genjax.contramap","title":"genjax.contramap","text":"<pre><code>contramap(\n    f: Callable[..., ArgTuple]\n) -&gt; Callable[\n    [GenerativeFunction[R]], Dimap[ArgTuple, R, R]\n]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> and applies a pre-processing function to its arguments.</p> <p>This is a specialized version of <code>genjax.dimap</code> where only the pre-processing function is applied.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[..., ArgTuple]</code> <p>A callable that preprocesses the arguments of the wrapped function. Note that <code>f</code> must return a tuple of arguments, not a bare argument.</p> required <p>Returns:</p> Type Description <code>Callable[[GenerativeFunction[R]], Dimap[ArgTuple, R, R]]</code> <p>A decorator that takes a <code>genjax.GenerativeFunction</code> and returns a new <code>genjax.GenerativeFunction</code> with the same behavior but with the arguments transformed according to <code>f</code>.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define a pre-processing function.\n# Note that this function must return a tuple of arguments!\ndef add_one(x):\n    return (x + 1,)\n\n\n# Apply contramap to a generative function\n@genjax.contramap(add_one)\n@genjax.gen\ndef contramap_model(x):\n    return genjax.normal(x, 1.0) @ \"z\"\n\n\n# Use the contramap model\nkey = jax.random.key(0)\ntrace = contramap_model.simulate(key, (2.0,))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/dimap.py</code> <pre><code>def contramap(\n    f: Callable[..., ArgTuple],\n) -&gt; Callable[[GenerativeFunction[R]], Dimap[ArgTuple, R, R]]:\n    \"\"\"\n    Returns a decorator that wraps a [`genjax.GenerativeFunction`][] and applies a pre-processing function to its arguments.\n\n    This is a specialized version of [`genjax.dimap`][] where only the pre-processing function is applied.\n\n    Args:\n        f: A callable that preprocesses the arguments of the wrapped function. Note that `f` must return a _tuple_ of arguments, not a bare argument.\n\n    Returns:\n        A decorator that takes a [`genjax.GenerativeFunction`][] and returns a new [`genjax.GenerativeFunction`][] with the same behavior but with the arguments transformed according to `f`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"contramap\"\n        import jax, genjax\n\n\n        # Define a pre-processing function.\n        # Note that this function must return a tuple of arguments!\n        def add_one(x):\n            return (x + 1,)\n\n\n        # Apply contramap to a generative function\n        @genjax.contramap(add_one)\n        @genjax.gen\n        def contramap_model(x):\n            return genjax.normal(x, 1.0) @ \"z\"\n\n\n        # Use the contramap model\n        key = jax.random.key(0)\n        trace = contramap_model.simulate(key, (2.0,))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n    return dimap(pre=f, post=lambda _args, _xformed, ret: ret)\n</code></pre>"},{"location":"library/combinators.html#genjax.contramap(f)","title":"<code>f</code>","text":""},{"location":"library/combinators.html#the-rest","title":"The Rest","text":""},{"location":"library/combinators.html#genjax.mask","title":"genjax.mask","text":"<pre><code>mask(f: GenerativeFunction[R]) -&gt; MaskCombinator[R]\n</code></pre> <p>Combinator which enables dynamic masking of generative functions. Takes a <code>genjax.GenerativeFunction</code> and returns a new <code>genjax.GenerativeFunction</code> which accepts an additional boolean first argument.</p> <p>If <code>True</code>, the invocation of the generative function is masked, and its contribution to the score is ignored. If <code>False</code>, it has the same semantics as if one was invoking the generative function without masking.</p> <p>The return value type is a <code>Mask</code>, with a flag value equal to the supplied boolean.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>The generative function to be masked.</p> required <p>Returns:</p> Type Description <code>MaskCombinator[R]</code> <p>The masked version of the input generative function.</p> <p>Examples:</p> <p>Masking a normal draw: <pre><code>import genjax, jax\n\n\n@genjax.mask\n@genjax.gen\ndef masked_normal_draw(mean):\n    return genjax.normal(mean, 1.0) @ \"x\"\n\n\nkey = jax.random.key(314159)\ntr = jax.jit(masked_normal_draw.simulate)(\n    key,\n    (\n        False,\n        2.0,\n    ),\n)\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/generative_functions/combinators/mask.py</code> <pre><code>def mask(f: GenerativeFunction[R]) -&gt; MaskCombinator[R]:\n    \"\"\"\n    Combinator which enables dynamic masking of generative functions. Takes a [`genjax.GenerativeFunction`][] and returns a new [`genjax.GenerativeFunction`][] which accepts an additional boolean first argument.\n\n    If `True`, the invocation of the generative function is masked, and its contribution to the score is ignored. If `False`, it has the same semantics as if one was invoking the generative function without masking.\n\n    The return value type is a `Mask`, with a flag value equal to the supplied boolean.\n\n    Args:\n        f: The generative function to be masked.\n\n    Returns:\n        The masked version of the input generative function.\n\n    Examples:\n        Masking a normal draw:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mask\"\n        import genjax, jax\n\n\n        @genjax.mask\n        @genjax.gen\n        def masked_normal_draw(mean):\n            return genjax.normal(mean, 1.0) @ \"x\"\n\n\n        key = jax.random.key(314159)\n        tr = jax.jit(masked_normal_draw.simulate)(\n            key,\n            (\n                False,\n                2.0,\n            ),\n        )\n        print(tr.render_html())\n        ```\n    \"\"\"\n    return MaskCombinator(f)\n</code></pre>"},{"location":"library/combinators.html#genjax.mask(f)","title":"<code>f</code>","text":""},{"location":"library/combinators.html#genjax.mix","title":"genjax.mix","text":"<pre><code>mix(\n    *gen_fns: GenerativeFunction[R],\n) -&gt; GenerativeFunction[R]\n</code></pre> <p>Creates a mixture model from a set of generative functions.</p> <p>This function takes multiple generative functions as input and returns a new generative function that represents a mixture model.</p> <p>The returned generative function takes the following arguments:</p> <ul> <li><code>mixture_logits</code>: Logits for the categorical distribution used to select a component.</li> <li><code>*args</code>: Argument tuples for each of the input generative functions</li> </ul> <p>and samples from one of the input generative functions based on draw from a categorical distribution defined by the provided mixture logits.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>Variable number of <code>genjax.GenerativeFunction</code>s to be mixed.</p> <code>()</code> <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A new <code>genjax.GenerativeFunction</code> representing the mixture model.</p> <p>Examples:</p> <pre><code>import jax\nimport genjax\n\n\n# Define component generative functions\n@genjax.gen\ndef component1(x):\n    return genjax.normal(x, 1.0) @ \"y\"\n\n\n@genjax.gen\ndef component2(x):\n    return genjax.normal(x, 2.0) @ \"y\"\n\n\n# Create mixture model\nmixture = genjax.mix(component1, component2)\n\n# Use the mixture model\nkey = jax.random.key(0)\nlogits = jax.numpy.array([0.3, 0.7])  # Favors component2\ntrace = mixture.simulate(key, (logits, (0.0,), (7.0,)))\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/combinators/mixture.py</code> <pre><code>def mix(*gen_fns: GenerativeFunction[R]) -&gt; GenerativeFunction[R]:\n    \"\"\"\n    Creates a mixture model from a set of generative functions.\n\n    This function takes multiple generative functions as input and returns a new generative function that represents a mixture model.\n\n    The returned generative function takes the following arguments:\n\n    - `mixture_logits`: Logits for the categorical distribution used to select a component.\n    - `*args`: Argument tuples for each of the input generative functions\n\n    and samples from one of the input generative functions based on draw from a categorical distribution defined by the provided mixture logits.\n\n    Args:\n        *gen_fns: Variable number of [`genjax.GenerativeFunction`][]s to be mixed.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] representing the mixture model.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mix\"\n        import jax\n        import genjax\n\n\n        # Define component generative functions\n        @genjax.gen\n        def component1(x):\n            return genjax.normal(x, 1.0) @ \"y\"\n\n\n        @genjax.gen\n        def component2(x):\n            return genjax.normal(x, 2.0) @ \"y\"\n\n\n        # Create mixture model\n        mixture = genjax.mix(component1, component2)\n\n        # Use the mixture model\n        key = jax.random.key(0)\n        logits = jax.numpy.array([0.3, 0.7])  # Favors component2\n        trace = mixture.simulate(key, (logits, (0.0,), (7.0,)))\n        print(trace.render_html())\n        ```\n    \"\"\"\n\n    inner_combinator_closure = switch(*gen_fns)\n\n    def mixture_model(mixture_logits, *args) -&gt; R:\n        mix_idx = categorical(logits=mixture_logits) @ \"mixture_component\"\n        v = inner_combinator_closure(mix_idx, *args) @ \"component_sample\"\n        return v\n\n    return gen(mixture_model)\n</code></pre>"},{"location":"library/combinators.html#genjax.mix(*gen_fns)","title":"<code>*gen_fns</code>","text":""},{"location":"library/core.html","title":"Journey to the center of <code>genjax.core</code>","text":"<p>This page describes the set of core concepts and datatypes in GenJAX, including Gen's generative datatypes and concepts (<code>GenerativeFunction</code>, <code>Trace</code>, <code>ChoiceMap</code>, and <code>EditRequest</code>), the core JAX compatibility datatypes (<code>Pytree</code>, <code>Const</code>, and <code>Closure</code>), as well as functionally inspired <code>Pytree</code> extensions (<code>Mask</code>), and GenJAX's approach to \"static\" (JAX tracing time) typechecking.</p> <p>Traces are data structures which record (execution and inference) data about the invocation of generative functions. Traces are often specialized to a generative function language, to take advantage of data locality, and other representation optimizations. Traces support a trace interface: a set of accessor methods designed to provide convenient manipulation when handling traces in inference algorithms. We document this interface below for the <code>Trace</code> data type.</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction","title":"genjax.core.GenerativeFunction","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p><code>GenerativeFunction</code> is the type of generative functions, the main computational object in Gen.</p> <p>Generative functions are a type of probabilistic program. In terms of their mathematical specification, they come equipped with a few ingredients:</p> <ul> <li>(Distribution over samples) \\(P(\\cdot_t, \\cdot_r; a)\\) - a probability distribution over samples \\(t\\) and untraced randomness \\(r\\), indexed by arguments \\(a\\). This ingredient is involved in all the interfaces and specifies the distribution over samples which the generative function represents.</li> <li>(Family of K/L proposals) \\((K(\\cdot_t, \\cdot_{K_r}; u, t), L(\\cdot_t, \\cdot_{L_r}; u, t)) = \\mathcal{F}(u, t)\\) - a family of pairs of probabilistic programs (referred to as K and L), indexed by <code>EditRequest</code> \\(u\\) and an existing sample \\(t\\). This ingredient supports the <code>edit</code> and <code>importance</code> interface, and is used to specify an SMCP3 move which the generative function must provide in response to an edit request. K and L must satisfy additional properties, described further in <code>edit</code>.</li> <li>(Return value function) \\(f(t, r, a)\\) - a deterministic return value function, which maps samples and untraced randomness to return values.</li> </ul> <p>Generative functions also support a family of <code>Target</code> distributions - a <code>Target</code> distribution is a (possibly unnormalized) distribution, typically induced by inference problems.</p> <ul> <li>\\(\\delta_\\emptyset\\) - the empty target, whose only possible value is the empty sample, with density 1.</li> <li>(Family of targets induced by \\(P\\)) \\(T_P(a, c)\\) - a family of targets indexed by arguments \\(a\\) and constraints (<code>ChoiceMap</code>), created by pairing the distribution over samples \\(P\\) with arguments and constraint.</li> </ul> <p>Generative functions expose computations using these ingredients through the generative function interface (the methods which are documented below).</p> <p>Examples:</p> <p>The interface methods can be used to implement inference algorithms directly - here's a simple example using bootstrap importance sampling directly: <pre><code>import jax\nfrom jax.scipy.special import logsumexp\nimport jax.tree_util as jtu\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax import gen, uniform, flip, categorical\n\n\n@gen\ndef model():\n    p = uniform(0.0, 1.0) @ \"p\"\n    f1 = flip(p) @ \"f1\"\n    f2 = flip(p) @ \"f2\"\n\n\n# Bootstrap importance sampling.\ndef importance_sampling(key, constraint):\n    key, sub_key = jax.random.split(key)\n    sub_keys = jax.random.split(sub_key, 5)\n    tr, log_weights = jax.vmap(model.importance, in_axes=(0, None, None))(\n        sub_keys, constraint, ()\n    )\n    logits = log_weights - logsumexp(log_weights)\n    idx = categorical(logits)(key)\n    return jtu.tree_map(lambda v: v[idx], tr.get_choices())\n\n\nsub_keys = jax.random.split(jax.random.key(0), 50)\nsamples = jax.jit(jax.vmap(importance_sampling, in_axes=(0, None)))(\n    sub_keys, C.kw(f1=True, f2=True)\n)\nprint(samples.render_html())\n</code></pre> </p> <p>Methods:</p> Name Description <code>__abstract_call__</code> <p>Used to support JAX tracing, although this default implementation involves no</p> <code>accumulate</code> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; [c]</code> where</p> <code>assess</code> <p>Return the score and the return value when the generative function is invoked with the provided arguments, and constrained to take the provided sample as the sampled value.</p> <code>contramap</code> <p>Specialized version of <code>genjax.GenerativeFunction.dimap</code> where only the pre-processing function is applied.</p> <code>dimap</code> <p>Returns a new <code>genjax.GenerativeFunction</code> and applies pre- and post-processing functions to its arguments and return value.</p> <code>edit</code> <p>Update a trace in response to an <code>EditRequest</code>, returning a new <code>Trace</code>, an incremental <code>Weight</code> for the new target, a <code>Retdiff</code> return value tagged with change information, and a backward <code>EditRequest</code> which requests the reverse move (to go back to the original trace).</p> <code>get_zero_trace</code> <pre><code>    Returns a trace with zero values for all leaves, generated without executing the generative function.\n</code></pre> <code>handle_kwargs</code> <p>Returns a new GenerativeFunction like <code>self</code>, but where all GFI methods accept a tuple of arguments and a dictionary of keyword arguments.</p> <code>importance</code> <p>Returns a properly weighted pair, a <code>Trace</code> and a <code>Weight</code>, properly weighted for the target induced by the generative function for the provided constraint and arguments.</p> <code>iterate</code> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; [a]</code> where</p> <code>iterate_final</code> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> where</p> <code>map</code> <p>Specialized version of <code>genjax.dimap</code> where only the post-processing function is applied.</p> <code>mask</code> <p>Enables dynamic masking of generative functions. Returns a new <code>genjax.GenerativeFunction</code> like <code>self</code>, but which accepts an additional boolean first argument.</p> <code>masked_iterate</code> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a list of values of type <code>a</code>.</p> <code>masked_iterate_final</code> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a value of type <code>a</code>.</p> <code>mix</code> <p>Takes any number of <code>genjax.GenerativeFunction</code>s and returns a new <code>genjax.GenerativeFunction</code> that represents a mixture model.</p> <code>or_else</code> <p>Returns a <code>GenerativeFunction</code> that accepts</p> <code>propose</code> <p>Samples a <code>ChoiceMap</code> and any untraced randomness \\(r\\) from the generative function's distribution over samples (\\(P\\)), and returns the <code>Score</code> of that sample under the distribution, and the <code>R</code> of the generative function's return value function \\(f(r, t, a)\\) for the sample and untraced randomness.</p> <code>reduce</code> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; c</code> where</p> <code>repeat</code> <p>Returns a <code>genjax.GenerativeFunction</code> that samples from <code>self</code> <code>n</code> times, returning a vector of <code>n</code> results.</p> <code>scan</code> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; (c, b)</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; (c, [b])</code> where</p> <code>simulate</code> <p>Execute the generative function, sampling from its distribution over samples, and return a <code>Trace</code>.</p> <code>switch</code> <p>Given <code>n</code> <code>genjax.GenerativeFunction</code> inputs, returns a new <code>genjax.GenerativeFunction</code> that accepts <code>n+2</code> arguments:</p> <code>vmap</code> <p>Returns a <code>GenerativeFunction</code> that performs a vectorized map over the argument specified by <code>in_axes</code>. Traced values are nested under an index, and the retval is vectorized.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>class GenerativeFunction(Generic[R], Pytree):\n    \"\"\"\n    `GenerativeFunction` is the type of _generative functions_, the main computational object in Gen.\n\n    Generative functions are a type of probabilistic program. In terms of their mathematical specification, they come equipped with a few ingredients:\n\n    * (**Distribution over samples**) $P(\\\\cdot_t, \\\\cdot_r; a)$ - a probability distribution over samples $t$ and untraced randomness $r$, indexed by arguments $a$. This ingredient is involved in all the interfaces and specifies the distribution over samples which the generative function represents.\n    * (**Family of K/L proposals**) $(K(\\\\cdot_t, \\\\cdot_{K_r}; u, t), L(\\\\cdot_t, \\\\cdot_{L_r}; u, t)) = \\\\mathcal{F}(u, t)$ - a family of pairs of probabilistic programs (referred to as K and L), indexed by [`EditRequest`][genjax.core.EditRequest] $u$ and an existing sample $t$. This ingredient supports the [`edit`][genjax.core.GenerativeFunction.edit] and [`importance`][genjax.core.GenerativeFunction.importance] interface, and is used to specify an SMCP3 move which the generative function must provide in response to an edit request. K and L must satisfy additional properties, described further in [`edit`][genjax.core.GenerativeFunction.edit].\n    * (**Return value function**) $f(t, r, a)$ - a deterministic return value function, which maps samples and untraced randomness to return values.\n\n    Generative functions also support a family of [`Target`][genjax.inference.Target] distributions - a [`Target`][genjax.inference.Target] distribution is a (possibly unnormalized) distribution, typically induced by inference problems.\n\n    * $\\\\delta_\\\\emptyset$ - the empty target, whose only possible value is the empty sample, with density 1.\n    * (**Family of targets induced by $P$**) $T_P(a, c)$ - a family of targets indexed by arguments $a$ and constraints (`ChoiceMap`), created by pairing the distribution over samples $P$ with arguments and constraint.\n\n    Generative functions expose computations using these ingredients through the _generative function interface_ (the methods which are documented below).\n\n    Examples:\n        The interface methods can be used to implement inference algorithms directly - here's a simple example using bootstrap importance sampling directly:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        import jax\n        from jax.scipy.special import logsumexp\n        import jax.tree_util as jtu\n        from genjax import ChoiceMapBuilder as C\n        from genjax import gen, uniform, flip, categorical\n\n\n        @gen\n        def model():\n            p = uniform(0.0, 1.0) @ \"p\"\n            f1 = flip(p) @ \"f1\"\n            f2 = flip(p) @ \"f2\"\n\n\n        # Bootstrap importance sampling.\n        def importance_sampling(key, constraint):\n            key, sub_key = jax.random.split(key)\n            sub_keys = jax.random.split(sub_key, 5)\n            tr, log_weights = jax.vmap(model.importance, in_axes=(0, None, None))(\n                sub_keys, constraint, ()\n            )\n            logits = log_weights - logsumexp(log_weights)\n            idx = categorical(logits)(key)\n            return jtu.tree_map(lambda v: v[idx], tr.get_choices())\n\n\n        sub_keys = jax.random.split(jax.random.key(0), 50)\n        samples = jax.jit(jax.vmap(importance_sampling, in_axes=(0, None)))(\n            sub_keys, C.kw(f1=True, f2=True)\n        )\n        print(samples.render_html())\n        ```\n    \"\"\"\n\n    def __call__(self, *args, **kwargs) -&gt; \"GenerativeFunctionClosure[R]\":\n        return GenerativeFunctionClosure(self, args, kwargs)\n\n    def __abstract_call__(self, *args) -&gt; R:\n        \"\"\"Used to support JAX tracing, although this default implementation involves no\n        JAX operations (it takes a fixed-key sample from the return value).\n\n        Generative functions may customize this to improve compilation time.\n        \"\"\"\n        return self.get_zero_trace(*args).get_retval()\n\n    def handle_kwargs(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Returns a new GenerativeFunction like `self`, but where all GFI methods accept a tuple of arguments and a dictionary of keyword arguments.\n\n        The returned GenerativeFunction can be invoked with `__call__` with no special argument handling (just like the original).\n\n        In place of `args` tuples in GFI methods, the new GenerativeFunction expects a 2-tuple containing:\n\n        1. A tuple containing the original positional arguments.\n        2. A dictionary containing the keyword arguments.\n\n        This allows for more flexible argument passing, especially useful in contexts where\n        keyword arguments need to be handled separately or passed through multiple layers.\n\n        Returns:\n            A new GenerativeFunction that accepts (args_tuple, kwargs_dict) for all GFI methods.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            import genjax\n            import jax\n\n\n            @genjax.gen\n            def model(x, y, z=1.0):\n                _ = genjax.normal(x + y, z) @ \"v\"\n                return x + y + z\n\n\n            key = jax.random.key(0)\n            kw_model = model.handle_kwargs()\n\n            tr = kw_model.simulate(key, ((1.0, 2.0), {\"z\": 3.0}))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        return IgnoreKwargs(self)\n\n    def get_zero_trace(self, *args, **_kwargs) -&gt; Trace[R]:\n        \"\"\"\n        Returns a trace with zero values for all leaves, generated without executing the generative function.\n\n        This method is useful for static analysis and shape inference without executing the generative function. It returns a trace with the same structure as a real trace, but filled with zero or default values.\n\n        Args:\n            *args: The arguments to the generative function.\n            **_kwargs: Ignored keyword arguments.\n\n        Returns:\n            A trace with zero values, matching the structure of a real trace.\n\n        Note:\n            This method uses the `empty_trace` utility function, which creates a trace without spending any FLOPs. The resulting trace has the correct structure but contains placeholder zero values.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            @genjax.gen\n            def weather_model():\n                temperature = genjax.normal(20.0, 5.0) @ \"temperature\"\n                is_sunny = genjax.bernoulli(0.7) @ \"is_sunny\"\n                return {\"temperature\": temperature, \"is_sunny\": is_sunny}\n\n\n            zero_trace = weather_model.get_zero_trace()\n            print(\"Zero trace structure:\")\n            print(zero_trace.render_html())\n\n            print(\"\\nActual simulation:\")\n            key = jax.random.key(0)\n            actual_trace = weather_model.simulate(key, ())\n            print(actual_trace.render_html())\n            ```\n        \"\"\"\n        return empty_trace(self, args)\n\n    @abstractmethod\n    def simulate(\n        self,\n        key: PRNGKey,\n        args: Arguments,\n    ) -&gt; Trace[R]:\n        \"\"\"\n        Execute the generative function, sampling from its distribution over samples, and return a [`Trace`][genjax.core.Trace].\n\n        ## More on traces\n\n        The [`Trace`][genjax.core.Trace] returned by `simulate` implements its own interface.\n\n        It is responsible for storing the arguments of the invocation ([`genjax.Trace.get_args`][]), the return value of the generative function ([`genjax.Trace.get_retval`][]), the identity of the generative function which produced the trace ([`genjax.Trace.get_gen_fn`][]), the sample of traced random choices produced during the invocation ([`genjax.Trace.get_choices`][]) and _the score_ of the sample ([`genjax.Trace.get_score`][]).\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            import genjax\n            import jax\n            from jax import vmap, jit\n            from jax.random import split\n\n\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                return x\n\n\n            key = jax.random.key(0)\n            tr = model.simulate(key, ())\n            print(tr.render_html())\n            ```\n\n            Another example, using the same model, composed into [`genjax.repeat`](combinators.md#genjax.repeat) - which creates a new generative function, which has the same interface:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            @genjax.gen\n            def model():\n                x = genjax.normal(0.0, 1.0) @ \"x\"\n                return x\n\n\n            key = jax.random.key(0)\n            tr = model.repeat(n=10).simulate(key, ())\n            print(tr.render_html())\n            ```\n\n            (**Fun, flirty, fast ... parallel?**) Feel free to use `jax.jit` and `jax.vmap`!\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            key = jax.random.key(0)\n            sub_keys = split(key, 10)\n            sim = model.repeat(n=10).simulate\n            tr = jit(vmap(sim, in_axes=(0, None)))(sub_keys, ())\n            print(tr.render_html())\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def assess(\n        self,\n        sample: ChoiceMap,\n        args: Arguments,\n    ) -&gt; tuple[Score, R]:\n        \"\"\"\n        Return [the score][genjax.core.Trace.get_score] and [the return value][genjax.core.Trace.get_retval] when the generative function is invoked with the provided arguments, and constrained to take the provided sample as the sampled value.\n\n        It is an error if the provided sample value is off the support of the distribution over the `ChoiceMap` type, or otherwise induces a partial constraint on the execution of the generative function (which would require the generative function to provide an `edit` implementation which responds to the `EditRequest` induced by the [`importance`][genjax.core.GenerativeFunction.importance] interface).\n\n        Examples:\n            This method is similar to density evaluation interfaces for distributions.\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            from genjax import normal\n            from genjax import ChoiceMapBuilder as C\n\n            sample = C.v(1.0)\n            score, retval = normal.assess(sample, (1.0, 1.0))\n            print((score, retval))\n            ```\n\n            But it also works with generative functions that sample from spaces with more structure:\n\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            from genjax import gen\n            from genjax import normal\n            from genjax import ChoiceMapBuilder as C\n\n\n            @gen\n            def model():\n                v1 = normal(0.0, 1.0) @ \"v1\"\n                v2 = normal(v1, 1.0) @ \"v2\"\n\n\n            sample = C.kw(v1=1.0, v2=0.0)\n            score, retval = model.assess(sample, ())\n            print((score, retval))\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def generate(\n        self,\n        key: PRNGKey,\n        constraint: ChoiceMap,\n        args: Arguments,\n    ) -&gt; tuple[Trace[R], Weight]:\n        pass\n\n    @abstractmethod\n    def project(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        selection: Selection,\n    ) -&gt; Weight:\n        pass\n\n    @abstractmethod\n    def edit(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        edit_request: EditRequest,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], EditRequest]:\n        \"\"\"\n        Update a trace in response to an [`EditRequest`][genjax.core.EditRequest], returning a new [`Trace`][genjax.core.Trace], an incremental [`Weight`][genjax.core.Weight] for the new target, a [`Retdiff`][genjax.core.Retdiff] return value tagged with change information, and a backward [`EditRequest`][genjax.core.EditRequest] which requests the reverse move (to go back to the original trace).\n\n        The specification of this interface is parametric over the kind of `EditRequest` -- responding to an `EditRequest` instance requires that the generative function provides an implementation of a sequential Monte Carlo move in the [SMCP3](https://proceedings.mlr.press/v206/lew23a.html) framework. Users of inference algorithms are not expected to understand the ingredients, but inference algorithm developers are.\n\n        Examples:\n            Updating a trace in response to a request for a [`Target`][genjax.inference.Target] change induced by a change to the arguments:\n            ```python exec=\"yes\" source=\"material-block\" session=\"core\"\n            import jax\n            from genjax import gen, normal, Diff, Update, ChoiceMap as C\n\n            key = jax.random.key(0)\n\n\n            @gen\n            def model(var):\n                v1 = normal(0.0, 1.0) @ \"v1\"\n                v2 = normal(v1, var) @ \"v2\"\n                return v2\n\n\n            # Generating an initial trace properly weighted according\n            # to the target induced by the constraint.\n            constraint = C.kw(v2=1.0)\n            initial_tr, w = model.importance(key, constraint, (1.0,))\n\n            # Updating the trace to a new target.\n            new_tr, inc_w, retdiff, bwd_prob = model.edit(\n                key,\n                initial_tr,\n                Update(\n                    C.empty(),\n                ),\n                Diff.unknown_change((3.0,)),\n            )\n            ```\n\n            Now, let's inspect the trace:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            # Inspect the trace, the sampled values should not have changed!\n            sample = new_tr.get_choices()\n            print(sample[\"v1\"], sample[\"v2\"])\n            ```\n\n            And the return value diff:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            # The return value also should not have changed!\n            print(retdiff.render_html())\n            ```\n\n            As expected, neither have changed -- but the weight is non-zero:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            print(w)\n            ```\n\n        ## Mathematical ingredients behind edit\n\n        The `edit` interface exposes [SMCP3 moves](https://proceedings.mlr.press/v206/lew23a.html). Here, we omit the measure theoretic description, and refer interested readers to [the paper](https://proceedings.mlr.press/v206/lew23a.html). Informally, the ingredients of such a move are:\n\n        * The previous target $T$.\n        * The new target $T'$.\n        * A pair of kernel probabilistic programs, called $K$ and $L$:\n            * The K kernel is a kernel probabilistic program which accepts a previous sample $x_{t-1}$ from $T$ as an argument, may sample auxiliary randomness $u_K$, and returns a new sample $x_t$ approximately distributed according to $T'$, along with transformed randomness $u_L$.\n            * The L kernel is a kernel probabilistic program which accepts the new sample $x_t$, and provides a density evaluator for the auxiliary randomness $u_L$ which K returns, and an inverter $x_t \\\\mapsto x_{t-1}$ which is _almost everywhere_ the identity function.\n\n        The specification of these ingredients are encapsulated in the type signature of the `edit` interface.\n\n        ## Understanding the `edit` interface\n\n        The `edit` interface uses the mathematical ingredients described above to perform probability-aware mutations and incremental [`Weight`][genjax.core.Weight] computations on [`Trace`][genjax.core.Trace] instances, which allows Gen to provide automation to support inference agorithms like importance sampling, SMC, MCMC and many more.\n\n        An `EditRequest` denotes a function $tr \\\\mapsto (T, T')$ from traces to a pair of targets (the previous [`Target`][genjax.inference.Target] $T$, and the final [`Target`][genjax.inference.Target] $T'$).\n\n        Several common types of moves can be requested via the `Update` type:\n\n        ```python exec=\"yes\" source=\"material-block\" session=\"core\"\n        from genjax import Update\n        from genjax import ChoiceMap\n\n        g = Update(\n            ChoiceMap.empty(),  # Constraint\n        )\n        ```\n\n        `Update` contains information about changes to the arguments of the generative function ([`Argdiffs`][genjax.core.Argdiffs]) and a constraint which specifies an additional move to be performed.\n\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        new_tr, inc_w, retdiff, bwd_prob = model.edit(\n            key,\n            initial_tr,\n            Update(\n                C.kw(v1=3.0),\n            ),\n            Diff.unknown_change((3.0,)),\n        )\n        print((new_tr.get_choices()[\"v1\"], w))\n        ```\n\n        **Additional notes on [`Argdiffs`][genjax.core.Argdiffs]**\n\n        Argument changes induce changes to the distribution over samples, internal K and L proposals, and (by virtue of changes to $P$) target distributions. The [`Argdiffs`][genjax.core.Argdiffs] type denotes the type of values attached with a _change type_, a piece of data which indicates how the value has changed from the arguments which created the trace. Generative functions can utilize change type information to inform efficient [`edit`][genjax.core.GenerativeFunction.edit] implementations.\n        \"\"\"\n        pass\n\n    ######################\n    # Derived interfaces #\n    ######################\n\n    def update(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        constraint: ChoiceMap,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], ChoiceMap]:\n        request = Update(\n            constraint,\n        )\n        tr, w, rd, bwd = request.edit(\n            key,\n            trace,\n            argdiffs,\n        )\n        assert isinstance(bwd, Update), type(bwd)\n        return tr, w, rd, bwd.constraint\n\n    def importance(\n        self,\n        key: PRNGKey,\n        constraint: ChoiceMap,\n        args: Arguments,\n    ) -&gt; tuple[Trace[R], Weight]:\n        \"\"\"\n        Returns a properly weighted pair, a [`Trace`][genjax.core.Trace] and a [`Weight`][genjax.core.Weight], properly weighted for the target induced by the generative function for the provided constraint and arguments.\n\n        Examples:\n            (**Full constraints**) A simple example using the `importance` interface on distributions:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            import jax\n            from genjax import normal\n            from genjax import ChoiceMapBuilder as C\n\n            key = jax.random.key(0)\n\n            tr, w = normal.importance(key, C.v(1.0), (0.0, 1.0))\n            print(tr.get_choices().render_html())\n            ```\n\n            (**Internal proposal for partial constraints**) Specifying a _partial_ constraint on a [`StaticGenerativeFunction`][genjax.StaticGenerativeFunction]:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            from genjax import flip, uniform, gen\n            from genjax import ChoiceMapBuilder as C\n\n\n            @gen\n            def model():\n                p = uniform(0.0, 1.0) @ \"p\"\n                f1 = flip(p) @ \"f1\"\n                f2 = flip(p) @ \"f2\"\n\n\n            tr, w = model.importance(key, C.kw(f1=True, f2=True), ())\n            print(tr.get_choices().render_html())\n            ```\n\n        Under the hood, creates an [`EditRequest`][genjax.core.EditRequest] which requests that the generative function respond with a move from the _empty_ trace (the only possible value for _empty_ target $\\\\delta_\\\\emptyset$) to the target induced by the generative function for constraint $C$ with arguments $a$.\n        \"\"\"\n\n        return self.generate(\n            key,\n            constraint,\n            args,\n        )\n\n    def propose(\n        self,\n        key: PRNGKey,\n        args: Arguments,\n    ) -&gt; tuple[ChoiceMap, Score, R]:\n        \"\"\"\n        Samples a [`ChoiceMap`][genjax.core.ChoiceMap] and any untraced randomness $r$ from the generative function's distribution over samples ($P$), and returns the [`Score`][genjax.core.Score] of that sample under the distribution, and the `R` of the generative function's return value function $f(r, t, a)$ for the sample and untraced randomness.\n        \"\"\"\n        tr = self.simulate(key, args)\n        sample = tr.get_choices()\n        score = tr.get_score()\n        retval = tr.get_retval()\n        return sample, score, retval\n\n    ######################################################\n    # Convenience: postfix syntax for combinators / DSLs #\n    ######################################################\n\n    ###############\n    # Combinators #\n    ###############\n\n    # TODO think through, or note, that the R that comes out will have to be bounded by pytree.\n    def vmap(self, /, *, in_axes: InAxes = 0) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Returns a [`GenerativeFunction`][genjax.GenerativeFunction] that performs a vectorized map over the argument specified by `in_axes`. Traced values are nested under an index, and the retval is vectorized.\n\n        Args:\n            in_axes: Selector specifying which input arguments (or index into them) should be vectorized. Defaults to 0, i.e., the first argument. See [this link](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees) for more detail.\n\n        Returns:\n            A new [`GenerativeFunction`][genjax.GenerativeFunction] that accepts an argument of one-higher dimension at the position specified by `in_axes`.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"gen-fn\"\n            import jax\n            import jax.numpy as jnp\n            import genjax\n\n\n            @genjax.gen\n            def model(x):\n                v = genjax.normal(x, 1.0) @ \"v\"\n                return genjax.normal(v, 0.01) @ \"q\"\n\n\n            vmapped = model.vmap(in_axes=0)\n\n            key = jax.random.key(314159)\n            arr = jnp.ones(100)\n\n            # `vmapped` accepts an array if numbers instead of the original\n            # single number that `model` accepted.\n            tr = jax.jit(vmapped.simulate)(key, (arr,))\n\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.vmap(in_axes=in_axes)(self)\n\n    def repeat(self, /, *, n: int) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Returns a [`genjax.GenerativeFunction`][] that samples from `self` `n` times, returning a vector of `n` results.\n\n        The values traced by each call `gen_fn` will be nested under an integer index that matches the loop iteration index that generated it.\n\n        This combinator is useful for creating multiple samples from `self` in a batched manner.\n\n        Args:\n            n: The number of times to sample from the generative function.\n\n        Returns:\n            A new [`genjax.GenerativeFunction`][] that samples from the original function `n` times.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"repeat\"\n            import genjax, jax\n\n\n            @genjax.gen\n            def normal_draw(mean):\n                return genjax.normal(mean, 1.0) @ \"x\"\n\n\n            normal_draws = normal_draw.repeat(n=10)\n\n            key = jax.random.key(314159)\n\n            # Generate 10 draws from a normal distribution with mean 2.0\n            tr = jax.jit(normal_draws.simulate)(key, (2.0,))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.repeat(n=n)(self)\n\n    def scan(\n        self: \"GenerativeFunction[tuple[Carry, Y]]\",\n        /,\n        *,\n        n: int | None = None,\n    ) -&gt; \"GenerativeFunction[tuple[Carry, Y]]\":\n        \"\"\"\n        When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; (c, b)`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; (c, [b])` where\n\n        - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n        - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n        - `b` may be a primitive, an array type or a pytree (container) type with array leaves.\n\n        The values traced by each call to the original generative function will be nested under an integer index that matches the loop iteration index that generated it.\n\n        For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n        When the type of `xs` in the snippet below (denoted `[a]` above) is an array type or None, and the type of `ys` in the snippet below (denoted `[b]` above) is an array type, the semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n        ```python\n        def scan(f, init, xs, length=None):\n            if xs is None:\n                xs = [None] * length\n            carry = init\n            ys = []\n            for x in xs:\n                carry, y = f(carry, x)\n                ys.append(y)\n            return carry, np.stack(ys)\n        ```\n\n        Unlike that Python version, both `xs` and `ys` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays. `None` is actually a special case of this, as it represents an empty pytree.\n\n        The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n        Args:\n            n: optional integer specifying the number of loop iterations, which (if supplied) must agree with the sizes of leading axes of the arrays in the returned function's second argument. If supplied then the returned generative function can take `None` as its second argument.\n\n        Returns:\n            A new [`genjax.GenerativeFunction`][] that takes a loop-carried value and a new input, and returns a new loop-carried value along with either `None` or an output to be collected into the second return value.\n\n        Examples:\n            Scan for 1000 iterations with no array input:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n\n\n            @genjax.gen\n            def random_walk_step(prev, _):\n                x = genjax.normal(prev, 1.0) @ \"x\"\n                return x, None\n\n\n            random_walk = random_walk_step.scan(n=1000)\n\n            init = 0.5\n            key = jax.random.key(314159)\n\n            tr = jax.jit(random_walk.simulate)(key, (init, None))\n            print(tr.render_html())\n            ```\n\n            Scan across an input array:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax.numpy as jnp\n\n\n            @genjax.gen\n            def add_and_square_step(sum, x):\n                new_sum = sum + x\n                return new_sum, sum * sum\n\n\n            # notice no `n` parameter supplied:\n            add_and_square_all = add_and_square_step.scan()\n            init = 0.0\n            xs = jnp.ones(10)\n\n            tr = jax.jit(add_and_square_all.simulate)(key, (init, xs))\n\n            # The retval has the final carry and an array of all `sum*sum` returned.\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.scan(n=n)(self)\n\n    def accumulate(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; c`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; [c]` where\n\n        - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n        - `[c]` is an array of all loop-carried values seen during iteration (including the first)\n        - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n        All traced values are nested under an index.\n\n        For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n        The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`itertools.accumulate`](https://docs.python.org/3/library/itertools.html#itertools.accumulate)):\n\n        ```python\n        def accumulate(f, init, xs):\n            carry = init\n            carries = [init]\n            for x in xs:\n                carry = f(carry, x)\n                carries.append(carry)\n            return carries\n        ```\n\n        Unlike that Python version, both `xs` and `carries` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n        The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n            import jax.numpy as jnp\n\n\n            @genjax.accumulate()\n            @genjax.gen\n            def add(sum, x):\n                new_sum = sum + x\n                return new_sum\n\n\n            init = 0.0\n            key = jax.random.key(314159)\n            xs = jnp.ones(10)\n\n            tr = jax.jit(add.simulate)(key, (init, xs))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.accumulate()(self)\n\n    def reduce(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; c`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; c` where\n\n        - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n        - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n        All traced values are nested under an index.\n\n        For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n        The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`functools.reduce`](https://docs.python.org/3/library/itertools.html#functools.reduce)):\n\n        ```python\n        def reduce(f, init, xs):\n            carry = init\n            for x in xs:\n                carry = f(carry, x)\n            return carry\n        ```\n\n        Unlike that Python version, both `xs` and `carry` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n        The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n        Examples:\n            sum an array of numbers:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n            import jax.numpy as jnp\n\n\n            @genjax.reduce()\n            @genjax.gen\n            def add(sum, x):\n                new_sum = sum + x\n                return new_sum\n\n\n            init = 0.0\n            key = jax.random.key(314159)\n            xs = jnp.ones(10)\n\n            tr = jax.jit(add.simulate)(key, (init, xs))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.reduce()(self)\n\n    def iterate(\n        self,\n        /,\n        *,\n        n: int,\n    ) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        When called on a [`genjax.GenerativeFunction`][] of type `a -&gt; a`, returns a new [`genjax.GenerativeFunction`][] of type `a -&gt; [a]` where\n\n        - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n        - `[a]` is an array of all `a`, `f(a)`, `f(f(a))` etc. values seen during iteration.\n\n        All traced values are nested under an index.\n\n        The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n        ```python\n        def iterate(f, n, init):\n            input = init\n            seen = [init]\n            for _ in range(n):\n                input = f(input)\n                seen.append(input)\n            return seen\n        ```\n\n        `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n        The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n        Args:\n            n: the number of iterations to run.\n\n        Examples:\n            iterative addition, returning all intermediate sums:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n\n\n            @genjax.iterate(n=100)\n            @genjax.gen\n            def inc(x):\n                return x + 1\n\n\n            init = 0.0\n            key = jax.random.key(314159)\n\n            tr = jax.jit(inc.simulate)(key, (init,))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.iterate(n=n)(self)\n\n    def iterate_final(\n        self,\n        /,\n        *,\n        n: int,\n    ) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type `a -&gt; a` and returns a new [`genjax.GenerativeFunction`][] of type `a -&gt; a` where\n\n        - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n        - the original function is invoked `n` times with each input coming from the previous invocation's output, so that the new function returns $f^n(a)$\n\n        All traced values are nested under an index.\n\n        The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n        ```python\n        def iterate_final(f, n, init):\n            ret = init\n            for _ in range(n):\n                ret = f(ret)\n            return ret\n        ```\n\n        `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n        The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n        Args:\n            n: the number of iterations to run.\n\n        Examples:\n            iterative addition:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n\n\n            @genjax.iterate_final(n=100)\n            @genjax.gen\n            def inc(x):\n                return x + 1\n\n\n            init = 0.0\n            key = jax.random.key(314159)\n\n            tr = jax.jit(inc.simulate)(key, (init,))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.iterate_final(n=n)(self)\n\n    def masked_iterate(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a list of values of type `a`.\n\n        The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns a Masked list of results of the original operation with the input [mask] as mask.\n\n        All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n\n            masks = jnp.array([True, False, True])\n\n\n            # Create a kernel generative function\n            @genjax.gen\n            def step(x):\n                _ = (\n                    genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                    @ \"rats\"\n                )\n                return x\n\n\n            # Create a model using masked_iterate\n            model = step.masked_iterate()\n\n            # Simulate from the model\n            key = jax.random.key(0)\n            mask_steps = jnp.arange(10) &lt; 5\n            tr = model.simulate(key, (0.0, mask_steps))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.masked_iterate()(self)\n\n    def masked_iterate_final(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a value of type `a`.\n\n        The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns the result of the original operation if `mask` is `True`, and the original input if `mask` is `False`.\n\n        All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n            import jax\n            import genjax\n\n            masks = jnp.array([True, False, True])\n\n\n            # Create a kernel generative function\n            @genjax.gen\n            def step(x):\n                _ = (\n                    genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                    @ \"rats\"\n                )\n                return x\n\n\n            # Create a model using masked_iterate_final\n            model = step.masked_iterate_final()\n\n            # Simulate from the model\n            key = jax.random.key(0)\n            mask_steps = jnp.arange(10) &lt; 5\n            tr = model.simulate(key, (0.0, mask_steps))\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.masked_iterate_final()(self)\n\n    def mask(self, /) -&gt; \"GenerativeFunction[genjax.Mask[R]]\":\n        \"\"\"\n        Enables dynamic masking of generative functions. Returns a new [`genjax.GenerativeFunction`][] like `self`, but which accepts an additional boolean first argument.\n\n        If `True`, the invocation of `self` is masked, and its contribution to the score is ignored. If `False`, it has the same semantics as if one was invoking `self` without masking.\n\n        The return value type is a `Mask`, with a flag value equal to the supplied boolean.\n\n        Returns:\n            The masked version of the original [`genjax.GenerativeFunction`][].\n\n        Examples:\n            Masking a normal draw:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mask\"\n            import genjax, jax\n\n\n            @genjax.gen\n            def normal_draw(mean):\n                return genjax.normal(mean, 1.0) @ \"x\"\n\n\n            masked_normal_draw = normal_draw.mask()\n\n            key = jax.random.key(314159)\n            tr = jax.jit(masked_normal_draw.simulate)(\n                key,\n                (\n                    False,\n                    2.0,\n                ),\n            )\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.mask(self)\n\n    def or_else(self, gen_fn: \"GenerativeFunction[R]\", /) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Returns a [`GenerativeFunction`][genjax.GenerativeFunction] that accepts\n\n        - a boolean argument\n        - an argument tuple for `self`\n        - an argument tuple for the supplied `gen_fn`\n\n        and acts like `self` when the boolean is `True` or like `gen_fn` otherwise.\n\n        Args:\n            gen_fn: called when the boolean argument is `False`.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"gen-fn\"\n            import jax\n            import jax.numpy as jnp\n            import genjax\n\n\n            @genjax.gen\n            def if_model(x):\n                return genjax.normal(x, 1.0) @ \"if_value\"\n\n\n            @genjax.gen\n            def else_model(x):\n                return genjax.normal(x, 5.0) @ \"else_value\"\n\n\n            @genjax.gen\n            def model(toss: bool):\n                # Note that the returned model takes a new boolean predicate in\n                # addition to argument tuples for each branch.\n                return if_model.or_else(else_model)(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\n            key = jax.random.key(314159)\n\n            tr = jax.jit(model.simulate)(key, (True,))\n\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.or_else(self, gen_fn)\n\n    def switch(self, *branches: \"GenerativeFunction[R]\") -&gt; \"genjax.Switch[R]\":\n        \"\"\"\n        Given `n` [`genjax.GenerativeFunction`][] inputs, returns a new [`genjax.GenerativeFunction`][] that accepts `n+2` arguments:\n\n        - an index in the range $[0, n+1)$\n        - a tuple of arguments for `self` and each of the input generative functions (`n+1` total tuples)\n\n        and executes the generative function at the supplied index with its provided arguments.\n\n        If `index` is out of bounds, `index` is clamped to within bounds.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"switch\"\n            import jax, genjax\n\n\n            @genjax.gen\n            def branch_1():\n                x = genjax.normal(0.0, 1.0) @ \"x1\"\n\n\n            @genjax.gen\n            def branch_2():\n                x = genjax.bernoulli(0.3) @ \"x2\"\n\n\n            switch = branch_1.switch(branch_2)\n\n            key = jax.random.key(314159)\n            jitted = jax.jit(switch.simulate)\n\n            # Select `branch_2` by providing 1:\n            tr = jitted(key, (1, (), ()))\n\n            print(tr.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.switch(self, *branches)\n\n    def mix(self, *fns: \"GenerativeFunction[R]\") -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Takes any number of [`genjax.GenerativeFunction`][]s and returns a new [`genjax.GenerativeFunction`][] that represents a mixture model.\n\n        The returned generative function takes the following arguments:\n\n        - `mixture_logits`: Logits for the categorical distribution used to select a component.\n        - `*args`: Argument tuples for `self` and each of the input generative functions\n\n        and samples from `self` or one of the input generative functions based on a draw from a categorical distribution defined by the provided mixture logits.\n\n        Args:\n            *fns: Variable number of [`genjax.GenerativeFunction`][]s to be mixed with `self`.\n\n        Returns:\n            A new [`genjax.GenerativeFunction`][] representing the mixture model.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mix\"\n            import jax\n            import genjax\n\n\n            # Define component generative functions\n            @genjax.gen\n            def component1(x):\n                return genjax.normal(x, 1.0) @ \"y\"\n\n\n            @genjax.gen\n            def component2(x):\n                return genjax.normal(x, 2.0) @ \"y\"\n\n\n            # Create mixture model\n            mixture = component1.mix(component2)\n\n            # Use the mixture model\n            key = jax.random.key(0)\n            logits = jax.numpy.array([0.3, 0.7])  # Favors component2\n            trace = mixture.simulate(key, (logits, (0.0,), (7.0,)))\n            print(trace.render_html())\n                ```\n        \"\"\"\n        import genjax\n\n        return genjax.mix(self, *fns)\n\n    def dimap(\n        self,\n        /,\n        *,\n        pre: Callable[..., ArgTuple],\n        post: Callable[[tuple[Any, ...], ArgTuple, R], S],\n    ) -&gt; \"GenerativeFunction[S]\":\n        \"\"\"\n        Returns a new [`genjax.GenerativeFunction`][] and applies pre- and post-processing functions to its arguments and return value.\n\n        !!! info\n            Prefer [`genjax.GenerativeFunction.map`][] if you only need to transform the return value, or [`genjax.GenerativeFunction.contramap`][] if you only need to transform the arguments.\n\n        Args:\n            pre: A callable that preprocesses the arguments before passing them to the wrapped function. Note that `pre` must return a _tuple_ of arguments, not a bare argument. Default is the identity function.\n            post: A callable that postprocesses the return value of the wrapped function. Default is the identity function.\n\n        Returns:\n            A new [`genjax.GenerativeFunction`][] with `pre` and `post` applied.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"dimap\"\n            import jax, genjax\n\n\n            # Define pre- and post-processing functions\n            def pre_process(x, y):\n                return (x + 1, y * 2)\n\n\n            def post_process(args, xformed, retval):\n                return retval**2\n\n\n            @genjax.gen\n            def model(x, y):\n                return genjax.normal(x, y) @ \"z\"\n\n\n            dimap_model = model.dimap(pre=pre_process, post=post_process)\n\n            # Use the dimap model\n            key = jax.random.key(0)\n            trace = dimap_model.simulate(key, (2.0, 3.0))\n\n            print(trace.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.dimap(pre=pre, post=post)(self)\n\n    def map(self, f: Callable[[R], S]) -&gt; \"GenerativeFunction[S]\":\n        \"\"\"\n        Specialized version of [`genjax.dimap`][] where only the post-processing function is applied.\n\n        Args:\n            f: A callable that postprocesses the return value of the wrapped function.\n\n        Returns:\n            A [`genjax.GenerativeFunction`][] that acts like `self` with a post-processing function to its return value.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"map\"\n            import jax, genjax\n\n\n            # Define a post-processing function\n            def square(x):\n                return x**2\n\n\n            @genjax.gen\n            def model(x):\n                return genjax.normal(x, 1.0) @ \"z\"\n\n\n            map_model = model.map(square)\n\n            # Use the map model\n            key = jax.random.key(0)\n            trace = map_model.simulate(key, (2.0,))\n\n            print(trace.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.map(f=f)(self)\n\n    def contramap(self, f: Callable[..., ArgTuple]) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"\n        Specialized version of [`genjax.GenerativeFunction.dimap`][] where only the pre-processing function is applied.\n\n        Args:\n            f: A callable that preprocesses the arguments of the wrapped function. Note that `f` must return a _tuple_ of arguments, not a bare argument.\n\n        Returns:\n            A [`genjax.GenerativeFunction`][] that acts like `self` with a pre-processing function to its arguments.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"contramap\"\n            import jax, genjax\n\n\n            # Define a pre-processing function.\n            # Note that this function must return a tuple of arguments!\n            def add_one(x):\n                return (x + 1,)\n\n\n            @genjax.gen\n            def model(x):\n                return genjax.normal(x, 1.0) @ \"z\"\n\n\n            contramap_model = model.contramap(add_one)\n\n            # Use the contramap model\n            key = jax.random.key(0)\n            trace = contramap_model.simulate(key, (2.0,))\n\n            print(trace.render_html())\n            ```\n        \"\"\"\n        import genjax\n\n        return genjax.contramap(f=f)(self)\n\n    #####################\n    # GenSP / inference #\n    #####################\n\n    def marginal(\n        self,\n        /,\n        *,\n        selection: Any | None = None,\n        algorithm: Any | None = None,\n    ) -&gt; \"genjax.Marginal[R]\":\n        from genjax import Selection, marginal\n\n        if selection is None:\n            selection = Selection.all()\n\n        return marginal(selection=selection, algorithm=algorithm)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.__abstract_call__","title":"__abstract_call__","text":"<pre><code>__abstract_call__(*args) -&gt; R\n</code></pre> <p>Used to support JAX tracing, although this default implementation involves no JAX operations (it takes a fixed-key sample from the return value).</p> <p>Generative functions may customize this to improve compilation time.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def __abstract_call__(self, *args) -&gt; R:\n    \"\"\"Used to support JAX tracing, although this default implementation involves no\n    JAX operations (it takes a fixed-key sample from the return value).\n\n    Generative functions may customize this to improve compilation time.\n    \"\"\"\n    return self.get_zero_trace(*args).get_retval()\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.accumulate","title":"accumulate","text":"<pre><code>accumulate() -&gt; GenerativeFunction[R]\n</code></pre> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; [c]</code> where</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>[c]</code> is an array of all loop-carried values seen during iteration (including the first)</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> </ul> <p>All traced values are nested under an index.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation (note the similarity to <code>itertools.accumulate</code>):</p> <pre><code>def accumulate(f, init, xs):\n    carry = init\n    carries = [init]\n    for x in xs:\n        carry = f(carry, x)\n        carries.append(carry)\n    return carries\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>carries</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Examples:</p> <pre><code>import jax\nimport genjax\nimport jax.numpy as jnp\n\n\n@genjax.accumulate()\n@genjax.gen\ndef add(sum, x):\n    new_sum = sum + x\n    return new_sum\n\n\ninit = 0.0\nkey = jax.random.key(314159)\nxs = jnp.ones(10)\n\ntr = jax.jit(add.simulate)(key, (init, xs))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def accumulate(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; c`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; [c]` where\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `[c]` is an array of all loop-carried values seen during iteration (including the first)\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n    All traced values are nested under an index.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`itertools.accumulate`](https://docs.python.org/3/library/itertools.html#itertools.accumulate)):\n\n    ```python\n    def accumulate(f, init, xs):\n        carry = init\n        carries = [init]\n        for x in xs:\n            carry = f(carry, x)\n            carries.append(carry)\n        return carries\n    ```\n\n    Unlike that Python version, both `xs` and `carries` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n        import jax.numpy as jnp\n\n\n        @genjax.accumulate()\n        @genjax.gen\n        def add(sum, x):\n            new_sum = sum + x\n            return new_sum\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add.simulate)(key, (init, xs))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.accumulate()(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.assess","title":"assess  <code>abstractmethod</code>","text":"<pre><code>assess(\n    sample: ChoiceMap, args: Arguments\n) -&gt; tuple[Score, R]\n</code></pre> <p>Return the score and the return value when the generative function is invoked with the provided arguments, and constrained to take the provided sample as the sampled value.</p> <p>It is an error if the provided sample value is off the support of the distribution over the <code>ChoiceMap</code> type, or otherwise induces a partial constraint on the execution of the generative function (which would require the generative function to provide an <code>edit</code> implementation which responds to the <code>EditRequest</code> induced by the <code>importance</code> interface).</p> <p>Examples:</p> <p>This method is similar to density evaluation interfaces for distributions. <pre><code>from genjax import normal\nfrom genjax import ChoiceMapBuilder as C\n\nsample = C.v(1.0)\nscore, retval = normal.assess(sample, (1.0, 1.0))\nprint((score, retval))\n</code></pre>  (Array(-0.9189385, dtype=float32), 1.0)  </p> <p>But it also works with generative functions that sample from spaces with more structure:</p> <pre><code>from genjax import gen\nfrom genjax import normal\nfrom genjax import ChoiceMapBuilder as C\n\n\n@gen\ndef model():\n    v1 = normal(0.0, 1.0) @ \"v1\"\n    v2 = normal(v1, 1.0) @ \"v2\"\n\n\nsample = C.kw(v1=1.0, v2=0.0)\nscore, retval = model.assess(sample, ())\nprint((score, retval))\n</code></pre>  (Array(-2.837877, dtype=float32), None)   Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef assess(\n    self,\n    sample: ChoiceMap,\n    args: Arguments,\n) -&gt; tuple[Score, R]:\n    \"\"\"\n    Return [the score][genjax.core.Trace.get_score] and [the return value][genjax.core.Trace.get_retval] when the generative function is invoked with the provided arguments, and constrained to take the provided sample as the sampled value.\n\n    It is an error if the provided sample value is off the support of the distribution over the `ChoiceMap` type, or otherwise induces a partial constraint on the execution of the generative function (which would require the generative function to provide an `edit` implementation which responds to the `EditRequest` induced by the [`importance`][genjax.core.GenerativeFunction.importance] interface).\n\n    Examples:\n        This method is similar to density evaluation interfaces for distributions.\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import normal\n        from genjax import ChoiceMapBuilder as C\n\n        sample = C.v(1.0)\n        score, retval = normal.assess(sample, (1.0, 1.0))\n        print((score, retval))\n        ```\n\n        But it also works with generative functions that sample from spaces with more structure:\n\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import gen\n        from genjax import normal\n        from genjax import ChoiceMapBuilder as C\n\n\n        @gen\n        def model():\n            v1 = normal(0.0, 1.0) @ \"v1\"\n            v2 = normal(v1, 1.0) @ \"v2\"\n\n\n        sample = C.kw(v1=1.0, v2=0.0)\n        score, retval = model.assess(sample, ())\n        print((score, retval))\n        ```\n    \"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.contramap","title":"contramap","text":"<pre><code>contramap(\n    f: Callable[..., ArgTuple]\n) -&gt; GenerativeFunction[R]\n</code></pre> <p>Specialized version of <code>genjax.GenerativeFunction.dimap</code> where only the pre-processing function is applied.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[..., ArgTuple]</code> <p>A callable that preprocesses the arguments of the wrapped function. Note that <code>f</code> must return a tuple of arguments, not a bare argument.</p> required <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A <code>genjax.GenerativeFunction</code> that acts like <code>self</code> with a pre-processing function to its arguments.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define a pre-processing function.\n# Note that this function must return a tuple of arguments!\ndef add_one(x):\n    return (x + 1,)\n\n\n@genjax.gen\ndef model(x):\n    return genjax.normal(x, 1.0) @ \"z\"\n\n\ncontramap_model = model.contramap(add_one)\n\n# Use the contramap model\nkey = jax.random.key(0)\ntrace = contramap_model.simulate(key, (2.0,))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def contramap(self, f: Callable[..., ArgTuple]) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Specialized version of [`genjax.GenerativeFunction.dimap`][] where only the pre-processing function is applied.\n\n    Args:\n        f: A callable that preprocesses the arguments of the wrapped function. Note that `f` must return a _tuple_ of arguments, not a bare argument.\n\n    Returns:\n        A [`genjax.GenerativeFunction`][] that acts like `self` with a pre-processing function to its arguments.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"contramap\"\n        import jax, genjax\n\n\n        # Define a pre-processing function.\n        # Note that this function must return a tuple of arguments!\n        def add_one(x):\n            return (x + 1,)\n\n\n        @genjax.gen\n        def model(x):\n            return genjax.normal(x, 1.0) @ \"z\"\n\n\n        contramap_model = model.contramap(add_one)\n\n        # Use the contramap model\n        key = jax.random.key(0)\n        trace = contramap_model.simulate(key, (2.0,))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.contramap(f=f)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.contramap(f)","title":"<code>f</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.dimap","title":"dimap","text":"<pre><code>dimap(\n    *,\n    pre: Callable[..., ArgTuple],\n    post: Callable[[tuple[Any, ...], ArgTuple, R], S]\n) -&gt; GenerativeFunction[S]\n</code></pre> <p>Returns a new <code>genjax.GenerativeFunction</code> and applies pre- and post-processing functions to its arguments and return value.</p> <p>Info</p> <p>Prefer <code>genjax.GenerativeFunction.map</code> if you only need to transform the return value, or <code>genjax.GenerativeFunction.contramap</code> if you only need to transform the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[..., ArgTuple]</code> <p>A callable that preprocesses the arguments before passing them to the wrapped function. Note that <code>pre</code> must return a tuple of arguments, not a bare argument. Default is the identity function.</p> required <code>Callable[[tuple[Any, ...], ArgTuple, R], S]</code> <p>A callable that postprocesses the return value of the wrapped function. Default is the identity function.</p> required <p>Returns:</p> Type Description <code>GenerativeFunction[S]</code> <p>A new <code>genjax.GenerativeFunction</code> with <code>pre</code> and <code>post</code> applied.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define pre- and post-processing functions\ndef pre_process(x, y):\n    return (x + 1, y * 2)\n\n\ndef post_process(args, xformed, retval):\n    return retval**2\n\n\n@genjax.gen\ndef model(x, y):\n    return genjax.normal(x, y) @ \"z\"\n\n\ndimap_model = model.dimap(pre=pre_process, post=post_process)\n\n# Use the dimap model\nkey = jax.random.key(0)\ntrace = dimap_model.simulate(key, (2.0, 3.0))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def dimap(\n    self,\n    /,\n    *,\n    pre: Callable[..., ArgTuple],\n    post: Callable[[tuple[Any, ...], ArgTuple, R], S],\n) -&gt; \"GenerativeFunction[S]\":\n    \"\"\"\n    Returns a new [`genjax.GenerativeFunction`][] and applies pre- and post-processing functions to its arguments and return value.\n\n    !!! info\n        Prefer [`genjax.GenerativeFunction.map`][] if you only need to transform the return value, or [`genjax.GenerativeFunction.contramap`][] if you only need to transform the arguments.\n\n    Args:\n        pre: A callable that preprocesses the arguments before passing them to the wrapped function. Note that `pre` must return a _tuple_ of arguments, not a bare argument. Default is the identity function.\n        post: A callable that postprocesses the return value of the wrapped function. Default is the identity function.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] with `pre` and `post` applied.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"dimap\"\n        import jax, genjax\n\n\n        # Define pre- and post-processing functions\n        def pre_process(x, y):\n            return (x + 1, y * 2)\n\n\n        def post_process(args, xformed, retval):\n            return retval**2\n\n\n        @genjax.gen\n        def model(x, y):\n            return genjax.normal(x, y) @ \"z\"\n\n\n        dimap_model = model.dimap(pre=pre_process, post=post_process)\n\n        # Use the dimap model\n        key = jax.random.key(0)\n        trace = dimap_model.simulate(key, (2.0, 3.0))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.dimap(pre=pre, post=post)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.dimap(pre)","title":"<code>pre</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.dimap(post)","title":"<code>post</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.edit","title":"edit  <code>abstractmethod</code>","text":"<pre><code>edit(\n    key: PRNGKey,\n    trace: Trace[R],\n    edit_request: EditRequest,\n    argdiffs: Argdiffs,\n) -&gt; tuple[Trace[R], Weight, Retdiff[R], EditRequest]\n</code></pre> <p>Update a trace in response to an <code>EditRequest</code>, returning a new <code>Trace</code>, an incremental <code>Weight</code> for the new target, a <code>Retdiff</code> return value tagged with change information, and a backward <code>EditRequest</code> which requests the reverse move (to go back to the original trace).</p> <p>The specification of this interface is parametric over the kind of <code>EditRequest</code> -- responding to an <code>EditRequest</code> instance requires that the generative function provides an implementation of a sequential Monte Carlo move in the SMCP3 framework. Users of inference algorithms are not expected to understand the ingredients, but inference algorithm developers are.</p> <p>Examples:</p> <p>Updating a trace in response to a request for a <code>Target</code> change induced by a change to the arguments: <pre><code>import jax\nfrom genjax import gen, normal, Diff, Update, ChoiceMap as C\n\nkey = jax.random.key(0)\n\n\n@gen\ndef model(var):\n    v1 = normal(0.0, 1.0) @ \"v1\"\n    v2 = normal(v1, var) @ \"v2\"\n    return v2\n\n\n# Generating an initial trace properly weighted according\n# to the target induced by the constraint.\nconstraint = C.kw(v2=1.0)\ninitial_tr, w = model.importance(key, constraint, (1.0,))\n\n# Updating the trace to a new target.\nnew_tr, inc_w, retdiff, bwd_prob = model.edit(\n    key,\n    initial_tr,\n    Update(\n        C.empty(),\n    ),\n    Diff.unknown_change((3.0,)),\n)\n</code></pre> </p> <p>Now, let's inspect the trace: <pre><code># Inspect the trace, the sampled values should not have changed!\nsample = new_tr.get_choices()\nprint(sample[\"v1\"], sample[\"v2\"])\n</code></pre>  -2.4424558 1.0  </p> <p>And the return value diff: <pre><code># The return value also should not have changed!\nprint(retdiff.render_html())\n</code></pre> </p> <p>As expected, neither have changed -- but the weight is non-zero: <pre><code>print(w)\n</code></pre>  -6.8441896  </p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.edit--mathematical-ingredients-behind-edit","title":"Mathematical ingredients behind edit","text":"<p>The <code>edit</code> interface exposes SMCP3 moves. Here, we omit the measure theoretic description, and refer interested readers to the paper. Informally, the ingredients of such a move are:</p> <ul> <li>The previous target \\(T\\).</li> <li>The new target \\(T'\\).</li> <li>A pair of kernel probabilistic programs, called \\(K\\) and \\(L\\):<ul> <li>The K kernel is a kernel probabilistic program which accepts a previous sample \\(x_{t-1}\\) from \\(T\\) as an argument, may sample auxiliary randomness \\(u_K\\), and returns a new sample \\(x_t\\) approximately distributed according to \\(T'\\), along with transformed randomness \\(u_L\\).</li> <li>The L kernel is a kernel probabilistic program which accepts the new sample \\(x_t\\), and provides a density evaluator for the auxiliary randomness \\(u_L\\) which K returns, and an inverter \\(x_t \\mapsto x_{t-1}\\) which is almost everywhere the identity function.</li> </ul> </li> </ul> <p>The specification of these ingredients are encapsulated in the type signature of the <code>edit</code> interface.</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.edit--understanding-the-edit-interface","title":"Understanding the <code>edit</code> interface","text":"<p>The <code>edit</code> interface uses the mathematical ingredients described above to perform probability-aware mutations and incremental <code>Weight</code> computations on <code>Trace</code> instances, which allows Gen to provide automation to support inference agorithms like importance sampling, SMC, MCMC and many more.</p> <p>An <code>EditRequest</code> denotes a function \\(tr \\mapsto (T, T')\\) from traces to a pair of targets (the previous <code>Target</code> \\(T\\), and the final <code>Target</code> \\(T'\\)).</p> <p>Several common types of moves can be requested via the <code>Update</code> type:</p> <pre><code>from genjax import Update\nfrom genjax import ChoiceMap\n\ng = Update(\n    ChoiceMap.empty(),  # Constraint\n)\n</code></pre> <p><code>Update</code> contains information about changes to the arguments of the generative function (<code>Argdiffs</code>) and a constraint which specifies an additional move to be performed.</p> <pre><code>new_tr, inc_w, retdiff, bwd_prob = model.edit(\n    key,\n    initial_tr,\n    Update(\n        C.kw(v1=3.0),\n    ),\n    Diff.unknown_change((3.0,)),\n)\nprint((new_tr.get_choices()[\"v1\"], w))\n</code></pre>  (3.0, Array(-6.8441896, dtype=float32))   <p>Additional notes on <code>Argdiffs</code></p> <p>Argument changes induce changes to the distribution over samples, internal K and L proposals, and (by virtue of changes to \\(P\\)) target distributions. The <code>Argdiffs</code> type denotes the type of values attached with a change type, a piece of data which indicates how the value has changed from the arguments which created the trace. Generative functions can utilize change type information to inform efficient <code>edit</code> implementations.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef edit(\n    self,\n    key: PRNGKey,\n    trace: Trace[R],\n    edit_request: EditRequest,\n    argdiffs: Argdiffs,\n) -&gt; tuple[Trace[R], Weight, Retdiff[R], EditRequest]:\n    \"\"\"\n    Update a trace in response to an [`EditRequest`][genjax.core.EditRequest], returning a new [`Trace`][genjax.core.Trace], an incremental [`Weight`][genjax.core.Weight] for the new target, a [`Retdiff`][genjax.core.Retdiff] return value tagged with change information, and a backward [`EditRequest`][genjax.core.EditRequest] which requests the reverse move (to go back to the original trace).\n\n    The specification of this interface is parametric over the kind of `EditRequest` -- responding to an `EditRequest` instance requires that the generative function provides an implementation of a sequential Monte Carlo move in the [SMCP3](https://proceedings.mlr.press/v206/lew23a.html) framework. Users of inference algorithms are not expected to understand the ingredients, but inference algorithm developers are.\n\n    Examples:\n        Updating a trace in response to a request for a [`Target`][genjax.inference.Target] change induced by a change to the arguments:\n        ```python exec=\"yes\" source=\"material-block\" session=\"core\"\n        import jax\n        from genjax import gen, normal, Diff, Update, ChoiceMap as C\n\n        key = jax.random.key(0)\n\n\n        @gen\n        def model(var):\n            v1 = normal(0.0, 1.0) @ \"v1\"\n            v2 = normal(v1, var) @ \"v2\"\n            return v2\n\n\n        # Generating an initial trace properly weighted according\n        # to the target induced by the constraint.\n        constraint = C.kw(v2=1.0)\n        initial_tr, w = model.importance(key, constraint, (1.0,))\n\n        # Updating the trace to a new target.\n        new_tr, inc_w, retdiff, bwd_prob = model.edit(\n            key,\n            initial_tr,\n            Update(\n                C.empty(),\n            ),\n            Diff.unknown_change((3.0,)),\n        )\n        ```\n\n        Now, let's inspect the trace:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        # Inspect the trace, the sampled values should not have changed!\n        sample = new_tr.get_choices()\n        print(sample[\"v1\"], sample[\"v2\"])\n        ```\n\n        And the return value diff:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        # The return value also should not have changed!\n        print(retdiff.render_html())\n        ```\n\n        As expected, neither have changed -- but the weight is non-zero:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        print(w)\n        ```\n\n    ## Mathematical ingredients behind edit\n\n    The `edit` interface exposes [SMCP3 moves](https://proceedings.mlr.press/v206/lew23a.html). Here, we omit the measure theoretic description, and refer interested readers to [the paper](https://proceedings.mlr.press/v206/lew23a.html). Informally, the ingredients of such a move are:\n\n    * The previous target $T$.\n    * The new target $T'$.\n    * A pair of kernel probabilistic programs, called $K$ and $L$:\n        * The K kernel is a kernel probabilistic program which accepts a previous sample $x_{t-1}$ from $T$ as an argument, may sample auxiliary randomness $u_K$, and returns a new sample $x_t$ approximately distributed according to $T'$, along with transformed randomness $u_L$.\n        * The L kernel is a kernel probabilistic program which accepts the new sample $x_t$, and provides a density evaluator for the auxiliary randomness $u_L$ which K returns, and an inverter $x_t \\\\mapsto x_{t-1}$ which is _almost everywhere_ the identity function.\n\n    The specification of these ingredients are encapsulated in the type signature of the `edit` interface.\n\n    ## Understanding the `edit` interface\n\n    The `edit` interface uses the mathematical ingredients described above to perform probability-aware mutations and incremental [`Weight`][genjax.core.Weight] computations on [`Trace`][genjax.core.Trace] instances, which allows Gen to provide automation to support inference agorithms like importance sampling, SMC, MCMC and many more.\n\n    An `EditRequest` denotes a function $tr \\\\mapsto (T, T')$ from traces to a pair of targets (the previous [`Target`][genjax.inference.Target] $T$, and the final [`Target`][genjax.inference.Target] $T'$).\n\n    Several common types of moves can be requested via the `Update` type:\n\n    ```python exec=\"yes\" source=\"material-block\" session=\"core\"\n    from genjax import Update\n    from genjax import ChoiceMap\n\n    g = Update(\n        ChoiceMap.empty(),  # Constraint\n    )\n    ```\n\n    `Update` contains information about changes to the arguments of the generative function ([`Argdiffs`][genjax.core.Argdiffs]) and a constraint which specifies an additional move to be performed.\n\n    ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n    new_tr, inc_w, retdiff, bwd_prob = model.edit(\n        key,\n        initial_tr,\n        Update(\n            C.kw(v1=3.0),\n        ),\n        Diff.unknown_change((3.0,)),\n    )\n    print((new_tr.get_choices()[\"v1\"], w))\n    ```\n\n    **Additional notes on [`Argdiffs`][genjax.core.Argdiffs]**\n\n    Argument changes induce changes to the distribution over samples, internal K and L proposals, and (by virtue of changes to $P$) target distributions. The [`Argdiffs`][genjax.core.Argdiffs] type denotes the type of values attached with a _change type_, a piece of data which indicates how the value has changed from the arguments which created the trace. Generative functions can utilize change type information to inform efficient [`edit`][genjax.core.GenerativeFunction.edit] implementations.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.get_zero_trace","title":"get_zero_trace","text":"<pre><code>get_zero_trace(*args, **_kwargs) -&gt; Trace[R]\n</code></pre> <pre><code>    Returns a trace with zero values for all leaves, generated without executing the generative function.\n\n    This method is useful for static analysis and shape inference without executing the generative function. It returns a trace with the same structure as a real trace, but filled with zero or default values.\n\n    Args:\n        *args: The arguments to the generative function.\n        **_kwargs: Ignored keyword arguments.\n\n    Returns:\n        A trace with zero values, matching the structure of a real trace.\n\n    Note:\n        This method uses the `empty_trace` utility function, which creates a trace without spending any FLOPs. The resulting trace has the correct structure but contains placeholder zero values.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        @genjax.gen\n        def weather_model():\n            temperature = genjax.normal(20.0, 5.0) @ \"temperature\"\n            is_sunny = genjax.bernoulli(0.7) @ \"is_sunny\"\n            return {\"temperature\": temperature, \"is_sunny\": is_sunny}\n\n\n        zero_trace = weather_model.get_zero_trace()\n        print(\"Zero trace structure:\")\n        print(zero_trace.render_html())\n\n        print(\"\n</code></pre> <p>Actual simulation:\")             key = jax.random.key(0)             actual_trace = weather_model.simulate(key, ())             print(actual_trace.render_html())             ```</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def get_zero_trace(self, *args, **_kwargs) -&gt; Trace[R]:\n    \"\"\"\n    Returns a trace with zero values for all leaves, generated without executing the generative function.\n\n    This method is useful for static analysis and shape inference without executing the generative function. It returns a trace with the same structure as a real trace, but filled with zero or default values.\n\n    Args:\n        *args: The arguments to the generative function.\n        **_kwargs: Ignored keyword arguments.\n\n    Returns:\n        A trace with zero values, matching the structure of a real trace.\n\n    Note:\n        This method uses the `empty_trace` utility function, which creates a trace without spending any FLOPs. The resulting trace has the correct structure but contains placeholder zero values.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        @genjax.gen\n        def weather_model():\n            temperature = genjax.normal(20.0, 5.0) @ \"temperature\"\n            is_sunny = genjax.bernoulli(0.7) @ \"is_sunny\"\n            return {\"temperature\": temperature, \"is_sunny\": is_sunny}\n\n\n        zero_trace = weather_model.get_zero_trace()\n        print(\"Zero trace structure:\")\n        print(zero_trace.render_html())\n\n        print(\"\\nActual simulation:\")\n        key = jax.random.key(0)\n        actual_trace = weather_model.simulate(key, ())\n        print(actual_trace.render_html())\n        ```\n    \"\"\"\n    return empty_trace(self, args)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.handle_kwargs","title":"handle_kwargs","text":"<pre><code>handle_kwargs() -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns a new GenerativeFunction like <code>self</code>, but where all GFI methods accept a tuple of arguments and a dictionary of keyword arguments.</p> <p>The returned GenerativeFunction can be invoked with <code>__call__</code> with no special argument handling (just like the original).</p> <p>In place of <code>args</code> tuples in GFI methods, the new GenerativeFunction expects a 2-tuple containing:</p> <ol> <li>A tuple containing the original positional arguments.</li> <li>A dictionary containing the keyword arguments.</li> </ol> <p>This allows for more flexible argument passing, especially useful in contexts where keyword arguments need to be handled separately or passed through multiple layers.</p> <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A new GenerativeFunction that accepts (args_tuple, kwargs_dict) for all GFI methods.</p> Example <pre><code>import genjax\nimport jax\n\n\n@genjax.gen\ndef model(x, y, z=1.0):\n    _ = genjax.normal(x + y, z) @ \"v\"\n    return x + y + z\n\n\nkey = jax.random.key(0)\nkw_model = model.handle_kwargs()\n\ntr = kw_model.simulate(key, ((1.0, 2.0), {\"z\": 3.0}))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def handle_kwargs(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Returns a new GenerativeFunction like `self`, but where all GFI methods accept a tuple of arguments and a dictionary of keyword arguments.\n\n    The returned GenerativeFunction can be invoked with `__call__` with no special argument handling (just like the original).\n\n    In place of `args` tuples in GFI methods, the new GenerativeFunction expects a 2-tuple containing:\n\n    1. A tuple containing the original positional arguments.\n    2. A dictionary containing the keyword arguments.\n\n    This allows for more flexible argument passing, especially useful in contexts where\n    keyword arguments need to be handled separately or passed through multiple layers.\n\n    Returns:\n        A new GenerativeFunction that accepts (args_tuple, kwargs_dict) for all GFI methods.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        import genjax\n        import jax\n\n\n        @genjax.gen\n        def model(x, y, z=1.0):\n            _ = genjax.normal(x + y, z) @ \"v\"\n            return x + y + z\n\n\n        key = jax.random.key(0)\n        kw_model = model.handle_kwargs()\n\n        tr = kw_model.simulate(key, ((1.0, 2.0), {\"z\": 3.0}))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    return IgnoreKwargs(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.importance","title":"importance","text":"<pre><code>importance(\n    key: PRNGKey, constraint: ChoiceMap, args: Arguments\n) -&gt; tuple[Trace[R], Weight]\n</code></pre> <p>Returns a properly weighted pair, a <code>Trace</code> and a <code>Weight</code>, properly weighted for the target induced by the generative function for the provided constraint and arguments.</p> <p>Examples:</p> <p>(Full constraints) A simple example using the <code>importance</code> interface on distributions: <pre><code>import jax\nfrom genjax import normal\nfrom genjax import ChoiceMapBuilder as C\n\nkey = jax.random.key(0)\n\ntr, w = normal.importance(key, C.v(1.0), (0.0, 1.0))\nprint(tr.get_choices().render_html())\n</code></pre> </p> <p>(Internal proposal for partial constraints) Specifying a partial constraint on a <code>StaticGenerativeFunction</code>: <pre><code>from genjax import flip, uniform, gen\nfrom genjax import ChoiceMapBuilder as C\n\n\n@gen\ndef model():\n    p = uniform(0.0, 1.0) @ \"p\"\n    f1 = flip(p) @ \"f1\"\n    f2 = flip(p) @ \"f2\"\n\n\ntr, w = model.importance(key, C.kw(f1=True, f2=True), ())\nprint(tr.get_choices().render_html())\n</code></pre> </p> <p>Under the hood, creates an <code>EditRequest</code> which requests that the generative function respond with a move from the empty trace (the only possible value for empty target \\(\\delta_\\emptyset\\)) to the target induced by the generative function for constraint \\(C\\) with arguments \\(a\\).</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def importance(\n    self,\n    key: PRNGKey,\n    constraint: ChoiceMap,\n    args: Arguments,\n) -&gt; tuple[Trace[R], Weight]:\n    \"\"\"\n    Returns a properly weighted pair, a [`Trace`][genjax.core.Trace] and a [`Weight`][genjax.core.Weight], properly weighted for the target induced by the generative function for the provided constraint and arguments.\n\n    Examples:\n        (**Full constraints**) A simple example using the `importance` interface on distributions:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        import jax\n        from genjax import normal\n        from genjax import ChoiceMapBuilder as C\n\n        key = jax.random.key(0)\n\n        tr, w = normal.importance(key, C.v(1.0), (0.0, 1.0))\n        print(tr.get_choices().render_html())\n        ```\n\n        (**Internal proposal for partial constraints**) Specifying a _partial_ constraint on a [`StaticGenerativeFunction`][genjax.StaticGenerativeFunction]:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import flip, uniform, gen\n        from genjax import ChoiceMapBuilder as C\n\n\n        @gen\n        def model():\n            p = uniform(0.0, 1.0) @ \"p\"\n            f1 = flip(p) @ \"f1\"\n            f2 = flip(p) @ \"f2\"\n\n\n        tr, w = model.importance(key, C.kw(f1=True, f2=True), ())\n        print(tr.get_choices().render_html())\n        ```\n\n    Under the hood, creates an [`EditRequest`][genjax.core.EditRequest] which requests that the generative function respond with a move from the _empty_ trace (the only possible value for _empty_ target $\\\\delta_\\\\emptyset$) to the target induced by the generative function for constraint $C$ with arguments $a$.\n    \"\"\"\n\n    return self.generate(\n        key,\n        constraint,\n        args,\n    )\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.iterate","title":"iterate","text":"<pre><code>iterate(*, n: int) -&gt; GenerativeFunction[R]\n</code></pre> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; [a]</code> where</p> <ul> <li><code>a</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>[a]</code> is an array of all <code>a</code>, <code>f(a)</code>, <code>f(f(a))</code> etc. values seen during iteration.</li> </ul> <p>All traced values are nested under an index.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def iterate(f, n, init):\n    input = init\n    seen = [init]\n    for _ in range(n):\n        input = f(input)\n        seen.append(input)\n    return seen\n</code></pre> <p><code>init</code> may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.</p> <p>The iterated value <code>a</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>a</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>the number of iterations to run.</p> required <p>Examples:</p> <p>iterative addition, returning all intermediate sums: <pre><code>import jax\nimport genjax\n\n\n@genjax.iterate(n=100)\n@genjax.gen\ndef inc(x):\n    return x + 1\n\n\ninit = 0.0\nkey = jax.random.key(314159)\n\ntr = jax.jit(inc.simulate)(key, (init,))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def iterate(\n    self,\n    /,\n    *,\n    n: int,\n) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    When called on a [`genjax.GenerativeFunction`][] of type `a -&gt; a`, returns a new [`genjax.GenerativeFunction`][] of type `a -&gt; [a]` where\n\n    - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `[a]` is an array of all `a`, `f(a)`, `f(f(a))` etc. values seen during iteration.\n\n    All traced values are nested under an index.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def iterate(f, n, init):\n        input = init\n        seen = [init]\n        for _ in range(n):\n            input = f(input)\n            seen.append(input)\n        return seen\n    ```\n\n    `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n    The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: the number of iterations to run.\n\n    Examples:\n        iterative addition, returning all intermediate sums:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.iterate(n=100)\n        @genjax.gen\n        def inc(x):\n            return x + 1\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n\n        tr = jax.jit(inc.simulate)(key, (init,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.iterate(n=n)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.iterate(n)","title":"<code>n</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.iterate_final","title":"iterate_final","text":"<pre><code>iterate_final(*, n: int) -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns a decorator that wraps a <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> and returns a new <code>genjax.GenerativeFunction</code> of type <code>a -&gt; a</code> where</p> <ul> <li><code>a</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li>the original function is invoked <code>n</code> times with each input coming from the previous invocation's output, so that the new function returns \\(f^n(a)\\)</li> </ul> <p>All traced values are nested under an index.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def iterate_final(f, n, init):\n    ret = init\n    for _ in range(n):\n        ret = f(ret)\n    return ret\n</code></pre> <p><code>init</code> may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.</p> <p>The iterated value <code>a</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>a</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>the number of iterations to run.</p> required <p>Examples:</p> <p>iterative addition: <pre><code>import jax\nimport genjax\n\n\n@genjax.iterate_final(n=100)\n@genjax.gen\ndef inc(x):\n    return x + 1\n\n\ninit = 0.0\nkey = jax.random.key(314159)\n\ntr = jax.jit(inc.simulate)(key, (init,))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def iterate_final(\n    self,\n    /,\n    *,\n    n: int,\n) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Returns a decorator that wraps a [`genjax.GenerativeFunction`][] of type `a -&gt; a` and returns a new [`genjax.GenerativeFunction`][] of type `a -&gt; a` where\n\n    - `a` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - the original function is invoked `n` times with each input coming from the previous invocation's output, so that the new function returns $f^n(a)$\n\n    All traced values are nested under an index.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def iterate_final(f, n, init):\n        ret = init\n        for _ in range(n):\n            ret = f(ret)\n        return ret\n    ```\n\n    `init` may be an arbitrary pytree value, and so multiple arrays can be iterated over at once and produce multiple output arrays.\n\n    The iterated value `a` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `a` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: the number of iterations to run.\n\n    Examples:\n        iterative addition:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.iterate_final(n=100)\n        @genjax.gen\n        def inc(x):\n            return x + 1\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n\n        tr = jax.jit(inc.simulate)(key, (init,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.iterate_final(n=n)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.iterate_final(n)","title":"<code>n</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.map","title":"map","text":"<pre><code>map(f: Callable[[R], S]) -&gt; GenerativeFunction[S]\n</code></pre> <p>Specialized version of <code>genjax.dimap</code> where only the post-processing function is applied.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[[R], S]</code> <p>A callable that postprocesses the return value of the wrapped function.</p> required <p>Returns:</p> Type Description <code>GenerativeFunction[S]</code> <p>A <code>genjax.GenerativeFunction</code> that acts like <code>self</code> with a post-processing function to its return value.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n# Define a post-processing function\ndef square(x):\n    return x**2\n\n\n@genjax.gen\ndef model(x):\n    return genjax.normal(x, 1.0) @ \"z\"\n\n\nmap_model = model.map(square)\n\n# Use the map model\nkey = jax.random.key(0)\ntrace = map_model.simulate(key, (2.0,))\n\nprint(trace.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def map(self, f: Callable[[R], S]) -&gt; \"GenerativeFunction[S]\":\n    \"\"\"\n    Specialized version of [`genjax.dimap`][] where only the post-processing function is applied.\n\n    Args:\n        f: A callable that postprocesses the return value of the wrapped function.\n\n    Returns:\n        A [`genjax.GenerativeFunction`][] that acts like `self` with a post-processing function to its return value.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"map\"\n        import jax, genjax\n\n\n        # Define a post-processing function\n        def square(x):\n            return x**2\n\n\n        @genjax.gen\n        def model(x):\n            return genjax.normal(x, 1.0) @ \"z\"\n\n\n        map_model = model.map(square)\n\n        # Use the map model\n        key = jax.random.key(0)\n        trace = map_model.simulate(key, (2.0,))\n\n        print(trace.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.map(f=f)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.map(f)","title":"<code>f</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.mask","title":"mask","text":"<pre><code>mask() -&gt; GenerativeFunction[Mask[R]]\n</code></pre> <p>Enables dynamic masking of generative functions. Returns a new <code>genjax.GenerativeFunction</code> like <code>self</code>, but which accepts an additional boolean first argument.</p> <p>If <code>True</code>, the invocation of <code>self</code> is masked, and its contribution to the score is ignored. If <code>False</code>, it has the same semantics as if one was invoking <code>self</code> without masking.</p> <p>The return value type is a <code>Mask</code>, with a flag value equal to the supplied boolean.</p> <p>Returns:</p> Type Description <code>GenerativeFunction[Mask[R]]</code> <p>The masked version of the original <code>genjax.GenerativeFunction</code>.</p> <p>Examples:</p> <p>Masking a normal draw: <pre><code>import genjax, jax\n\n\n@genjax.gen\ndef normal_draw(mean):\n    return genjax.normal(mean, 1.0) @ \"x\"\n\n\nmasked_normal_draw = normal_draw.mask()\n\nkey = jax.random.key(314159)\ntr = jax.jit(masked_normal_draw.simulate)(\n    key,\n    (\n        False,\n        2.0,\n    ),\n)\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def mask(self, /) -&gt; \"GenerativeFunction[genjax.Mask[R]]\":\n    \"\"\"\n    Enables dynamic masking of generative functions. Returns a new [`genjax.GenerativeFunction`][] like `self`, but which accepts an additional boolean first argument.\n\n    If `True`, the invocation of `self` is masked, and its contribution to the score is ignored. If `False`, it has the same semantics as if one was invoking `self` without masking.\n\n    The return value type is a `Mask`, with a flag value equal to the supplied boolean.\n\n    Returns:\n        The masked version of the original [`genjax.GenerativeFunction`][].\n\n    Examples:\n        Masking a normal draw:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mask\"\n        import genjax, jax\n\n\n        @genjax.gen\n        def normal_draw(mean):\n            return genjax.normal(mean, 1.0) @ \"x\"\n\n\n        masked_normal_draw = normal_draw.mask()\n\n        key = jax.random.key(314159)\n        tr = jax.jit(masked_normal_draw.simulate)(\n            key,\n            (\n                False,\n                2.0,\n            ),\n        )\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.mask(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.masked_iterate","title":"masked_iterate","text":"<pre><code>masked_iterate() -&gt; GenerativeFunction[R]\n</code></pre> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a list of values of type <code>a</code>.</p> <p>The original function is modified to accept an additional argument <code>mask</code>, which is a boolean value indicating whether the operation should be masked or not. The function returns a Masked list of results of the original operation with the input [mask] as mask.</p> <p>All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.</p> Example <pre><code>import jax\nimport genjax\n\nmasks = jnp.array([True, False, True])\n\n\n# Create a kernel generative function\n@genjax.gen\ndef step(x):\n    _ = (\n        genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n        @ \"rats\"\n    )\n    return x\n\n\n# Create a model using masked_iterate\nmodel = step.masked_iterate()\n\n# Simulate from the model\nkey = jax.random.key(0)\nmask_steps = jnp.arange(10) &lt; 5\ntr = model.simulate(key, (0.0, mask_steps))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def masked_iterate(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a list of values of type `a`.\n\n    The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns a Masked list of results of the original operation with the input [mask] as mask.\n\n    All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n        masks = jnp.array([True, False, True])\n\n\n        # Create a kernel generative function\n        @genjax.gen\n        def step(x):\n            _ = (\n                genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                @ \"rats\"\n            )\n            return x\n\n\n        # Create a model using masked_iterate\n        model = step.masked_iterate()\n\n        # Simulate from the model\n        key = jax.random.key(0)\n        mask_steps = jnp.arange(10) &lt; 5\n        tr = model.simulate(key, (0.0, mask_steps))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.masked_iterate()(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.masked_iterate_final","title":"masked_iterate_final","text":"<pre><code>masked_iterate_final() -&gt; GenerativeFunction[R]\n</code></pre> <p>Transforms a generative function that takes a single argument of type <code>a</code> and returns a value of type <code>a</code>, into a function that takes a tuple of arguments <code>(a, [mask])</code> and returns a value of type <code>a</code>.</p> <p>The original function is modified to accept an additional argument <code>mask</code>, which is a boolean value indicating whether the operation should be masked or not. The function returns the result of the original operation if <code>mask</code> is <code>True</code>, and the original input if <code>mask</code> is <code>False</code>.</p> <p>All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.</p> Example <pre><code>import jax\nimport genjax\n\nmasks = jnp.array([True, False, True])\n\n\n# Create a kernel generative function\n@genjax.gen\ndef step(x):\n    _ = (\n        genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n        @ \"rats\"\n    )\n    return x\n\n\n# Create a model using masked_iterate_final\nmodel = step.masked_iterate_final()\n\n# Simulate from the model\nkey = jax.random.key(0)\nmask_steps = jnp.arange(10) &lt; 5\ntr = model.simulate(key, (0.0, mask_steps))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def masked_iterate_final(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Transforms a generative function that takes a single argument of type `a` and returns a value of type `a`, into a function that takes a tuple of arguments `(a, [mask])` and returns a value of type `a`.\n\n    The original function is modified to accept an additional argument `mask`, which is a boolean value indicating whether the operation should be masked or not. The function returns the result of the original operation if `mask` is `True`, and the original input if `mask` is `False`.\n\n    All traced values from the kernel generative function are traced (with an added axis due to the scan) but only those indices from [mask] with a flag of True will accounted for in inference, notably for score computations.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n        masks = jnp.array([True, False, True])\n\n\n        # Create a kernel generative function\n        @genjax.gen\n        def step(x):\n            _ = (\n                genjax.normal.mask().vmap(in_axes=(0, None, None))(masks, x, 1.0)\n                @ \"rats\"\n            )\n            return x\n\n\n        # Create a model using masked_iterate_final\n        model = step.masked_iterate_final()\n\n        # Simulate from the model\n        key = jax.random.key(0)\n        mask_steps = jnp.arange(10) &lt; 5\n        tr = model.simulate(key, (0.0, mask_steps))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.masked_iterate_final()(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.mix","title":"mix","text":"<pre><code>mix(*fns: GenerativeFunction[R]) -&gt; GenerativeFunction[R]\n</code></pre> <p>Takes any number of <code>genjax.GenerativeFunction</code>s and returns a new <code>genjax.GenerativeFunction</code> that represents a mixture model.</p> <p>The returned generative function takes the following arguments:</p> <ul> <li><code>mixture_logits</code>: Logits for the categorical distribution used to select a component.</li> <li><code>*args</code>: Argument tuples for <code>self</code> and each of the input generative functions</li> </ul> <p>and samples from <code>self</code> or one of the input generative functions based on a draw from a categorical distribution defined by the provided mixture logits.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>Variable number of <code>genjax.GenerativeFunction</code>s to be mixed with <code>self</code>.</p> <code>()</code> <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A new <code>genjax.GenerativeFunction</code> representing the mixture model.</p> <p>Examples:</p> <p>```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mix\" import jax import genjax</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.mix(*fns)","title":"<code>*fns</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.mix--define-component-generative-functions","title":"Define component generative functions","text":"<p>@genjax.gen def component1(x):     return genjax.normal(x, 1.0) @ \"y\"</p> <p>@genjax.gen def component2(x):     return genjax.normal(x, 2.0) @ \"y\"</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.mix--create-mixture-model","title":"Create mixture model","text":"<p>mixture = component1.mix(component2)</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.mix--use-the-mixture-model","title":"Use the mixture model","text":"<p>key = jax.random.key(0) logits = jax.numpy.array([0.3, 0.7])  # Favors component2 trace = mixture.simulate(key, (logits, (0.0,), (7.0,))) print(trace.render_html())     ```</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def mix(self, *fns: \"GenerativeFunction[R]\") -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Takes any number of [`genjax.GenerativeFunction`][]s and returns a new [`genjax.GenerativeFunction`][] that represents a mixture model.\n\n    The returned generative function takes the following arguments:\n\n    - `mixture_logits`: Logits for the categorical distribution used to select a component.\n    - `*args`: Argument tuples for `self` and each of the input generative functions\n\n    and samples from `self` or one of the input generative functions based on a draw from a categorical distribution defined by the provided mixture logits.\n\n    Args:\n        *fns: Variable number of [`genjax.GenerativeFunction`][]s to be mixed with `self`.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] representing the mixture model.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"mix\"\n        import jax\n        import genjax\n\n\n        # Define component generative functions\n        @genjax.gen\n        def component1(x):\n            return genjax.normal(x, 1.0) @ \"y\"\n\n\n        @genjax.gen\n        def component2(x):\n            return genjax.normal(x, 2.0) @ \"y\"\n\n\n        # Create mixture model\n        mixture = component1.mix(component2)\n\n        # Use the mixture model\n        key = jax.random.key(0)\n        logits = jax.numpy.array([0.3, 0.7])  # Favors component2\n        trace = mixture.simulate(key, (logits, (0.0,), (7.0,)))\n        print(trace.render_html())\n            ```\n    \"\"\"\n    import genjax\n\n    return genjax.mix(self, *fns)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.or_else","title":"or_else","text":"<pre><code>or_else(\n    gen_fn: GenerativeFunction[R],\n) -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns a <code>GenerativeFunction</code> that accepts</p> <ul> <li>a boolean argument</li> <li>an argument tuple for <code>self</code></li> <li>an argument tuple for the supplied <code>gen_fn</code></li> </ul> <p>and acts like <code>self</code> when the boolean is <code>True</code> or like <code>gen_fn</code> otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[R]</code> <p>called when the boolean argument is <code>False</code>.</p> required <p>Examples:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\n\n\n@genjax.gen\ndef if_model(x):\n    return genjax.normal(x, 1.0) @ \"if_value\"\n\n\n@genjax.gen\ndef else_model(x):\n    return genjax.normal(x, 5.0) @ \"else_value\"\n\n\n@genjax.gen\ndef model(toss: bool):\n    # Note that the returned model takes a new boolean predicate in\n    # addition to argument tuples for each branch.\n    return if_model.or_else(else_model)(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\nkey = jax.random.key(314159)\n\ntr = jax.jit(model.simulate)(key, (True,))\n\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def or_else(self, gen_fn: \"GenerativeFunction[R]\", /) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Returns a [`GenerativeFunction`][genjax.GenerativeFunction] that accepts\n\n    - a boolean argument\n    - an argument tuple for `self`\n    - an argument tuple for the supplied `gen_fn`\n\n    and acts like `self` when the boolean is `True` or like `gen_fn` otherwise.\n\n    Args:\n        gen_fn: called when the boolean argument is `False`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"gen-fn\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n\n\n        @genjax.gen\n        def if_model(x):\n            return genjax.normal(x, 1.0) @ \"if_value\"\n\n\n        @genjax.gen\n        def else_model(x):\n            return genjax.normal(x, 5.0) @ \"else_value\"\n\n\n        @genjax.gen\n        def model(toss: bool):\n            # Note that the returned model takes a new boolean predicate in\n            # addition to argument tuples for each branch.\n            return if_model.or_else(else_model)(toss, (1.0,), (10.0,)) @ \"tossed\"\n\n\n        key = jax.random.key(314159)\n\n        tr = jax.jit(model.simulate)(key, (True,))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.or_else(self, gen_fn)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.or_else(gen_fn)","title":"<code>gen_fn</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.propose","title":"propose","text":"<pre><code>propose(\n    key: PRNGKey, args: Arguments\n) -&gt; tuple[ChoiceMap, Score, R]\n</code></pre> <p>Samples a <code>ChoiceMap</code> and any untraced randomness \\(r\\) from the generative function's distribution over samples (\\(P\\)), and returns the <code>Score</code> of that sample under the distribution, and the <code>R</code> of the generative function's return value function \\(f(r, t, a)\\) for the sample and untraced randomness.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def propose(\n    self,\n    key: PRNGKey,\n    args: Arguments,\n) -&gt; tuple[ChoiceMap, Score, R]:\n    \"\"\"\n    Samples a [`ChoiceMap`][genjax.core.ChoiceMap] and any untraced randomness $r$ from the generative function's distribution over samples ($P$), and returns the [`Score`][genjax.core.Score] of that sample under the distribution, and the `R` of the generative function's return value function $f(r, t, a)$ for the sample and untraced randomness.\n    \"\"\"\n    tr = self.simulate(key, args)\n    sample = tr.get_choices()\n    score = tr.get_score()\n    retval = tr.get_retval()\n    return sample, score, retval\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.reduce","title":"reduce","text":"<pre><code>reduce() -&gt; GenerativeFunction[R]\n</code></pre> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; c</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; c</code> where</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> </ul> <p>All traced values are nested under an index.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>The semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation (note the similarity to <code>functools.reduce</code>):</p> <pre><code>def reduce(f, init, xs):\n    carry = init\n    for x in xs:\n        carry = f(carry, x)\n    return carry\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>carry</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Examples:</p> <p>sum an array of numbers: <pre><code>import jax\nimport genjax\nimport jax.numpy as jnp\n\n\n@genjax.reduce()\n@genjax.gen\ndef add(sum, x):\n    new_sum = sum + x\n    return new_sum\n\n\ninit = 0.0\nkey = jax.random.key(314159)\nxs = jnp.ones(10)\n\ntr = jax.jit(add.simulate)(key, (init, xs))\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def reduce(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; c`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; c` where\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n\n    All traced values are nested under an index.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    The semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation (note the similarity to [`functools.reduce`](https://docs.python.org/3/library/itertools.html#functools.reduce)):\n\n    ```python\n    def reduce(f, init, xs):\n        carry = init\n        for x in xs:\n            carry = f(carry, x)\n        return carry\n    ```\n\n    Unlike that Python version, both `xs` and `carry` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Examples:\n        sum an array of numbers:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n        import jax.numpy as jnp\n\n\n        @genjax.reduce()\n        @genjax.gen\n        def add(sum, x):\n            new_sum = sum + x\n            return new_sum\n\n\n        init = 0.0\n        key = jax.random.key(314159)\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add.simulate)(key, (init, xs))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.reduce()(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.repeat","title":"repeat","text":"<pre><code>repeat(*, n: int) -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns a <code>genjax.GenerativeFunction</code> that samples from <code>self</code> <code>n</code> times, returning a vector of <code>n</code> results.</p> <p>The values traced by each call <code>gen_fn</code> will be nested under an integer index that matches the loop iteration index that generated it.</p> <p>This combinator is useful for creating multiple samples from <code>self</code> in a batched manner.</p> <p>Parameters:</p> Name Type Description Default <code>int</code> <p>The number of times to sample from the generative function.</p> required <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A new <code>genjax.GenerativeFunction</code> that samples from the original function <code>n</code> times.</p> <p>Examples:</p> <pre><code>import genjax, jax\n\n\n@genjax.gen\ndef normal_draw(mean):\n    return genjax.normal(mean, 1.0) @ \"x\"\n\n\nnormal_draws = normal_draw.repeat(n=10)\n\nkey = jax.random.key(314159)\n\n# Generate 10 draws from a normal distribution with mean 2.0\ntr = jax.jit(normal_draws.simulate)(key, (2.0,))\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def repeat(self, /, *, n: int) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Returns a [`genjax.GenerativeFunction`][] that samples from `self` `n` times, returning a vector of `n` results.\n\n    The values traced by each call `gen_fn` will be nested under an integer index that matches the loop iteration index that generated it.\n\n    This combinator is useful for creating multiple samples from `self` in a batched manner.\n\n    Args:\n        n: The number of times to sample from the generative function.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] that samples from the original function `n` times.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"repeat\"\n        import genjax, jax\n\n\n        @genjax.gen\n        def normal_draw(mean):\n            return genjax.normal(mean, 1.0) @ \"x\"\n\n\n        normal_draws = normal_draw.repeat(n=10)\n\n        key = jax.random.key(314159)\n\n        # Generate 10 draws from a normal distribution with mean 2.0\n        tr = jax.jit(normal_draws.simulate)(key, (2.0,))\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.repeat(n=n)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.repeat(n)","title":"<code>n</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.scan","title":"scan","text":"<pre><code>scan(\n    *, n: int | None = None\n) -&gt; GenerativeFunction[tuple[Carry, Y]]\n</code></pre> <p>When called on a <code>genjax.GenerativeFunction</code> of type <code>(c, a) -&gt; (c, b)</code>, returns a new <code>genjax.GenerativeFunction</code> of type <code>(c, [a]) -&gt; (c, [b])</code> where</p> <ul> <li><code>c</code> is a loop-carried value, which must hold a fixed shape and dtype across all iterations</li> <li><code>a</code> may be a primitive, an array type or a pytree (container) type with array leaves</li> <li><code>b</code> may be a primitive, an array type or a pytree (container) type with array leaves.</li> </ul> <p>The values traced by each call to the original generative function will be nested under an integer index that matches the loop iteration index that generated it.</p> <p>For any array type specifier <code>t</code>, <code>[t]</code> represents the type with an additional leading axis, and if <code>t</code> is a pytree (container) type with array leaves then <code>[t]</code> represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.</p> <p>When the type of <code>xs</code> in the snippet below (denoted <code>[a]</code> above) is an array type or None, and the type of <code>ys</code> in the snippet below (denoted <code>[b]</code> above) is an array type, the semantics of the returned <code>genjax.GenerativeFunction</code> are given roughly by this Python implementation:</p> <pre><code>def scan(f, init, xs, length=None):\n    if xs is None:\n        xs = [None] * length\n    carry = init\n    ys = []\n    for x in xs:\n        carry, y = f(carry, x)\n        ys.append(y)\n    return carry, np.stack(ys)\n</code></pre> <p>Unlike that Python version, both <code>xs</code> and <code>ys</code> may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays. <code>None</code> is actually a special case of this, as it represents an empty pytree.</p> <p>The loop-carried value <code>c</code> must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type <code>c</code> in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).</p> <p>Parameters:</p> Name Type Description Default <code>int | None</code> <p>optional integer specifying the number of loop iterations, which (if supplied) must agree with the sizes of leading axes of the arrays in the returned function's second argument. If supplied then the returned generative function can take <code>None</code> as its second argument.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerativeFunction[tuple[Carry, Y]]</code> <p>A new <code>genjax.GenerativeFunction</code> that takes a loop-carried value and a new input, and returns a new loop-carried value along with either <code>None</code> or an output to be collected into the second return value.</p> <p>Examples:</p> <p>Scan for 1000 iterations with no array input: <pre><code>import jax\nimport genjax\n\n\n@genjax.gen\ndef random_walk_step(prev, _):\n    x = genjax.normal(prev, 1.0) @ \"x\"\n    return x, None\n\n\nrandom_walk = random_walk_step.scan(n=1000)\n\ninit = 0.5\nkey = jax.random.key(314159)\n\ntr = jax.jit(random_walk.simulate)(key, (init, None))\nprint(tr.render_html())\n</code></pre> </p> <p>Scan across an input array: <pre><code>import jax.numpy as jnp\n\n\n@genjax.gen\ndef add_and_square_step(sum, x):\n    new_sum = sum + x\n    return new_sum, sum * sum\n\n\n# notice no `n` parameter supplied:\nadd_and_square_all = add_and_square_step.scan()\ninit = 0.0\nxs = jnp.ones(10)\n\ntr = jax.jit(add_and_square_all.simulate)(key, (init, xs))\n\n# The retval has the final carry and an array of all `sum*sum` returned.\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def scan(\n    self: \"GenerativeFunction[tuple[Carry, Y]]\",\n    /,\n    *,\n    n: int | None = None,\n) -&gt; \"GenerativeFunction[tuple[Carry, Y]]\":\n    \"\"\"\n    When called on a [`genjax.GenerativeFunction`][] of type `(c, a) -&gt; (c, b)`, returns a new [`genjax.GenerativeFunction`][] of type `(c, [a]) -&gt; (c, [b])` where\n\n    - `c` is a loop-carried value, which must hold a fixed shape and dtype across all iterations\n    - `a` may be a primitive, an array type or a pytree (container) type with array leaves\n    - `b` may be a primitive, an array type or a pytree (container) type with array leaves.\n\n    The values traced by each call to the original generative function will be nested under an integer index that matches the loop iteration index that generated it.\n\n    For any array type specifier `t`, `[t]` represents the type with an additional leading axis, and if `t` is a pytree (container) type with array leaves then `[t]` represents the type with the same pytree structure and corresponding leaves each with an additional leading axis.\n\n    When the type of `xs` in the snippet below (denoted `[a]` above) is an array type or None, and the type of `ys` in the snippet below (denoted `[b]` above) is an array type, the semantics of the returned [`genjax.GenerativeFunction`][] are given roughly by this Python implementation:\n\n    ```python\n    def scan(f, init, xs, length=None):\n        if xs is None:\n            xs = [None] * length\n        carry = init\n        ys = []\n        for x in xs:\n            carry, y = f(carry, x)\n            ys.append(y)\n        return carry, np.stack(ys)\n    ```\n\n    Unlike that Python version, both `xs` and `ys` may be arbitrary pytree values, and so multiple arrays can be scanned over at once and produce multiple output arrays. `None` is actually a special case of this, as it represents an empty pytree.\n\n    The loop-carried value `c` must hold a fixed shape and dtype across all iterations (and not just be consistent up to NumPy rank/shape broadcasting and dtype promotion rules, for example). In other words, the type `c` in the type signature above represents an array with a fixed shape and dtype (or a nested tuple/list/dict container data structure with a fixed structure and arrays with fixed shape and dtype at the leaves).\n\n    Args:\n        n: optional integer specifying the number of loop iterations, which (if supplied) must agree with the sizes of leading axes of the arrays in the returned function's second argument. If supplied then the returned generative function can take `None` as its second argument.\n\n    Returns:\n        A new [`genjax.GenerativeFunction`][] that takes a loop-carried value and a new input, and returns a new loop-carried value along with either `None` or an output to be collected into the second return value.\n\n    Examples:\n        Scan for 1000 iterations with no array input:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax\n        import genjax\n\n\n        @genjax.gen\n        def random_walk_step(prev, _):\n            x = genjax.normal(prev, 1.0) @ \"x\"\n            return x, None\n\n\n        random_walk = random_walk_step.scan(n=1000)\n\n        init = 0.5\n        key = jax.random.key(314159)\n\n        tr = jax.jit(random_walk.simulate)(key, (init, None))\n        print(tr.render_html())\n        ```\n\n        Scan across an input array:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"scan\"\n        import jax.numpy as jnp\n\n\n        @genjax.gen\n        def add_and_square_step(sum, x):\n            new_sum = sum + x\n            return new_sum, sum * sum\n\n\n        # notice no `n` parameter supplied:\n        add_and_square_all = add_and_square_step.scan()\n        init = 0.0\n        xs = jnp.ones(10)\n\n        tr = jax.jit(add_and_square_all.simulate)(key, (init, xs))\n\n        # The retval has the final carry and an array of all `sum*sum` returned.\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.scan(n=n)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.scan(n)","title":"<code>n</code>","text":""},{"location":"library/core.html#genjax.core.GenerativeFunction.simulate","title":"simulate  <code>abstractmethod</code>","text":"<pre><code>simulate(key: PRNGKey, args: Arguments) -&gt; Trace[R]\n</code></pre> <p>Execute the generative function, sampling from its distribution over samples, and return a <code>Trace</code>.</p>"},{"location":"library/core.html#genjax.core.GenerativeFunction.simulate--more-on-traces","title":"More on traces","text":"<p>The <code>Trace</code> returned by <code>simulate</code> implements its own interface.</p> <p>It is responsible for storing the arguments of the invocation (<code>genjax.Trace.get_args</code>), the return value of the generative function (<code>genjax.Trace.get_retval</code>), the identity of the generative function which produced the trace (<code>genjax.Trace.get_gen_fn</code>), the sample of traced random choices produced during the invocation (<code>genjax.Trace.get_choices</code>) and the score of the sample (<code>genjax.Trace.get_score</code>).</p> <p>Examples:</p> <pre><code>import genjax\nimport jax\nfrom jax import vmap, jit\nfrom jax.random import split\n\n\n@genjax.gen\ndef model():\n    x = genjax.normal(0.0, 1.0) @ \"x\"\n    return x\n\n\nkey = jax.random.key(0)\ntr = model.simulate(key, ())\nprint(tr.render_html())\n</code></pre> <p>Another example, using the same model, composed into <code>genjax.repeat</code> - which creates a new generative function, which has the same interface: <pre><code>@genjax.gen\ndef model():\n    x = genjax.normal(0.0, 1.0) @ \"x\"\n    return x\n\n\nkey = jax.random.key(0)\ntr = model.repeat(n=10).simulate(key, ())\nprint(tr.render_html())\n</code></pre> </p> <p>(Fun, flirty, fast ... parallel?) Feel free to use <code>jax.jit</code> and <code>jax.vmap</code>! <pre><code>key = jax.random.key(0)\nsub_keys = split(key, 10)\nsim = model.repeat(n=10).simulate\ntr = jit(vmap(sim, in_axes=(0, None)))(sub_keys, ())\nprint(tr.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef simulate(\n    self,\n    key: PRNGKey,\n    args: Arguments,\n) -&gt; Trace[R]:\n    \"\"\"\n    Execute the generative function, sampling from its distribution over samples, and return a [`Trace`][genjax.core.Trace].\n\n    ## More on traces\n\n    The [`Trace`][genjax.core.Trace] returned by `simulate` implements its own interface.\n\n    It is responsible for storing the arguments of the invocation ([`genjax.Trace.get_args`][]), the return value of the generative function ([`genjax.Trace.get_retval`][]), the identity of the generative function which produced the trace ([`genjax.Trace.get_gen_fn`][]), the sample of traced random choices produced during the invocation ([`genjax.Trace.get_choices`][]) and _the score_ of the sample ([`genjax.Trace.get_score`][]).\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        import genjax\n        import jax\n        from jax import vmap, jit\n        from jax.random import split\n\n\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            return x\n\n\n        key = jax.random.key(0)\n        tr = model.simulate(key, ())\n        print(tr.render_html())\n        ```\n\n        Another example, using the same model, composed into [`genjax.repeat`](combinators.md#genjax.repeat) - which creates a new generative function, which has the same interface:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            return x\n\n\n        key = jax.random.key(0)\n        tr = model.repeat(n=10).simulate(key, ())\n        print(tr.render_html())\n        ```\n\n        (**Fun, flirty, fast ... parallel?**) Feel free to use `jax.jit` and `jax.vmap`!\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        key = jax.random.key(0)\n        sub_keys = split(key, 10)\n        sim = model.repeat(n=10).simulate\n        tr = jit(vmap(sim, in_axes=(0, None)))(sub_keys, ())\n        print(tr.render_html())\n        ```\n    \"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.switch","title":"switch","text":"<pre><code>switch(*branches: GenerativeFunction[R]) -&gt; Switch[R]\n</code></pre> <p>Given <code>n</code> <code>genjax.GenerativeFunction</code> inputs, returns a new <code>genjax.GenerativeFunction</code> that accepts <code>n+2</code> arguments:</p> <ul> <li>an index in the range \\([0, n+1)\\)</li> <li>a tuple of arguments for <code>self</code> and each of the input generative functions (<code>n+1</code> total tuples)</li> </ul> <p>and executes the generative function at the supplied index with its provided arguments.</p> <p>If <code>index</code> is out of bounds, <code>index</code> is clamped to within bounds.</p> <p>Examples:</p> <pre><code>import jax, genjax\n\n\n@genjax.gen\ndef branch_1():\n    x = genjax.normal(0.0, 1.0) @ \"x1\"\n\n\n@genjax.gen\ndef branch_2():\n    x = genjax.bernoulli(0.3) @ \"x2\"\n\n\nswitch = branch_1.switch(branch_2)\n\nkey = jax.random.key(314159)\njitted = jax.jit(switch.simulate)\n\n# Select `branch_2` by providing 1:\ntr = jitted(key, (1, (), ()))\n\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def switch(self, *branches: \"GenerativeFunction[R]\") -&gt; \"genjax.Switch[R]\":\n    \"\"\"\n    Given `n` [`genjax.GenerativeFunction`][] inputs, returns a new [`genjax.GenerativeFunction`][] that accepts `n+2` arguments:\n\n    - an index in the range $[0, n+1)$\n    - a tuple of arguments for `self` and each of the input generative functions (`n+1` total tuples)\n\n    and executes the generative function at the supplied index with its provided arguments.\n\n    If `index` is out of bounds, `index` is clamped to within bounds.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"switch\"\n        import jax, genjax\n\n\n        @genjax.gen\n        def branch_1():\n            x = genjax.normal(0.0, 1.0) @ \"x1\"\n\n\n        @genjax.gen\n        def branch_2():\n            x = genjax.bernoulli(0.3) @ \"x2\"\n\n\n        switch = branch_1.switch(branch_2)\n\n        key = jax.random.key(314159)\n        jitted = jax.jit(switch.simulate)\n\n        # Select `branch_2` by providing 1:\n        tr = jitted(key, (1, (), ()))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.switch(self, *branches)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.vmap","title":"vmap","text":"<pre><code>vmap(*, in_axes: InAxes = 0) -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns a <code>GenerativeFunction</code> that performs a vectorized map over the argument specified by <code>in_axes</code>. Traced values are nested under an index, and the retval is vectorized.</p> <p>Parameters:</p> Name Type Description Default <code>InAxes</code> <p>Selector specifying which input arguments (or index into them) should be vectorized. Defaults to 0, i.e., the first argument. See this link for more detail.</p> <code>0</code> <p>Returns:</p> Type Description <code>GenerativeFunction[R]</code> <p>A new <code>GenerativeFunction</code> that accepts an argument of one-higher dimension at the position specified by <code>in_axes</code>.</p> <p>Examples:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport genjax\n\n\n@genjax.gen\ndef model(x):\n    v = genjax.normal(x, 1.0) @ \"v\"\n    return genjax.normal(v, 0.01) @ \"q\"\n\n\nvmapped = model.vmap(in_axes=0)\n\nkey = jax.random.key(314159)\narr = jnp.ones(100)\n\n# `vmapped` accepts an array if numbers instead of the original\n# single number that `model` accepted.\ntr = jax.jit(vmapped.simulate)(key, (arr,))\n\nprint(tr.render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def vmap(self, /, *, in_axes: InAxes = 0) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"\n    Returns a [`GenerativeFunction`][genjax.GenerativeFunction] that performs a vectorized map over the argument specified by `in_axes`. Traced values are nested under an index, and the retval is vectorized.\n\n    Args:\n        in_axes: Selector specifying which input arguments (or index into them) should be vectorized. Defaults to 0, i.e., the first argument. See [this link](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees) for more detail.\n\n    Returns:\n        A new [`GenerativeFunction`][genjax.GenerativeFunction] that accepts an argument of one-higher dimension at the position specified by `in_axes`.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"gen-fn\"\n        import jax\n        import jax.numpy as jnp\n        import genjax\n\n\n        @genjax.gen\n        def model(x):\n            v = genjax.normal(x, 1.0) @ \"v\"\n            return genjax.normal(v, 0.01) @ \"q\"\n\n\n        vmapped = model.vmap(in_axes=0)\n\n        key = jax.random.key(314159)\n        arr = jnp.ones(100)\n\n        # `vmapped` accepts an array if numbers instead of the original\n        # single number that `model` accepted.\n        tr = jax.jit(vmapped.simulate)(key, (arr,))\n\n        print(tr.render_html())\n        ```\n    \"\"\"\n    import genjax\n\n    return genjax.vmap(in_axes=in_axes)(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.GenerativeFunction.vmap(in_axes)","title":"<code>in_axes</code>","text":""},{"location":"library/core.html#genjax.core.Trace","title":"genjax.core.Trace","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p><code>Trace</code> is the type of traces of generative functions.</p> <p>A trace is a data structure used to represent sampled executions of generative functions. Traces track metadata associated with the probabilities of choices, as well as other data associated with the invocation of a generative function, including the arguments it was invoked with, its return value, and the identity of the generative function itself.</p> <p>Methods:</p> Name Description <code>edit</code> <p>This method calls out to the underlying <code>GenerativeFunction.edit</code> method - see <code>EditRequest</code> and <code>edit</code> for more information.</p> <code>get_args</code> <p>Returns the <code>Arguments</code> for the <code>GenerativeFunction</code> invocation which created the <code>Trace</code>.</p> <code>get_choices</code> <p>Retrieves the random choices made in a trace in the form of a <code>genjax.ChoiceMap</code>.</p> <code>get_gen_fn</code> <p>Returns the <code>GenerativeFunction</code> whose invocation created the <code>Trace</code>.</p> <code>get_inner_trace</code> <p>Override this method to provide <code>Trace.get_subtrace</code> support</p> <code>get_retval</code> <p>Returns the <code>R</code> from the <code>GenerativeFunction</code> invocation which created the <code>Trace</code>.</p> <code>get_score</code> <p>Return the <code>Score</code> of the <code>Trace</code>.</p> <code>get_subtrace</code> <p>Return the subtrace having the supplied address. Specifying multiple addresses</p> <code>update</code> <p>This method calls out to the underlying <code>GenerativeFunction.edit</code> method - see <code>EditRequest</code> and <code>edit</code> for more information.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>class Trace(Generic[R], Pytree):\n    \"\"\"\n    `Trace` is the type of traces of generative functions.\n\n    A trace is a data structure used to represent sampled executions of\n    generative functions. Traces track metadata associated with the probabilities\n    of choices, as well as other data associated with\n    the invocation of a generative function, including the arguments it\n    was invoked with, its return value, and the identity of the generative function itself.\n    \"\"\"\n\n    @abstractmethod\n    def get_args(self) -&gt; Arguments:\n        \"\"\"Returns the [`Arguments`][genjax.core.Arguments] for the [`GenerativeFunction`][genjax.core.GenerativeFunction] invocation which created the [`Trace`][genjax.core.Trace].\"\"\"\n\n    @abstractmethod\n    def get_retval(self) -&gt; R:\n        \"\"\"Returns the `R` from the [`GenerativeFunction`][genjax.core.GenerativeFunction] invocation which created the [`Trace`][genjax.core.Trace].\"\"\"\n\n    @abstractmethod\n    def get_score(self) -&gt; Score:\n        \"\"\"Return the [`Score`][genjax.core.Score] of the `Trace`.\n\n        The score must satisfy a particular mathematical specification: it's either an exact density evaluation of $P$ (the distribution over samples) for the sample returned by [`genjax.Trace.get_choices`][], or _a sample from an estimator_ (a density estimate) if the generative function contains _untraced randomness_.\n\n        Let $s$ be the score, $t$ the sample, and $a$ the arguments: when the generative function contains no _untraced randomness_, the score (in logspace) is given by:\n\n        $$\n        \\\\log s := \\\\log P(t; a)\n        $$\n\n        (**With untraced randomness**) Gen allows for the possibility of sources of randomness _which are not traced_. When these sources are included in generative computations, the score is defined so that the following property holds:\n\n        $$\n        \\\\mathbb{E}_{r\\\\sim~P(r | t; a)}\\\\big[\\\\frac{1}{s}\\\\big] = \\\\frac{1}{P(t; a)}\n        $$\n\n        This property is the one you'd want to be true if you were using a generative function with untraced randomness _as a proposal_ in a routine which uses importance sampling, for instance.\n\n        In GenJAX, one way you might encounter this is by using pseudo-random routines in your modeling code:\n        ```python\n        # notice how the key is explicit\n        @genjax.gen\n        def model_with_untraced_randomness(key: PRNGKey):\n            x = genjax.normal(0.0, 1.0) \"x\"\n            v = some_random_process(key, x)\n            y = genjax.normal(v, 1.0) @ \"y\"\n        ```\n\n        In this case, the score (in logspace) is given by:\n\n        $$\n        \\\\log s := \\\\log P(r, t; a) - \\\\log Q(r; a)\n        $$\n\n        which satisfies the requirement by virtue of the fact:\n\n        $$\n        \\\\begin{aligned}\n        \\\\mathbb{E}_{r\\\\sim~P(r | t; a)}\\\\big[\\\\frac{1}{s}\\\\big] &amp;= \\\\mathbb{E}_{r\\\\sim P(r | t; a)}\\\\big[\\\\frac{Q(r; a)}{P(r, t; a)} \\\\big] \\\\\\\\ &amp;= \\\\frac{1}{P(t; a)} \\\\mathbb{E}_{r\\\\sim P(r | t; a)}\\\\big[\\\\frac{Q(r; a)}{P(r | t; a)}\\\\big] \\\\\\\\\n        &amp;= \\\\frac{1}{P(t; a)}\n        \\\\end{aligned}\n        $$\n\n        \"\"\"\n\n    @abstractmethod\n    def get_choices(self) -&gt; \"genjax.ChoiceMap\":\n        \"\"\"Retrieves the random choices made in a trace in the form of a [`genjax.ChoiceMap`][].\"\"\"\n        pass\n\n    @nobeartype\n    @deprecated(reason=\"Use .get_choices() instead.\", version=\"0.8.1\")\n    def get_sample(self):\n        return self.get_choices()\n\n    @abstractmethod\n    def get_gen_fn(self) -&gt; \"GenerativeFunction[R]\":\n        \"\"\"Returns the [`GenerativeFunction`][genjax.core.GenerativeFunction] whose invocation created the [`Trace`][genjax.core.Trace].\"\"\"\n        pass\n\n    def edit(\n        self,\n        key: PRNGKey,\n        request: EditRequest,\n        argdiffs: tuple[Any, ...] | None = None,\n    ) -&gt; tuple[Self, Weight, Retdiff[R], EditRequest]:\n        \"\"\"\n        This method calls out to the underlying [`GenerativeFunction.edit`][genjax.core.GenerativeFunction.edit] method - see [`EditRequest`][genjax.core.EditRequest] and [`edit`][genjax.core.GenerativeFunction.edit] for more information.\n        \"\"\"\n        return request.edit(\n            key,\n            self,\n            Diff.no_change(self.get_args()) if argdiffs is None else argdiffs,\n        )  # pyright: ignore[reportReturnType]\n\n    def update(\n        self,\n        key: PRNGKey,\n        constraint: ChoiceMap,\n        argdiffs: tuple[Any, ...] | None = None,\n    ) -&gt; tuple[Self, Weight, Retdiff[R], ChoiceMap]:\n        \"\"\"\n        This method calls out to the underlying [`GenerativeFunction.edit`][genjax.core.GenerativeFunction.edit] method - see [`EditRequest`][genjax.core.EditRequest] and [`edit`][genjax.core.GenerativeFunction.edit] for more information.\n        \"\"\"\n        return self.get_gen_fn().update(\n            key,\n            self,\n            constraint,\n            Diff.no_change(self.get_args()) if argdiffs is None else argdiffs,\n        )  # pyright: ignore[reportReturnType]\n\n    def project(\n        self,\n        key: PRNGKey,\n        selection: Selection,\n    ) -&gt; Weight:\n        gen_fn = self.get_gen_fn()\n        return gen_fn.project(\n            key,\n            self,\n            selection,\n        )\n\n    def get_subtrace(self, *addresses: Address) -&gt; \"Trace[Any]\":\n        \"\"\"\n        Return the subtrace having the supplied address. Specifying multiple addresses\n        will apply the operation recursively.\n\n        GenJAX does not guarantee the validity of any inference computations performed\n        using information from the returned subtrace. In other words, it is safe to\n        inspect the data of subtraces -- but it not safe to use that data to make decisions\n        about inference. This is true of all the methods on the subtrace, including\n        `Trace.get_args`, `Trace.get_score`, `Trace.get_retval`, etc. It is safe to look,\n        but don't use the data for non-trivial things!\"\"\"\n\n        return functools.reduce(\n            lambda tr, addr: tr.get_inner_trace(addr), addresses, self\n        )\n\n    def get_inner_trace(self, _address: Address) -&gt; \"Trace[Any]\":\n        \"\"\"Override this method to provide `Trace.get_subtrace` support\n        for those trace types that have substructure that can be addressed\n        in this way.\n\n        NOTE: `get_inner_trace` takes a full `Address` because, unlike `ChoiceMap`, if a user traces to a tupled address like (\"a\", \"b\"), then the resulting `StaticTrace` will store a sub-trace at this address, vs flattening it out.\n\n        As a result, `tr.get_inner_trace((\"a\", \"b\"))` does not equal `tr.get_inner_trace(\"a\").get_inner_trace(\"b\")`.\"\"\"\n        raise NotImplementedError(\n            \"This type of Trace object does not possess subtraces.\"\n        )\n\n    ###################\n    # Batch semantics #\n    ###################\n\n    @property\n    def batch_shape(self):\n        return len(self.get_score())\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.edit","title":"edit","text":"<pre><code>edit(\n    key: PRNGKey,\n    request: EditRequest,\n    argdiffs: tuple[Any, ...] | None = None,\n) -&gt; tuple[Self, Weight, Retdiff[R], EditRequest]\n</code></pre> <p>This method calls out to the underlying <code>GenerativeFunction.edit</code> method - see <code>EditRequest</code> and <code>edit</code> for more information.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def edit(\n    self,\n    key: PRNGKey,\n    request: EditRequest,\n    argdiffs: tuple[Any, ...] | None = None,\n) -&gt; tuple[Self, Weight, Retdiff[R], EditRequest]:\n    \"\"\"\n    This method calls out to the underlying [`GenerativeFunction.edit`][genjax.core.GenerativeFunction.edit] method - see [`EditRequest`][genjax.core.EditRequest] and [`edit`][genjax.core.GenerativeFunction.edit] for more information.\n    \"\"\"\n    return request.edit(\n        key,\n        self,\n        Diff.no_change(self.get_args()) if argdiffs is None else argdiffs,\n    )  # pyright: ignore[reportReturnType]\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_args","title":"get_args  <code>abstractmethod</code>","text":"<pre><code>get_args() -&gt; Arguments\n</code></pre> <p>Returns the <code>Arguments</code> for the <code>GenerativeFunction</code> invocation which created the <code>Trace</code>.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef get_args(self) -&gt; Arguments:\n    \"\"\"Returns the [`Arguments`][genjax.core.Arguments] for the [`GenerativeFunction`][genjax.core.GenerativeFunction] invocation which created the [`Trace`][genjax.core.Trace].\"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_choices","title":"get_choices  <code>abstractmethod</code>","text":"<pre><code>get_choices() -&gt; ChoiceMap\n</code></pre> <p>Retrieves the random choices made in a trace in the form of a <code>genjax.ChoiceMap</code>.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef get_choices(self) -&gt; \"genjax.ChoiceMap\":\n    \"\"\"Retrieves the random choices made in a trace in the form of a [`genjax.ChoiceMap`][].\"\"\"\n    pass\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_gen_fn","title":"get_gen_fn  <code>abstractmethod</code>","text":"<pre><code>get_gen_fn() -&gt; GenerativeFunction[R]\n</code></pre> <p>Returns the <code>GenerativeFunction</code> whose invocation created the <code>Trace</code>.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef get_gen_fn(self) -&gt; \"GenerativeFunction[R]\":\n    \"\"\"Returns the [`GenerativeFunction`][genjax.core.GenerativeFunction] whose invocation created the [`Trace`][genjax.core.Trace].\"\"\"\n    pass\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_inner_trace","title":"get_inner_trace","text":"<pre><code>get_inner_trace(_address: Address) -&gt; Trace[Any]\n</code></pre> <p>Override this method to provide <code>Trace.get_subtrace</code> support for those trace types that have substructure that can be addressed in this way.</p> <p>NOTE: <code>get_inner_trace</code> takes a full <code>Address</code> because, unlike <code>ChoiceMap</code>, if a user traces to a tupled address like (\"a\", \"b\"), then the resulting <code>StaticTrace</code> will store a sub-trace at this address, vs flattening it out.</p> <p>As a result, <code>tr.get_inner_trace((\"a\", \"b\"))</code> does not equal <code>tr.get_inner_trace(\"a\").get_inner_trace(\"b\")</code>.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def get_inner_trace(self, _address: Address) -&gt; \"Trace[Any]\":\n    \"\"\"Override this method to provide `Trace.get_subtrace` support\n    for those trace types that have substructure that can be addressed\n    in this way.\n\n    NOTE: `get_inner_trace` takes a full `Address` because, unlike `ChoiceMap`, if a user traces to a tupled address like (\"a\", \"b\"), then the resulting `StaticTrace` will store a sub-trace at this address, vs flattening it out.\n\n    As a result, `tr.get_inner_trace((\"a\", \"b\"))` does not equal `tr.get_inner_trace(\"a\").get_inner_trace(\"b\")`.\"\"\"\n    raise NotImplementedError(\n        \"This type of Trace object does not possess subtraces.\"\n    )\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_retval","title":"get_retval  <code>abstractmethod</code>","text":"<pre><code>get_retval() -&gt; R\n</code></pre> <p>Returns the <code>R</code> from the <code>GenerativeFunction</code> invocation which created the <code>Trace</code>.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef get_retval(self) -&gt; R:\n    \"\"\"Returns the `R` from the [`GenerativeFunction`][genjax.core.GenerativeFunction] invocation which created the [`Trace`][genjax.core.Trace].\"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_score","title":"get_score  <code>abstractmethod</code>","text":"<pre><code>get_score() -&gt; Score\n</code></pre> <p>Return the <code>Score</code> of the <code>Trace</code>.</p> <p>The score must satisfy a particular mathematical specification: it's either an exact density evaluation of \\(P\\) (the distribution over samples) for the sample returned by <code>genjax.Trace.get_choices</code>, or a sample from an estimator (a density estimate) if the generative function contains untraced randomness.</p> <p>Let \\(s\\) be the score, \\(t\\) the sample, and \\(a\\) the arguments: when the generative function contains no untraced randomness, the score (in logspace) is given by:</p> \\[ \\log s := \\log P(t; a) \\] <p>(With untraced randomness) Gen allows for the possibility of sources of randomness which are not traced. When these sources are included in generative computations, the score is defined so that the following property holds:</p> \\[ \\mathbb{E}_{r\\sim~P(r | t; a)}\\big[\\frac{1}{s}\\big] = \\frac{1}{P(t; a)} \\] <p>This property is the one you'd want to be true if you were using a generative function with untraced randomness as a proposal in a routine which uses importance sampling, for instance.</p> <p>In GenJAX, one way you might encounter this is by using pseudo-random routines in your modeling code: <pre><code># notice how the key is explicit\n@genjax.gen\ndef model_with_untraced_randomness(key: PRNGKey):\n    x = genjax.normal(0.0, 1.0) \"x\"\n    v = some_random_process(key, x)\n    y = genjax.normal(v, 1.0) @ \"y\"\n</code></pre></p> <p>In this case, the score (in logspace) is given by:</p> \\[ \\log s := \\log P(r, t; a) - \\log Q(r; a) \\] <p>which satisfies the requirement by virtue of the fact:</p> \\[ \\begin{aligned} \\mathbb{E}_{r\\sim~P(r | t; a)}\\big[\\frac{1}{s}\\big] &amp;= \\mathbb{E}_{r\\sim P(r | t; a)}\\big[\\frac{Q(r; a)}{P(r, t; a)} \\big] \\\\ &amp;= \\frac{1}{P(t; a)} \\mathbb{E}_{r\\sim P(r | t; a)}\\big[\\frac{Q(r; a)}{P(r | t; a)}\\big] \\\\ &amp;= \\frac{1}{P(t; a)} \\end{aligned} \\] Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>@abstractmethod\ndef get_score(self) -&gt; Score:\n    \"\"\"Return the [`Score`][genjax.core.Score] of the `Trace`.\n\n    The score must satisfy a particular mathematical specification: it's either an exact density evaluation of $P$ (the distribution over samples) for the sample returned by [`genjax.Trace.get_choices`][], or _a sample from an estimator_ (a density estimate) if the generative function contains _untraced randomness_.\n\n    Let $s$ be the score, $t$ the sample, and $a$ the arguments: when the generative function contains no _untraced randomness_, the score (in logspace) is given by:\n\n    $$\n    \\\\log s := \\\\log P(t; a)\n    $$\n\n    (**With untraced randomness**) Gen allows for the possibility of sources of randomness _which are not traced_. When these sources are included in generative computations, the score is defined so that the following property holds:\n\n    $$\n    \\\\mathbb{E}_{r\\\\sim~P(r | t; a)}\\\\big[\\\\frac{1}{s}\\\\big] = \\\\frac{1}{P(t; a)}\n    $$\n\n    This property is the one you'd want to be true if you were using a generative function with untraced randomness _as a proposal_ in a routine which uses importance sampling, for instance.\n\n    In GenJAX, one way you might encounter this is by using pseudo-random routines in your modeling code:\n    ```python\n    # notice how the key is explicit\n    @genjax.gen\n    def model_with_untraced_randomness(key: PRNGKey):\n        x = genjax.normal(0.0, 1.0) \"x\"\n        v = some_random_process(key, x)\n        y = genjax.normal(v, 1.0) @ \"y\"\n    ```\n\n    In this case, the score (in logspace) is given by:\n\n    $$\n    \\\\log s := \\\\log P(r, t; a) - \\\\log Q(r; a)\n    $$\n\n    which satisfies the requirement by virtue of the fact:\n\n    $$\n    \\\\begin{aligned}\n    \\\\mathbb{E}_{r\\\\sim~P(r | t; a)}\\\\big[\\\\frac{1}{s}\\\\big] &amp;= \\\\mathbb{E}_{r\\\\sim P(r | t; a)}\\\\big[\\\\frac{Q(r; a)}{P(r, t; a)} \\\\big] \\\\\\\\ &amp;= \\\\frac{1}{P(t; a)} \\\\mathbb{E}_{r\\\\sim P(r | t; a)}\\\\big[\\\\frac{Q(r; a)}{P(r | t; a)}\\\\big] \\\\\\\\\n    &amp;= \\\\frac{1}{P(t; a)}\n    \\\\end{aligned}\n    $$\n\n    \"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.get_subtrace","title":"get_subtrace","text":"<pre><code>get_subtrace(*addresses: Address) -&gt; Trace[Any]\n</code></pre> <p>Return the subtrace having the supplied address. Specifying multiple addresses will apply the operation recursively.</p> <p>GenJAX does not guarantee the validity of any inference computations performed using information from the returned subtrace. In other words, it is safe to inspect the data of subtraces -- but it not safe to use that data to make decisions about inference. This is true of all the methods on the subtrace, including <code>Trace.get_args</code>, <code>Trace.get_score</code>, <code>Trace.get_retval</code>, etc. It is safe to look, but don't use the data for non-trivial things!</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def get_subtrace(self, *addresses: Address) -&gt; \"Trace[Any]\":\n    \"\"\"\n    Return the subtrace having the supplied address. Specifying multiple addresses\n    will apply the operation recursively.\n\n    GenJAX does not guarantee the validity of any inference computations performed\n    using information from the returned subtrace. In other words, it is safe to\n    inspect the data of subtraces -- but it not safe to use that data to make decisions\n    about inference. This is true of all the methods on the subtrace, including\n    `Trace.get_args`, `Trace.get_score`, `Trace.get_retval`, etc. It is safe to look,\n    but don't use the data for non-trivial things!\"\"\"\n\n    return functools.reduce(\n        lambda tr, addr: tr.get_inner_trace(addr), addresses, self\n    )\n</code></pre>"},{"location":"library/core.html#genjax.core.Trace.update","title":"update","text":"<pre><code>update(\n    key: PRNGKey,\n    constraint: ChoiceMap,\n    argdiffs: tuple[Any, ...] | None = None,\n) -&gt; tuple[Self, Weight, Retdiff[R], ChoiceMap]\n</code></pre> <p>This method calls out to the underlying <code>GenerativeFunction.edit</code> method - see <code>EditRequest</code> and <code>edit</code> for more information.</p> Source code in <code>src/genjax/_src/core/generative/generative_function.py</code> <pre><code>def update(\n    self,\n    key: PRNGKey,\n    constraint: ChoiceMap,\n    argdiffs: tuple[Any, ...] | None = None,\n) -&gt; tuple[Self, Weight, Retdiff[R], ChoiceMap]:\n    \"\"\"\n    This method calls out to the underlying [`GenerativeFunction.edit`][genjax.core.GenerativeFunction.edit] method - see [`EditRequest`][genjax.core.EditRequest] and [`edit`][genjax.core.GenerativeFunction.edit] for more information.\n    \"\"\"\n    return self.get_gen_fn().update(\n        key,\n        self,\n        constraint,\n        Diff.no_change(self.get_args()) if argdiffs is None else argdiffs,\n    )  # pyright: ignore[reportReturnType]\n</code></pre>"},{"location":"library/core.html#genjax.core.EditRequest","title":"genjax.core.EditRequest","text":"<p>               Bases: <code>Pytree</code></p> <p>An <code>EditRequest</code> is a request to edit a trace of a generative function. Generative functions respond to instances of subtypes of <code>EditRequest</code> by providing an <code>edit</code> implementation.</p> <p>Updating a trace is a common operation in inference processes, but naively mutating the trace will invalidate the mathematical invariants that Gen retains. <code>EditRequest</code> instances denote requests for SMC moves in the framework of SMCP3, which preserve these invariants.</p> Source code in <code>src/genjax/_src/core/generative/concepts.py</code> <pre><code>class EditRequest(Pytree):\n    \"\"\"\n    An `EditRequest` is a request to edit a trace of a generative function. Generative functions respond to instances of subtypes of `EditRequest` by providing an [`edit`][genjax.core.GenerativeFunction.edit] implementation.\n\n    Updating a trace is a common operation in inference processes, but naively mutating the trace will invalidate the mathematical invariants that Gen retains. `EditRequest` instances denote requests for _SMC moves_ in the framework of [SMCP3](https://proceedings.mlr.press/v206/lew23a.html), which preserve these invariants.\n    \"\"\"\n\n    @abstractmethod\n    def edit(\n        self,\n        key: PRNGKey,\n        tr: \"genjax.Trace[R]\",\n        argdiffs: Argdiffs,\n    ) -&gt; \"tuple[genjax.Trace[R], Weight, Retdiff[R], EditRequest]\":\n        pass\n\n    def dimap(\n        self,\n        /,\n        *,\n        pre: Callable[[Argdiffs], Argdiffs] = lambda v: v,\n        post: Callable[[Retdiff[R]], Retdiff[R]] = lambda v: v,\n    ) -&gt; \"genjax.DiffAnnotate[Self]\":\n        from genjax import DiffAnnotate\n\n        return DiffAnnotate(self, argdiff_fn=pre, retdiff_fn=post)\n\n    def map(\n        self,\n        post: Callable[[Retdiff[R]], Retdiff[R]],\n    ) -&gt; \"genjax.DiffAnnotate[Self]\":\n        return self.dimap(post=post)\n\n    def contramap(\n        self,\n        pre: Callable[[Argdiffs], Argdiffs],\n    ) -&gt; \"genjax.DiffAnnotate[Self]\":\n        return self.dimap(pre=pre)\n</code></pre>"},{"location":"library/core.html#generative-functions-with-addressed-random-choices","title":"Generative functions with addressed random choices","text":"<p>Generative functions will often include addressed random choices. These are random choices which are given a name via an addressing syntax, and can be accessed by name via extended interfaces on the <code>ChoiceMap</code> type which supports the addressing.</p>"},{"location":"library/core.html#genjax.core.ChoiceMap","title":"genjax.core.ChoiceMap","text":"<p>               Bases: <code>Pytree</code></p> <p>The type <code>ChoiceMap</code> denotes a map-like value which can be sampled from generative functions.</p> <p>Generative functions which utilize <code>ChoiceMap</code> as their sample representation typically support a notion of addressing for the random choices they make. <code>ChoiceMap</code> stores addressed random choices, and provides a data language for querying and manipulating these choices.</p> <p>Examples:</p> <p>(Making choice maps) Choice maps can be constructed using the <code>ChoiceMapBuilder</code> interface <pre><code>from genjax import ChoiceMapBuilder as C\n\nchm = C[\"x\"].set(3.0)\nprint(chm.render_html())\n</code></pre> </p> <p>(Getting submaps) Hierarchical choice maps support <code>__call__</code>, which allows for the retrieval of submaps at addresses: <pre><code>from genjax import ChoiceMapBuilder as C\n\nchm = C[\"x\", \"y\"].set(3.0)\nsubmap = chm(\"x\")\nprint(submap.render_html())\n</code></pre> </p> <p>(Getting values) Choice maps support <code>__getitem__</code>, which allows for the retrieval of values at addresses: <pre><code>from genjax import ChoiceMapBuilder as C\n\nchm = C[\"x\", \"y\"].set(3.0)\nvalue = chm[\"x\", \"y\"]\nprint(value)\n</code></pre>  3.0  </p> <p>(Making vectorized choice maps) Choice maps can be constructed using <code>jax.vmap</code>: <pre><code>from genjax import ChoiceMapBuilder as C\nfrom jax import vmap\nimport jax.numpy as jnp\n\nvec_chm = vmap(lambda idx, v: C[\"x\", idx].set(v))(jnp.arange(10), jnp.ones(10))\nprint(vec_chm.render_html())\n</code></pre> </p> <p>Methods:</p> Name Description <code>__call__</code> <p>Alias for <code>get_submap(*addresses)</code>.</p> <code>choice</code> <p>Creates a ChoiceMap containing a single value.</p> <code>d</code> <p>Creates a ChoiceMap from a dictionary.</p> <code>empty</code> <p>Returns a ChoiceMap with no values or submaps.</p> <code>entry</code> <p>Creates a ChoiceMap with a single value at a specified address.</p> <code>extend</code> <p>Returns a new ChoiceMap with the given address component as its root.</p> <code>filter</code> <p>Filter the choice map on the <code>Selection</code>. The resulting choice map only contains the addresses that return True when presented to the selection.</p> <code>from_mapping</code> <p>Creates a ChoiceMap from an iterable of address-value pairs.</p> <code>get_selection</code> <p>Returns a Selection representing the structure of this ChoiceMap.</p> <code>invalid_subset</code> <p>Identifies the subset of choices that are invalid for a given generative function and its arguments.</p> <code>kw</code> <p>Creates a ChoiceMap from keyword arguments.</p> <code>mask</code> <p>Returns a new ChoiceMap with values masked by a boolean flag.</p> <code>merge</code> <p>Merges this ChoiceMap with another ChoiceMap.</p> <code>simplify</code> <p>Previously pushed down filters, now acts as identity.</p> <code>static_is_empty</code> <p>Returns True if this ChoiceMap is equal to <code>ChoiceMap.empty()</code>, False otherwise.</p> <code>switch</code> <p>Creates a ChoiceMap that switches between multiple ChoiceMaps based on an index.</p> <p>Attributes:</p> Name Type Description <code>at</code> <code>_ChoiceMapBuilder</code> <p>Returns a _ChoiceMapBuilder instance for constructing nested ChoiceMaps.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>class ChoiceMap(Pytree):\n    \"\"\"The type `ChoiceMap` denotes a map-like value which can be sampled from\n    generative functions.\n\n    Generative functions which utilize `ChoiceMap` as their sample representation typically support a notion of _addressing_ for the random choices they make. `ChoiceMap` stores addressed random choices, and provides a data language for querying and manipulating these choices.\n\n    Examples:\n        (**Making choice maps**) Choice maps can be constructed using the `ChoiceMapBuilder` interface\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMapBuilder as C\n\n        chm = C[\"x\"].set(3.0)\n        print(chm.render_html())\n        ```\n\n        (**Getting submaps**) Hierarchical choice maps support `__call__`, which allows for the retrieval of _submaps_ at addresses:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMapBuilder as C\n\n        chm = C[\"x\", \"y\"].set(3.0)\n        submap = chm(\"x\")\n        print(submap.render_html())\n        ```\n\n        (**Getting values**) Choice maps support `__getitem__`, which allows for the retrieval of _values_ at addresses:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMapBuilder as C\n\n        chm = C[\"x\", \"y\"].set(3.0)\n        value = chm[\"x\", \"y\"]\n        print(value)\n        ```\n\n        (**Making vectorized choice maps**) Choice maps can be constructed using `jax.vmap`:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMapBuilder as C\n        from jax import vmap\n        import jax.numpy as jnp\n\n        vec_chm = vmap(lambda idx, v: C[\"x\", idx].set(v))(jnp.arange(10), jnp.ones(10))\n        print(vec_chm.render_html())\n        ```\n    \"\"\"\n\n    #######################\n    # Map-like interfaces #\n    #######################\n\n    @abstractmethod\n    def filter(self, selection: Selection | Flag) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Filter the choice map on the `Selection`. The resulting choice map only contains the addresses that return True when presented to the selection.\n\n        Args:\n            selection: The Selection to filter the choice map with.\n\n        Returns:\n            A new ChoiceMap containing only the addresses selected by the given Selection.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            import jax\n            import genjax\n            from genjax import bernoulli\n            from genjax import SelectionBuilder as S\n\n\n            @genjax.gen\n            def model():\n                x = bernoulli(0.3) @ \"x\"\n                y = bernoulli(0.3) @ \"y\"\n                return x\n\n\n            key = jax.random.key(314159)\n            tr = model.simulate(key, ())\n            chm = tr.get_choices()\n            selection = S[\"x\"]\n            filtered = chm.filter(selection)\n            assert \"y\" not in filtered\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def get_value(self) -&gt; Any:\n        pass\n\n    @abstractmethod\n    def get_inner_map(\n        self,\n        addr: AddressComponent,\n    ) -&gt; \"ChoiceMap\":\n        pass\n\n    def get_submap(self, *addresses: Address) -&gt; \"ChoiceMap\":\n        addr = tuple(\n            label for a in addresses for label in (a if isinstance(a, tuple) else (a,))\n        )\n        addr: tuple[AddressComponent, ...] = _validate_addr(\n            addr, allow_partial_slice=True\n        )\n        return functools.reduce(lambda chm, addr: chm.get_inner_map(addr), addr, self)\n\n    def has_value(self) -&gt; bool:\n        return self.get_value() is not None\n\n    ######################################\n    # Convenient syntax for construction #\n    ######################################\n\n    builder: Final[_ChoiceMapBuilder] = _ChoiceMapBuilder(None, [])\n\n    @staticmethod\n    def empty() -&gt; \"ChoiceMap\":\n        \"\"\"\n        Returns a ChoiceMap with no values or submaps.\n\n        Returns:\n            An empty ChoiceMap.\n        \"\"\"\n        return _empty\n\n    @staticmethod\n    def choice(v: Any) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap containing a single value.\n\n        This method creates and returns an instance of Choice, which represents\n        a ChoiceMap with a single value at the root level.\n\n        Args:\n            v: The value to be stored in the ChoiceMap.\n\n        Returns:\n            A ChoiceMap containing the single value.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            from genjax import ChoiceMap\n\n            value_chm = ChoiceMap.value(42)\n            assert value_chm.get_value() == 42\n            ```\n        \"\"\"\n        return Choice.build(v)\n\n    @staticmethod\n    @nobeartype\n    @deprecated(\"Use ChoiceMap.choice() instead.\")\n    def value(v: Any) -&gt; \"ChoiceMap\":\n        return ChoiceMap.choice(v)\n\n    @staticmethod\n    def entry(\n        v: \"dict[K_addr, Any] | ChoiceMap | Any\", *addrs: AddressComponent\n    ) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap with a single value at a specified address.\n\n        This method creates and returns a ChoiceMap with a new ChoiceMap stored at\n        the given address.\n\n        - if the provided value is already a ChoiceMap, it will be used directly;\n        - `dict` values will be passed to `ChoiceMap.d`;\n        - any other value will be passed to `ChoiceMap.value`.\n\n        Args:\n            v: The value to be stored in the ChoiceMap. Can be any value, a dict or a ChoiceMap.\n            addrs: The address at which to store the value. Can be a static or dynamic address component.\n\n        Returns:\n            A ChoiceMap with the value stored at the specified address.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            import genjax\n            import jax.numpy as jnp\n\n            # Using an existing ChoiceMap\n            nested_chm = ChoiceMap.entry(ChoiceMap.value(42), \"x\")\n            assert nested_chm[\"x\"] == 42\n\n            # Using a dict generates a new `ChoiceMap.d` call\n            nested_chm = ChoiceMap.entry({\"y\": 42}, \"x\")\n            assert nested_chm[\"x\", \"y\"] == 42\n\n            # Static address\n            static_chm = ChoiceMap.entry(42, \"x\")\n            assert static_chm[\"x\"] == 42\n\n            # Dynamic address\n            dynamic_chm = ChoiceMap.entry(\n                jnp.array([1.1, 2.2, 3.3]), jnp.array([1, 2, 3])\n            )\n            assert dynamic_chm[1] == genjax.Mask(1.1, True)\n            ```\n        \"\"\"\n        if isinstance(v, ChoiceMap):\n            chm = v\n        elif isinstance(v, dict):\n            chm = ChoiceMap.d(v)\n        else:\n            chm = ChoiceMap.choice(v)\n\n        return chm.extend(*addrs)\n\n    @staticmethod\n    def from_mapping(pairs: Iterable[tuple[K_addr, Any]]) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap from an iterable of address-value pairs.\n\n        This method constructs a ChoiceMap by iterating through the provided pairs,\n        where each pair consists of an address (or address component) and a corresponding value.\n        The resulting ChoiceMap will contain all the values at their respective addresses.\n\n        Args:\n            pairs: An iterable of tuples, where each tuple contains an address (or address component) and its corresponding value. The address can be a single component or a tuple of components.\n\n        Returns:\n            A ChoiceMap containing all the address-value pairs from the input.\n\n        Example:\n            ```python\n            pairs = [(\"x\", 42), ((\"y\", \"z\"), 10), (\"w\", [1, 2, 3])]\n            chm = ChoiceMap.from_mapping(pairs)\n            assert chm[\"x\"] == 42\n            assert chm[\"y\", \"z\"] == 10\n            assert chm[\"w\"] == [1, 2, 3]\n            ```\n\n        Note:\n            If multiple pairs have the same address, later pairs will overwrite earlier ones.\n        \"\"\"\n        acc = ChoiceMap.empty()\n\n        for addr, v in pairs:\n            addr = addr if isinstance(addr, tuple) else (addr,)\n            acc |= ChoiceMap.entry(v, *addr)\n\n        return acc\n\n    @staticmethod\n    def d(d: dict[K_addr, Any]) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap from a dictionary.\n\n        This method creates and returns a ChoiceMap based on the key-value pairs in the provided dictionary. Each key in the dictionary becomes an address in the ChoiceMap, and the corresponding value is stored at that address.\n\n        Dict-shaped values are recursively converted to ChoiceMap instances.\n\n        Args:\n            d: A dictionary where keys are addresses and values are the corresponding data to be stored in the ChoiceMap.\n\n        Returns:\n            A ChoiceMap containing the key-value pairs from the input dictionary.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            from genjax import ChoiceMap\n\n            dict_chm = ChoiceMap.d({\"x\": 42, \"y\": {\"z\": [1, 2, 3]}})\n            assert dict_chm[\"x\"] == 42\n            assert dict_chm[\"y\", \"z\"] == [1, 2, 3]\n            ```\n        \"\"\"\n        return ChoiceMap.from_mapping(d.items())\n\n    @staticmethod\n    def kw(**kwargs) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap from keyword arguments.\n\n        This method creates and returns a ChoiceMap based on the provided keyword arguments.\n        Each keyword argument becomes an address in the ChoiceMap, and its value is stored at that address.\n\n        Dict-shaped values are recursively converted to ChoiceMap instances with calls to `ChoiceMap.d`.\n\n        Returns:\n            A ChoiceMap containing the key-value pairs from the input keyword arguments.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            kw_chm = ChoiceMap.kw(x=42, y=[1, 2, 3], z={\"w\": 10.0})\n            assert kw_chm[\"x\"] == 42\n            assert kw_chm[\"y\"] == [1, 2, 3]\n            assert kw_chm[\"z\", \"w\"] == 10.0\n            ```\n        \"\"\"\n        return ChoiceMap.d(kwargs)\n\n    @staticmethod\n    def switch(idx: int | IntArray, chms: Iterable[\"ChoiceMap\"]) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Creates a ChoiceMap that switches between multiple ChoiceMaps based on an index.\n\n        This method creates a new ChoiceMap that selectively includes values from a sequence of\n        input ChoiceMaps based on the provided index. The resulting ChoiceMap will contain\n        values from the ChoiceMap at the position specified by the index, while masking out\n        values from all other ChoiceMaps.\n\n        Args:\n            idx: An index or array of indices specifying which ChoiceMap(s) to select from.\n            chms: An iterable of ChoiceMaps to switch between.\n\n        Returns:\n            A new ChoiceMap containing values from the selected ChoiceMap(s).\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            chm1 = ChoiceMap.d({\"x\": 1, \"y\": 2})\n            chm2 = ChoiceMap.d({\"x\": 3, \"y\": 4})\n            chm3 = ChoiceMap.d({\"x\": 5, \"y\": 6})\n\n            switched = ChoiceMap.switch(jnp.array(1), [chm1, chm2, chm3])\n            assert switched[\"x\"].unmask() == 3\n            assert switched[\"y\"].unmask() == 4\n            ```\n        \"\"\"\n        return Switch.build(idx, chms)\n\n    ######################\n    # Combinator methods #\n    ######################\n\n    def mask(self, flag: Flag) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Returns a new ChoiceMap with values masked by a boolean flag.\n\n        This method creates a new ChoiceMap where the values are conditionally\n        included based on the provided flag. If the flag is True, the original\n        values are retained; if False, the ChoiceMap behaves as if it's empty.\n\n        Args:\n            flag: A boolean flag determining whether to include the values.\n\n        Returns:\n            A new ChoiceMap with values conditionally masked.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            original_chm = ChoiceMap.value(42)\n            masked_chm = original_chm.mask(True)\n            assert masked_chm.get_value() == 42\n\n            masked_chm = original_chm.mask(False)\n            assert masked_chm.get_value() is None\n            ```\n        \"\"\"\n        return self.filter(flag)\n\n    def extend(self, *addrs: AddressComponent) -&gt; \"ChoiceMap\":\n        \"\"\"\n        Returns a new ChoiceMap with the given address component as its root.\n\n        This method creates a new ChoiceMap where the current ChoiceMap becomes a submap\n        under the specified address component. It effectively adds a new level of hierarchy\n        to the ChoiceMap structure.\n\n        Args:\n            addrs: The address components to use as the new root.\n\n        Returns:\n            A new ChoiceMap with the current ChoiceMap nested under the given address.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            original_chm = ChoiceMap.value(42)\n            indexed_chm = original_chm.extend(\"x\")\n            assert indexed_chm[\"x\"] == 42\n            ```\n        \"\"\"\n        acc = self\n        for addr in reversed(addrs):\n            if isinstance(addr, StaticAddressComponent):\n                acc = Static.build({addr: acc})\n            else:\n                acc = Indexed.build(acc, addr)\n\n        return acc\n\n    def merge(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        \"\"\"\n        Merges this ChoiceMap with another ChoiceMap.\n\n        This method combines the current ChoiceMap with another ChoiceMap using the XOR operation (^). It creates a new ChoiceMap that contains all addresses from both input ChoiceMaps; any overlapping addresses will trigger an error on access at the address via `[&lt;addr&gt;]` or `get_value()`. Use `|` if you don't want this behavior.\n\n        Args:\n            other: The ChoiceMap to merge with the current one.\n\n        Returns:\n            A new ChoiceMap resulting from the merge operation.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            chm1 = ChoiceMap.value(5).extend(\"x\")\n            chm2 = ChoiceMap.value(10).extend(\"y\")\n            merged_chm = chm1.merge(chm2)\n            assert merged_chm[\"x\"] == 5\n            assert merged_chm[\"y\"] == 10\n            ```\n\n        Note:\n            This method is equivalent to using the | operator between two ChoiceMaps.\n        \"\"\"\n        return self | other\n\n    def get_selection(self) -&gt; Selection:\n        \"\"\"\n        Returns a Selection representing the structure of this ChoiceMap.\n\n        This method creates a Selection that matches the hierarchical structure\n        of the current ChoiceMap. The resulting Selection can be used to filter\n        or query other ChoiceMaps with the same structure.\n\n        Returns:\n            A Selection object representing the structure of this ChoiceMap.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            chm = ChoiceMap.value(5).extend(\"x\")\n            sel = chm.get_selection()\n            assert sel[\"x\"] == True\n            assert sel[\"y\"] == False\n            ```\n        \"\"\"\n        return ChmSel.build(self)\n\n    def static_is_empty(self) -&gt; bool:\n        \"\"\"\n        Returns True if this ChoiceMap is equal to `ChoiceMap.empty()`, False otherwise.\n        \"\"\"\n        return False\n\n    ###########\n    # Dunders #\n    ###########\n\n    @nobeartype\n    @deprecated(\n        reason=\"^ is deprecated, please use | or _.merge(...) instead.\",\n        version=\"0.8.0\",\n    )\n    def __xor__(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        return self | other\n\n    def __or__(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        return Or.build(self, other)\n\n    def __and__(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        return other.filter(self.get_selection())\n\n    def __add__(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        return self | other\n\n    def __call__(\n        self,\n        *addresses: Address,\n    ) -&gt; \"ChoiceMap\":\n        \"\"\"Alias for `get_submap(*addresses)`.\"\"\"\n        return self.get_submap(*addresses)\n\n    def __getitem__(\n        self,\n        addr: Address,\n    ):\n        submap = self.get_submap(addr)\n        v = submap.get_value()\n        if v is None:\n            raise ChoiceMapNoValueAtAddress(addr)\n        else:\n            return v\n\n    def __contains__(\n        self,\n        addr: Address,\n    ) -&gt; bool:\n        return self.get_submap(addr).has_value()\n\n    @property\n    def at(self) -&gt; _ChoiceMapBuilder:\n        \"\"\"\n        Returns a _ChoiceMapBuilder instance for constructing nested ChoiceMaps.\n\n        This property allows for a fluent interface to build complex ChoiceMaps\n        by chaining address components and setting values.\n\n        Returns:\n            A builder object for constructing ChoiceMaps.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            from genjax import ChoiceMap\n\n            chm = ChoiceMap.d({(\"x\", \"y\"): 3.0, \"z\": 12.0})\n            updated = chm.at[\"x\", \"y\"].set(4.0)\n\n            assert updated[\"x\", \"y\"] == 4.0\n            assert updated[\"z\"] == chm[\"z\"]\n            ```\n        \"\"\"\n        return _ChoiceMapBuilder(self, [])\n\n    @nobeartype\n    @deprecated(\n        reason=\"Acts as identity; filters are now automatically pushed down.\",\n        version=\"0.8.0\",\n    )\n    def simplify(self) -&gt; \"ChoiceMap\":\n        \"\"\"Previously pushed down filters, now acts as identity.\"\"\"\n        return self\n\n    def invalid_subset(\n        self,\n        gen_fn: \"genjax.GenerativeFunction[Any]\",\n        args: tuple[Any, ...],\n    ) -&gt; \"ChoiceMap | None\":\n        \"\"\"\n        Identifies the subset of choices that are invalid for a given generative function and its arguments.\n\n        This method checks if all choices in the current ChoiceMap are valid for the given\n        generative function and its arguments.\n\n        Args:\n            gen_fn: The generative function to check against.\n            args: The arguments to the generative function.\n\n        Returns:\n            A ChoiceMap containing any extra choices not reachable in the course of `gen_fn`'s execution, or None if no extra choices are found.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            @genjax.gen\n            def model(x):\n                y = bernoulli(0.5) @ \"y\"\n                return x + y\n\n\n            chm = ChoiceMap.d({\"y\": 1, \"z\": 2})\n            extras = chm.invalid_subset(model, (1,))\n            assert \"z\" in extras  # \"z\" is an extra choice not in the model\n            ```\n        \"\"\"\n        shape_chm = gen_fn.get_zero_trace(*args).get_choices()\n        shape_sel = _shape_selection(shape_chm)\n        extras = self.filter(~shape_sel)\n        if not extras.static_is_empty():\n            return extras\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.at","title":"at  <code>property</code>","text":"<pre><code>at: _ChoiceMapBuilder\n</code></pre> <p>Returns a _ChoiceMapBuilder instance for constructing nested ChoiceMaps.</p> <p>This property allows for a fluent interface to build complex ChoiceMaps by chaining address components and setting values.</p> <p>Returns:</p> Type Description <code>_ChoiceMapBuilder</code> <p>A builder object for constructing ChoiceMaps.</p> Example <pre><code>from genjax import ChoiceMap\n\nchm = ChoiceMap.d({(\"x\", \"y\"): 3.0, \"z\": 12.0})\nupdated = chm.at[\"x\", \"y\"].set(4.0)\n\nassert updated[\"x\", \"y\"] == 4.0\nassert updated[\"z\"] == chm[\"z\"]\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.__call__","title":"__call__","text":"<pre><code>__call__(*addresses: Address) -&gt; ChoiceMap\n</code></pre> <p>Alias for <code>get_submap(*addresses)</code>.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def __call__(\n    self,\n    *addresses: Address,\n) -&gt; \"ChoiceMap\":\n    \"\"\"Alias for `get_submap(*addresses)`.\"\"\"\n    return self.get_submap(*addresses)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.choice","title":"choice  <code>staticmethod</code>","text":"<pre><code>choice(v: Any) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap containing a single value.</p> <p>This method creates and returns an instance of Choice, which represents a ChoiceMap with a single value at the root level.</p> <p>Parameters:</p> Name Type Description Default <code>Any</code> <p>The value to be stored in the ChoiceMap.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A ChoiceMap containing the single value.</p> Example <pre><code>from genjax import ChoiceMap\n\nvalue_chm = ChoiceMap.value(42)\nassert value_chm.get_value() == 42\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef choice(v: Any) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap containing a single value.\n\n    This method creates and returns an instance of Choice, which represents\n    a ChoiceMap with a single value at the root level.\n\n    Args:\n        v: The value to be stored in the ChoiceMap.\n\n    Returns:\n        A ChoiceMap containing the single value.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMap\n\n        value_chm = ChoiceMap.value(42)\n        assert value_chm.get_value() == 42\n        ```\n    \"\"\"\n    return Choice.build(v)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.choice(v)","title":"<code>v</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.d","title":"d  <code>staticmethod</code>","text":"<pre><code>d(d: dict[K_addr, Any]) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap from a dictionary.</p> <p>This method creates and returns a ChoiceMap based on the key-value pairs in the provided dictionary. Each key in the dictionary becomes an address in the ChoiceMap, and the corresponding value is stored at that address.</p> <p>Dict-shaped values are recursively converted to ChoiceMap instances.</p> <p>Parameters:</p> Name Type Description Default <code>dict[K_addr, Any]</code> <p>A dictionary where keys are addresses and values are the corresponding data to be stored in the ChoiceMap.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A ChoiceMap containing the key-value pairs from the input dictionary.</p> Example <pre><code>from genjax import ChoiceMap\n\ndict_chm = ChoiceMap.d({\"x\": 42, \"y\": {\"z\": [1, 2, 3]}})\nassert dict_chm[\"x\"] == 42\nassert dict_chm[\"y\", \"z\"] == [1, 2, 3]\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef d(d: dict[K_addr, Any]) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap from a dictionary.\n\n    This method creates and returns a ChoiceMap based on the key-value pairs in the provided dictionary. Each key in the dictionary becomes an address in the ChoiceMap, and the corresponding value is stored at that address.\n\n    Dict-shaped values are recursively converted to ChoiceMap instances.\n\n    Args:\n        d: A dictionary where keys are addresses and values are the corresponding data to be stored in the ChoiceMap.\n\n    Returns:\n        A ChoiceMap containing the key-value pairs from the input dictionary.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import ChoiceMap\n\n        dict_chm = ChoiceMap.d({\"x\": 42, \"y\": {\"z\": [1, 2, 3]}})\n        assert dict_chm[\"x\"] == 42\n        assert dict_chm[\"y\", \"z\"] == [1, 2, 3]\n        ```\n    \"\"\"\n    return ChoiceMap.from_mapping(d.items())\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.d(d)","title":"<code>d</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.empty","title":"empty  <code>staticmethod</code>","text":"<pre><code>empty() -&gt; ChoiceMap\n</code></pre> <p>Returns a ChoiceMap with no values or submaps.</p> <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>An empty ChoiceMap.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef empty() -&gt; \"ChoiceMap\":\n    \"\"\"\n    Returns a ChoiceMap with no values or submaps.\n\n    Returns:\n        An empty ChoiceMap.\n    \"\"\"\n    return _empty\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.entry","title":"entry  <code>staticmethod</code>","text":"<pre><code>entry(\n    v: dict[K_addr, Any] | ChoiceMap | Any,\n    *addrs: AddressComponent\n) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap with a single value at a specified address.</p> <p>This method creates and returns a ChoiceMap with a new ChoiceMap stored at the given address.</p> <ul> <li>if the provided value is already a ChoiceMap, it will be used directly;</li> <li><code>dict</code> values will be passed to <code>ChoiceMap.d</code>;</li> <li>any other value will be passed to <code>ChoiceMap.value</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dict[K_addr, Any] | ChoiceMap | Any</code> <p>The value to be stored in the ChoiceMap. Can be any value, a dict or a ChoiceMap.</p> required <code>AddressComponent</code> <p>The address at which to store the value. Can be a static or dynamic address component.</p> <code>()</code> <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A ChoiceMap with the value stored at the specified address.</p> Example <pre><code>import genjax\nimport jax.numpy as jnp\n\n# Using an existing ChoiceMap\nnested_chm = ChoiceMap.entry(ChoiceMap.value(42), \"x\")\nassert nested_chm[\"x\"] == 42\n\n# Using a dict generates a new `ChoiceMap.d` call\nnested_chm = ChoiceMap.entry({\"y\": 42}, \"x\")\nassert nested_chm[\"x\", \"y\"] == 42\n\n# Static address\nstatic_chm = ChoiceMap.entry(42, \"x\")\nassert static_chm[\"x\"] == 42\n\n# Dynamic address\ndynamic_chm = ChoiceMap.entry(\n    jnp.array([1.1, 2.2, 3.3]), jnp.array([1, 2, 3])\n)\nassert dynamic_chm[1] == genjax.Mask(1.1, True)\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef entry(\n    v: \"dict[K_addr, Any] | ChoiceMap | Any\", *addrs: AddressComponent\n) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap with a single value at a specified address.\n\n    This method creates and returns a ChoiceMap with a new ChoiceMap stored at\n    the given address.\n\n    - if the provided value is already a ChoiceMap, it will be used directly;\n    - `dict` values will be passed to `ChoiceMap.d`;\n    - any other value will be passed to `ChoiceMap.value`.\n\n    Args:\n        v: The value to be stored in the ChoiceMap. Can be any value, a dict or a ChoiceMap.\n        addrs: The address at which to store the value. Can be a static or dynamic address component.\n\n    Returns:\n        A ChoiceMap with the value stored at the specified address.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        import genjax\n        import jax.numpy as jnp\n\n        # Using an existing ChoiceMap\n        nested_chm = ChoiceMap.entry(ChoiceMap.value(42), \"x\")\n        assert nested_chm[\"x\"] == 42\n\n        # Using a dict generates a new `ChoiceMap.d` call\n        nested_chm = ChoiceMap.entry({\"y\": 42}, \"x\")\n        assert nested_chm[\"x\", \"y\"] == 42\n\n        # Static address\n        static_chm = ChoiceMap.entry(42, \"x\")\n        assert static_chm[\"x\"] == 42\n\n        # Dynamic address\n        dynamic_chm = ChoiceMap.entry(\n            jnp.array([1.1, 2.2, 3.3]), jnp.array([1, 2, 3])\n        )\n        assert dynamic_chm[1] == genjax.Mask(1.1, True)\n        ```\n    \"\"\"\n    if isinstance(v, ChoiceMap):\n        chm = v\n    elif isinstance(v, dict):\n        chm = ChoiceMap.d(v)\n    else:\n        chm = ChoiceMap.choice(v)\n\n    return chm.extend(*addrs)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.entry(v)","title":"<code>v</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.entry(addrs)","title":"<code>addrs</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.extend","title":"extend","text":"<pre><code>extend(*addrs: AddressComponent) -&gt; ChoiceMap\n</code></pre> <p>Returns a new ChoiceMap with the given address component as its root.</p> <p>This method creates a new ChoiceMap where the current ChoiceMap becomes a submap under the specified address component. It effectively adds a new level of hierarchy to the ChoiceMap structure.</p> <p>Parameters:</p> Name Type Description Default <code>AddressComponent</code> <p>The address components to use as the new root.</p> <code>()</code> <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap with the current ChoiceMap nested under the given address.</p> Example <pre><code>original_chm = ChoiceMap.value(42)\nindexed_chm = original_chm.extend(\"x\")\nassert indexed_chm[\"x\"] == 42\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def extend(self, *addrs: AddressComponent) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Returns a new ChoiceMap with the given address component as its root.\n\n    This method creates a new ChoiceMap where the current ChoiceMap becomes a submap\n    under the specified address component. It effectively adds a new level of hierarchy\n    to the ChoiceMap structure.\n\n    Args:\n        addrs: The address components to use as the new root.\n\n    Returns:\n        A new ChoiceMap with the current ChoiceMap nested under the given address.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        original_chm = ChoiceMap.value(42)\n        indexed_chm = original_chm.extend(\"x\")\n        assert indexed_chm[\"x\"] == 42\n        ```\n    \"\"\"\n    acc = self\n    for addr in reversed(addrs):\n        if isinstance(addr, StaticAddressComponent):\n            acc = Static.build({addr: acc})\n        else:\n            acc = Indexed.build(acc, addr)\n\n    return acc\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.extend(addrs)","title":"<code>addrs</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.filter","title":"filter  <code>abstractmethod</code>","text":"<pre><code>filter(selection: Selection | Flag) -&gt; ChoiceMap\n</code></pre> <p>Filter the choice map on the <code>Selection</code>. The resulting choice map only contains the addresses that return True when presented to the selection.</p> <p>Parameters:</p> Name Type Description Default <code>Selection | Flag</code> <p>The Selection to filter the choice map with.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap containing only the addresses selected by the given Selection.</p> <p>Examples:</p> <pre><code>import jax\nimport genjax\nfrom genjax import bernoulli\nfrom genjax import SelectionBuilder as S\n\n\n@genjax.gen\ndef model():\n    x = bernoulli(0.3) @ \"x\"\n    y = bernoulli(0.3) @ \"y\"\n    return x\n\n\nkey = jax.random.key(314159)\ntr = model.simulate(key, ())\nchm = tr.get_choices()\nselection = S[\"x\"]\nfiltered = chm.filter(selection)\nassert \"y\" not in filtered\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@abstractmethod\ndef filter(self, selection: Selection | Flag) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Filter the choice map on the `Selection`. The resulting choice map only contains the addresses that return True when presented to the selection.\n\n    Args:\n        selection: The Selection to filter the choice map with.\n\n    Returns:\n        A new ChoiceMap containing only the addresses selected by the given Selection.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        import jax\n        import genjax\n        from genjax import bernoulli\n        from genjax import SelectionBuilder as S\n\n\n        @genjax.gen\n        def model():\n            x = bernoulli(0.3) @ \"x\"\n            y = bernoulli(0.3) @ \"y\"\n            return x\n\n\n        key = jax.random.key(314159)\n        tr = model.simulate(key, ())\n        chm = tr.get_choices()\n        selection = S[\"x\"]\n        filtered = chm.filter(selection)\n        assert \"y\" not in filtered\n        ```\n    \"\"\"\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.filter(selection)","title":"<code>selection</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.from_mapping","title":"from_mapping  <code>staticmethod</code>","text":"<pre><code>from_mapping(\n    pairs: Iterable[tuple[K_addr, Any]]\n) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap from an iterable of address-value pairs.</p> <p>This method constructs a ChoiceMap by iterating through the provided pairs, where each pair consists of an address (or address component) and a corresponding value. The resulting ChoiceMap will contain all the values at their respective addresses.</p> <p>Parameters:</p> Name Type Description Default <code>Iterable[tuple[K_addr, Any]]</code> <p>An iterable of tuples, where each tuple contains an address (or address component) and its corresponding value. The address can be a single component or a tuple of components.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A ChoiceMap containing all the address-value pairs from the input.</p> Example <pre><code>pairs = [(\"x\", 42), ((\"y\", \"z\"), 10), (\"w\", [1, 2, 3])]\nchm = ChoiceMap.from_mapping(pairs)\nassert chm[\"x\"] == 42\nassert chm[\"y\", \"z\"] == 10\nassert chm[\"w\"] == [1, 2, 3]\n</code></pre> Note <p>If multiple pairs have the same address, later pairs will overwrite earlier ones.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef from_mapping(pairs: Iterable[tuple[K_addr, Any]]) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap from an iterable of address-value pairs.\n\n    This method constructs a ChoiceMap by iterating through the provided pairs,\n    where each pair consists of an address (or address component) and a corresponding value.\n    The resulting ChoiceMap will contain all the values at their respective addresses.\n\n    Args:\n        pairs: An iterable of tuples, where each tuple contains an address (or address component) and its corresponding value. The address can be a single component or a tuple of components.\n\n    Returns:\n        A ChoiceMap containing all the address-value pairs from the input.\n\n    Example:\n        ```python\n        pairs = [(\"x\", 42), ((\"y\", \"z\"), 10), (\"w\", [1, 2, 3])]\n        chm = ChoiceMap.from_mapping(pairs)\n        assert chm[\"x\"] == 42\n        assert chm[\"y\", \"z\"] == 10\n        assert chm[\"w\"] == [1, 2, 3]\n        ```\n\n    Note:\n        If multiple pairs have the same address, later pairs will overwrite earlier ones.\n    \"\"\"\n    acc = ChoiceMap.empty()\n\n    for addr, v in pairs:\n        addr = addr if isinstance(addr, tuple) else (addr,)\n        acc |= ChoiceMap.entry(v, *addr)\n\n    return acc\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.from_mapping(pairs)","title":"<code>pairs</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.get_selection","title":"get_selection","text":"<pre><code>get_selection() -&gt; Selection\n</code></pre> <p>Returns a Selection representing the structure of this ChoiceMap.</p> <p>This method creates a Selection that matches the hierarchical structure of the current ChoiceMap. The resulting Selection can be used to filter or query other ChoiceMaps with the same structure.</p> <p>Returns:</p> Type Description <code>Selection</code> <p>A Selection object representing the structure of this ChoiceMap.</p> Example <pre><code>chm = ChoiceMap.value(5).extend(\"x\")\nsel = chm.get_selection()\nassert sel[\"x\"] == True\nassert sel[\"y\"] == False\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def get_selection(self) -&gt; Selection:\n    \"\"\"\n    Returns a Selection representing the structure of this ChoiceMap.\n\n    This method creates a Selection that matches the hierarchical structure\n    of the current ChoiceMap. The resulting Selection can be used to filter\n    or query other ChoiceMaps with the same structure.\n\n    Returns:\n        A Selection object representing the structure of this ChoiceMap.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        chm = ChoiceMap.value(5).extend(\"x\")\n        sel = chm.get_selection()\n        assert sel[\"x\"] == True\n        assert sel[\"y\"] == False\n        ```\n    \"\"\"\n    return ChmSel.build(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.invalid_subset","title":"invalid_subset","text":"<pre><code>invalid_subset(\n    gen_fn: GenerativeFunction[Any], args: tuple[Any, ...]\n) -&gt; ChoiceMap | None\n</code></pre> <p>Identifies the subset of choices that are invalid for a given generative function and its arguments.</p> <p>This method checks if all choices in the current ChoiceMap are valid for the given generative function and its arguments.</p> <p>Parameters:</p> Name Type Description Default <code>GenerativeFunction[Any]</code> <p>The generative function to check against.</p> required <code>tuple[Any, ...]</code> <p>The arguments to the generative function.</p> required <p>Returns:</p> Type Description <code>ChoiceMap | None</code> <p>A ChoiceMap containing any extra choices not reachable in the course of <code>gen_fn</code>'s execution, or None if no extra choices are found.</p> Example <pre><code>@genjax.gen\ndef model(x):\n    y = bernoulli(0.5) @ \"y\"\n    return x + y\n\n\nchm = ChoiceMap.d({\"y\": 1, \"z\": 2})\nextras = chm.invalid_subset(model, (1,))\nassert \"z\" in extras  # \"z\" is an extra choice not in the model\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def invalid_subset(\n    self,\n    gen_fn: \"genjax.GenerativeFunction[Any]\",\n    args: tuple[Any, ...],\n) -&gt; \"ChoiceMap | None\":\n    \"\"\"\n    Identifies the subset of choices that are invalid for a given generative function and its arguments.\n\n    This method checks if all choices in the current ChoiceMap are valid for the given\n    generative function and its arguments.\n\n    Args:\n        gen_fn: The generative function to check against.\n        args: The arguments to the generative function.\n\n    Returns:\n        A ChoiceMap containing any extra choices not reachable in the course of `gen_fn`'s execution, or None if no extra choices are found.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        @genjax.gen\n        def model(x):\n            y = bernoulli(0.5) @ \"y\"\n            return x + y\n\n\n        chm = ChoiceMap.d({\"y\": 1, \"z\": 2})\n        extras = chm.invalid_subset(model, (1,))\n        assert \"z\" in extras  # \"z\" is an extra choice not in the model\n        ```\n    \"\"\"\n    shape_chm = gen_fn.get_zero_trace(*args).get_choices()\n    shape_sel = _shape_selection(shape_chm)\n    extras = self.filter(~shape_sel)\n    if not extras.static_is_empty():\n        return extras\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.invalid_subset(gen_fn)","title":"<code>gen_fn</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.invalid_subset(args)","title":"<code>args</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.kw","title":"kw  <code>staticmethod</code>","text":"<pre><code>kw(**kwargs) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap from keyword arguments.</p> <p>This method creates and returns a ChoiceMap based on the provided keyword arguments. Each keyword argument becomes an address in the ChoiceMap, and its value is stored at that address.</p> <p>Dict-shaped values are recursively converted to ChoiceMap instances with calls to <code>ChoiceMap.d</code>.</p> <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A ChoiceMap containing the key-value pairs from the input keyword arguments.</p> Example <pre><code>kw_chm = ChoiceMap.kw(x=42, y=[1, 2, 3], z={\"w\": 10.0})\nassert kw_chm[\"x\"] == 42\nassert kw_chm[\"y\"] == [1, 2, 3]\nassert kw_chm[\"z\", \"w\"] == 10.0\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef kw(**kwargs) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap from keyword arguments.\n\n    This method creates and returns a ChoiceMap based on the provided keyword arguments.\n    Each keyword argument becomes an address in the ChoiceMap, and its value is stored at that address.\n\n    Dict-shaped values are recursively converted to ChoiceMap instances with calls to `ChoiceMap.d`.\n\n    Returns:\n        A ChoiceMap containing the key-value pairs from the input keyword arguments.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        kw_chm = ChoiceMap.kw(x=42, y=[1, 2, 3], z={\"w\": 10.0})\n        assert kw_chm[\"x\"] == 42\n        assert kw_chm[\"y\"] == [1, 2, 3]\n        assert kw_chm[\"z\", \"w\"] == 10.0\n        ```\n    \"\"\"\n    return ChoiceMap.d(kwargs)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.mask","title":"mask","text":"<pre><code>mask(flag: Flag) -&gt; ChoiceMap\n</code></pre> <p>Returns a new ChoiceMap with values masked by a boolean flag.</p> <p>This method creates a new ChoiceMap where the values are conditionally included based on the provided flag. If the flag is True, the original values are retained; if False, the ChoiceMap behaves as if it's empty.</p> <p>Parameters:</p> Name Type Description Default <code>Flag</code> <p>A boolean flag determining whether to include the values.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap with values conditionally masked.</p> Example <pre><code>original_chm = ChoiceMap.value(42)\nmasked_chm = original_chm.mask(True)\nassert masked_chm.get_value() == 42\n\nmasked_chm = original_chm.mask(False)\nassert masked_chm.get_value() is None\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def mask(self, flag: Flag) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Returns a new ChoiceMap with values masked by a boolean flag.\n\n    This method creates a new ChoiceMap where the values are conditionally\n    included based on the provided flag. If the flag is True, the original\n    values are retained; if False, the ChoiceMap behaves as if it's empty.\n\n    Args:\n        flag: A boolean flag determining whether to include the values.\n\n    Returns:\n        A new ChoiceMap with values conditionally masked.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        original_chm = ChoiceMap.value(42)\n        masked_chm = original_chm.mask(True)\n        assert masked_chm.get_value() == 42\n\n        masked_chm = original_chm.mask(False)\n        assert masked_chm.get_value() is None\n        ```\n    \"\"\"\n    return self.filter(flag)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.mask(flag)","title":"<code>flag</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.merge","title":"merge","text":"<pre><code>merge(other: ChoiceMap) -&gt; ChoiceMap\n</code></pre> <p>Merges this ChoiceMap with another ChoiceMap.</p> <p>This method combines the current ChoiceMap with another ChoiceMap using the XOR operation (^). It creates a new ChoiceMap that contains all addresses from both input ChoiceMaps; any overlapping addresses will trigger an error on access at the address via <code>[&lt;addr&gt;]</code> or <code>get_value()</code>. Use <code>|</code> if you don't want this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>ChoiceMap</code> <p>The ChoiceMap to merge with the current one.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap resulting from the merge operation.</p> Example <pre><code>chm1 = ChoiceMap.value(5).extend(\"x\")\nchm2 = ChoiceMap.value(10).extend(\"y\")\nmerged_chm = chm1.merge(chm2)\nassert merged_chm[\"x\"] == 5\nassert merged_chm[\"y\"] == 10\n</code></pre> Note <p>This method is equivalent to using the | operator between two ChoiceMaps.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def merge(self, other: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n    \"\"\"\n    Merges this ChoiceMap with another ChoiceMap.\n\n    This method combines the current ChoiceMap with another ChoiceMap using the XOR operation (^). It creates a new ChoiceMap that contains all addresses from both input ChoiceMaps; any overlapping addresses will trigger an error on access at the address via `[&lt;addr&gt;]` or `get_value()`. Use `|` if you don't want this behavior.\n\n    Args:\n        other: The ChoiceMap to merge with the current one.\n\n    Returns:\n        A new ChoiceMap resulting from the merge operation.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        chm1 = ChoiceMap.value(5).extend(\"x\")\n        chm2 = ChoiceMap.value(10).extend(\"y\")\n        merged_chm = chm1.merge(chm2)\n        assert merged_chm[\"x\"] == 5\n        assert merged_chm[\"y\"] == 10\n        ```\n\n    Note:\n        This method is equivalent to using the | operator between two ChoiceMaps.\n    \"\"\"\n    return self | other\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.merge(other)","title":"<code>other</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.simplify","title":"simplify","text":"<pre><code>simplify() -&gt; ChoiceMap\n</code></pre> <p>Previously pushed down filters, now acts as identity.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@nobeartype\n@deprecated(\n    reason=\"Acts as identity; filters are now automatically pushed down.\",\n    version=\"0.8.0\",\n)\ndef simplify(self) -&gt; \"ChoiceMap\":\n    \"\"\"Previously pushed down filters, now acts as identity.\"\"\"\n    return self\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.static_is_empty","title":"static_is_empty","text":"<pre><code>static_is_empty() -&gt; bool\n</code></pre> <p>Returns True if this ChoiceMap is equal to <code>ChoiceMap.empty()</code>, False otherwise.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def static_is_empty(self) -&gt; bool:\n    \"\"\"\n    Returns True if this ChoiceMap is equal to `ChoiceMap.empty()`, False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.switch","title":"switch  <code>staticmethod</code>","text":"<pre><code>switch(\n    idx: int | IntArray, chms: Iterable[ChoiceMap]\n) -&gt; ChoiceMap\n</code></pre> <p>Creates a ChoiceMap that switches between multiple ChoiceMaps based on an index.</p> <p>This method creates a new ChoiceMap that selectively includes values from a sequence of input ChoiceMaps based on the provided index. The resulting ChoiceMap will contain values from the ChoiceMap at the position specified by the index, while masking out values from all other ChoiceMaps.</p> <p>Parameters:</p> Name Type Description Default <code>int | IntArray</code> <p>An index or array of indices specifying which ChoiceMap(s) to select from.</p> required <code>Iterable[ChoiceMap]</code> <p>An iterable of ChoiceMaps to switch between.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap containing values from the selected ChoiceMap(s).</p> Example <pre><code>chm1 = ChoiceMap.d({\"x\": 1, \"y\": 2})\nchm2 = ChoiceMap.d({\"x\": 3, \"y\": 4})\nchm3 = ChoiceMap.d({\"x\": 5, \"y\": 6})\n\nswitched = ChoiceMap.switch(jnp.array(1), [chm1, chm2, chm3])\nassert switched[\"x\"].unmask() == 3\nassert switched[\"y\"].unmask() == 4\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef switch(idx: int | IntArray, chms: Iterable[\"ChoiceMap\"]) -&gt; \"ChoiceMap\":\n    \"\"\"\n    Creates a ChoiceMap that switches between multiple ChoiceMaps based on an index.\n\n    This method creates a new ChoiceMap that selectively includes values from a sequence of\n    input ChoiceMaps based on the provided index. The resulting ChoiceMap will contain\n    values from the ChoiceMap at the position specified by the index, while masking out\n    values from all other ChoiceMaps.\n\n    Args:\n        idx: An index or array of indices specifying which ChoiceMap(s) to select from.\n        chms: An iterable of ChoiceMaps to switch between.\n\n    Returns:\n        A new ChoiceMap containing values from the selected ChoiceMap(s).\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        chm1 = ChoiceMap.d({\"x\": 1, \"y\": 2})\n        chm2 = ChoiceMap.d({\"x\": 3, \"y\": 4})\n        chm3 = ChoiceMap.d({\"x\": 5, \"y\": 6})\n\n        switched = ChoiceMap.switch(jnp.array(1), [chm1, chm2, chm3])\n        assert switched[\"x\"].unmask() == 3\n        assert switched[\"y\"].unmask() == 4\n        ```\n    \"\"\"\n    return Switch.build(idx, chms)\n</code></pre>"},{"location":"library/core.html#genjax.core.ChoiceMap.switch(idx)","title":"<code>idx</code>","text":""},{"location":"library/core.html#genjax.core.ChoiceMap.switch(chms)","title":"<code>chms</code>","text":""},{"location":"library/core.html#genjax.core.Selection","title":"genjax.core.Selection","text":"<p>               Bases: <code>Pytree</code></p> <p>A class representing a selection of addresses in a ChoiceMap.</p> <p>Selection objects are used to filter and manipulate ChoiceMaps by specifying which addresses should be included or excluded.</p> <p>Selection instances support various operations such as union (via <code>&amp;</code>), intersection (via <code>|</code>), and complement (via <code>~</code>), allowing for complex selection criteria to be constructed.</p> <p>Methods:</p> Name Description <code>all</code> <p>Creates a Selection that includes all addresses.</p> <code>none</code> <p>Creates a Selection that includes no addresses.</p> <code>at</code> <p>A builder instance for creating Selection objects using indexing syntax.</p> <p>Examples:</p> <p>Creating selections: <pre><code>import genjax\nfrom genjax import Selection\n\n# Select all addresses\nall_sel = Selection.all()\n\n# Select no addresses\nnone_sel = Selection.none()\n\n# Select specific addresses\nspecific_sel = Selection.at[\"x\", \"y\"]\n\n# Match (&lt;wildcard&gt;, \"y\")\nwildcard_sel = Selection.at[..., \"y\"]\n\n# Combine selections\ncombined_sel = specific_sel | Selection.at[\"z\"]\n</code></pre> </p> <p>Querying selections: <pre><code># Create a selection\nsel = Selection.at[\"x\", \"y\"]\n\n# Querying the selection using () returns a sub-selection\nassert sel(\"x\") == Selection.at[\"y\"]\nassert sel(\"z\") == Selection.none()\n\n# Querying the selection using [] returns a `bool` representing whether or not the input matches:\nassert sel[\"x\"] == False\nassert sel[\"x\", \"y\"] == True\n\n# Querying the selection using \"in\" acts the same:\nassert not \"x\" in sel\nassert (\"x\", \"y\") in sel\n\n# Nested querying\nnested_sel = Selection.at[\"a\", \"b\", \"c\"]\nassert nested_sel(\"a\")(\"b\") == Selection.at[\"c\"]\n</code></pre> </p> <p>Selection objects can passed to a <code>ChoiceMap</code> via the <code>filter</code> method to filter and manipulate data based on address patterns.</p> <p>Attributes:</p> Name Type Description <code>at</code> <code>Final[_SelectionBuilder]</code> <p>A builder instance for creating Selection objects.</p> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>class Selection(Pytree):\n    \"\"\"\n    A class representing a selection of addresses in a ChoiceMap.\n\n    Selection objects are used to filter and manipulate ChoiceMaps by specifying which addresses should be included or excluded.\n\n    Selection instances support various operations such as union (via `&amp;`), intersection (via `|`), and complement (via `~`), allowing for complex selection criteria to be constructed.\n\n    Methods:\n        all(): Creates a Selection that includes all addresses.\n        none(): Creates a Selection that includes no addresses.\n        at: A builder instance for creating Selection objects using indexing syntax.\n\n    Examples:\n        Creating selections:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        import genjax\n        from genjax import Selection\n\n        # Select all addresses\n        all_sel = Selection.all()\n\n        # Select no addresses\n        none_sel = Selection.none()\n\n        # Select specific addresses\n        specific_sel = Selection.at[\"x\", \"y\"]\n\n        # Match (&lt;wildcard&gt;, \"y\")\n        wildcard_sel = Selection.at[..., \"y\"]\n\n        # Combine selections\n        combined_sel = specific_sel | Selection.at[\"z\"]\n        ```\n\n        Querying selections:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        # Create a selection\n        sel = Selection.at[\"x\", \"y\"]\n\n        # Querying the selection using () returns a sub-selection\n        assert sel(\"x\") == Selection.at[\"y\"]\n        assert sel(\"z\") == Selection.none()\n\n        # Querying the selection using [] returns a `bool` representing whether or not the input matches:\n        assert sel[\"x\"] == False\n        assert sel[\"x\", \"y\"] == True\n\n        # Querying the selection using \"in\" acts the same:\n        assert not \"x\" in sel\n        assert (\"x\", \"y\") in sel\n\n        # Nested querying\n        nested_sel = Selection.at[\"a\", \"b\", \"c\"]\n        assert nested_sel(\"a\")(\"b\") == Selection.at[\"c\"]\n        ```\n\n    Selection objects can passed to a `ChoiceMap` via the `filter` method to filter and manipulate data based on address patterns.\n    \"\"\"\n\n    #################################################\n    # Convenient syntax for constructing selections #\n    #################################################\n\n    at: Final[_SelectionBuilder] = _SelectionBuilder()\n    \"\"\"A builder instance for creating Selection objects.\n\n    `at` provides a convenient interface for constructing Selection objects\n    using a familiar indexing syntax. It allows for the creation of complex\n    selections by chaining multiple address components.\n\n    Examples:\n        Creating a selection:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import Selection\n        Selection.at[\"x\", \"y\"]\n        ```\n    \"\"\"\n\n    @staticmethod\n    def all() -&gt; \"Selection\":\n        \"\"\"\n        Returns a Selection that selects all addresses.\n\n        Returns:\n            A Selection that selects everything.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            from genjax import Selection\n\n            all_selection = Selection.all()\n            assert all_selection[\"any_address\"] == True\n            ```\n        \"\"\"\n        return AllSel()\n\n    @staticmethod\n    def none() -&gt; \"Selection\":\n        \"\"\"\n        Returns a Selection that selects no addresses.\n\n        Returns:\n            A Selection that selects nothing.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            none_selection = Selection.none()\n            assert none_selection[\"any_address\"] == False\n            ```\n        \"\"\"\n        return NoneSel()\n\n    @staticmethod\n    def leaf() -&gt; \"Selection\":\n        \"\"\"\n        Returns a Selection that selects only leaf addresses.\n\n        A leaf address is an address that doesn't have any sub-addresses.\n        This selection is useful when you want to target only the final elements in a nested structure.\n\n        Returns:\n            A Selection that selects only leaf addresses.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            leaf_selection = Selection.leaf().extend(\"a\", \"b\")\n            assert leaf_selection[\"a\", \"b\"]\n            assert not leaf_selection[\"a\", \"b\", \"anything\"]\n            ```\n        \"\"\"\n        return LeafSel()\n\n    ######################\n    # Combinator methods #\n    ######################\n\n    def __or__(self, other: \"Selection\") -&gt; \"Selection\":\n        return OrSel.build(self, other)\n\n    def __and__(self, other: \"Selection\") -&gt; \"Selection\":\n        return AndSel.build(self, other)\n\n    def __invert__(self) -&gt; \"Selection\":\n        return ComplementSel.build(self)\n\n    def complement(self) -&gt; \"Selection\":\n        return ~self\n\n    def filter(self, sample: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n        \"\"\"\n        Returns a new ChoiceMap filtered with this Selection.\n\n        This method applies the current Selection to the given ChoiceMap, effectively filtering out addresses that are not matched.\n\n        Args:\n            sample: The ChoiceMap to be filtered.\n\n        Returns:\n            A new ChoiceMap containing only the addresses selected by this Selection.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            selection = Selection.at[\"x\"]\n\n            chm = ChoiceMap.kw(x=1, y=2)\n            filtered_chm = selection.filter(chm)\n\n            assert \"x\" in filtered_chm\n            assert \"y\" not in filtered_chm\n            ```\n        \"\"\"\n        return sample.filter(self)\n\n    def extend(self, *addrs: ExtendedStaticAddressComponent) -&gt; \"Selection\":\n        \"\"\"\n        Returns a new Selection that is prefixed by the given address components.\n\n        This method creates a new Selection that applies the current selection\n        to the specified address components. It handles both static and dynamic\n        address components.\n\n        Note that `...` as an address component will match any supplied address.\n\n        Args:\n            addrs: The address components under which to nest the selection.\n\n        Returns:\n            A new Selection extended by the given address component.\n\n        Example:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n            base_selection = Selection.all()\n            indexed_selection = base_selection.extend(\"x\")\n            assert indexed_selection[\"x\", \"any_subaddress\"] == True\n            assert indexed_selection[\"y\"] == False\n            ```\n        \"\"\"\n        acc = self\n        for addr in reversed(addrs):\n            acc = StaticSel.build(acc, addr)\n        return acc\n\n    def __call__(\n        self,\n        addr: StaticAddress,\n    ) -&gt; \"Selection\":\n        addr = addr if isinstance(addr, tuple) else (addr,)\n        subselection = self\n        for comp in addr:\n            subselection = subselection.get_subselection(comp)\n        return subselection\n\n    def __getitem__(\n        self,\n        addr: StaticAddress,\n    ) -&gt; bool:\n        return self(addr).check()\n\n    def __contains__(\n        self,\n        addr: StaticAddress,\n    ) -&gt; bool:\n        return self[addr]\n\n    @abstractmethod\n    def check(self) -&gt; bool:\n        pass\n\n    @abstractmethod\n    def get_subselection(self, addr: StaticAddressComponent) -&gt; \"Selection\":\n        pass\n</code></pre>"},{"location":"library/core.html#genjax.core.Selection.at","title":"at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>at: Final[_SelectionBuilder] = _SelectionBuilder()\n</code></pre> <p>A builder instance for creating Selection objects.</p> <p><code>at</code> provides a convenient interface for constructing Selection objects using a familiar indexing syntax. It allows for the creation of complex selections by chaining multiple address components.</p> <p>Examples:</p> <p>Creating a selection: <pre><code>from genjax import Selection\nSelection.at[\"x\", \"y\"]\n</code></pre> </p>"},{"location":"library/core.html#genjax.core.Selection.all","title":"all  <code>staticmethod</code>","text":"<pre><code>all() -&gt; Selection\n</code></pre> <p>Returns a Selection that selects all addresses.</p> <p>Returns:</p> Type Description <code>Selection</code> <p>A Selection that selects everything.</p> Example <pre><code>from genjax import Selection\n\nall_selection = Selection.all()\nassert all_selection[\"any_address\"] == True\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef all() -&gt; \"Selection\":\n    \"\"\"\n    Returns a Selection that selects all addresses.\n\n    Returns:\n        A Selection that selects everything.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        from genjax import Selection\n\n        all_selection = Selection.all()\n        assert all_selection[\"any_address\"] == True\n        ```\n    \"\"\"\n    return AllSel()\n</code></pre>"},{"location":"library/core.html#genjax.core.Selection.extend","title":"extend","text":"<pre><code>extend(*addrs: ExtendedStaticAddressComponent) -&gt; Selection\n</code></pre> <p>Returns a new Selection that is prefixed by the given address components.</p> <p>This method creates a new Selection that applies the current selection to the specified address components. It handles both static and dynamic address components.</p> <p>Note that <code>...</code> as an address component will match any supplied address.</p> <p>Parameters:</p> Name Type Description Default <code>ExtendedStaticAddressComponent</code> <p>The address components under which to nest the selection.</p> <code>()</code> <p>Returns:</p> Type Description <code>Selection</code> <p>A new Selection extended by the given address component.</p> Example <pre><code>base_selection = Selection.all()\nindexed_selection = base_selection.extend(\"x\")\nassert indexed_selection[\"x\", \"any_subaddress\"] == True\nassert indexed_selection[\"y\"] == False\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def extend(self, *addrs: ExtendedStaticAddressComponent) -&gt; \"Selection\":\n    \"\"\"\n    Returns a new Selection that is prefixed by the given address components.\n\n    This method creates a new Selection that applies the current selection\n    to the specified address components. It handles both static and dynamic\n    address components.\n\n    Note that `...` as an address component will match any supplied address.\n\n    Args:\n        addrs: The address components under which to nest the selection.\n\n    Returns:\n        A new Selection extended by the given address component.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        base_selection = Selection.all()\n        indexed_selection = base_selection.extend(\"x\")\n        assert indexed_selection[\"x\", \"any_subaddress\"] == True\n        assert indexed_selection[\"y\"] == False\n        ```\n    \"\"\"\n    acc = self\n    for addr in reversed(addrs):\n        acc = StaticSel.build(acc, addr)\n    return acc\n</code></pre>"},{"location":"library/core.html#genjax.core.Selection.extend(addrs)","title":"<code>addrs</code>","text":""},{"location":"library/core.html#genjax.core.Selection.filter","title":"filter","text":"<pre><code>filter(sample: ChoiceMap) -&gt; ChoiceMap\n</code></pre> <p>Returns a new ChoiceMap filtered with this Selection.</p> <p>This method applies the current Selection to the given ChoiceMap, effectively filtering out addresses that are not matched.</p> <p>Parameters:</p> Name Type Description Default <code>ChoiceMap</code> <p>The ChoiceMap to be filtered.</p> required <p>Returns:</p> Type Description <code>ChoiceMap</code> <p>A new ChoiceMap containing only the addresses selected by this Selection.</p> Example <pre><code>selection = Selection.at[\"x\"]\n\nchm = ChoiceMap.kw(x=1, y=2)\nfiltered_chm = selection.filter(chm)\n\nassert \"x\" in filtered_chm\nassert \"y\" not in filtered_chm\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>def filter(self, sample: \"ChoiceMap\") -&gt; \"ChoiceMap\":\n    \"\"\"\n    Returns a new ChoiceMap filtered with this Selection.\n\n    This method applies the current Selection to the given ChoiceMap, effectively filtering out addresses that are not matched.\n\n    Args:\n        sample: The ChoiceMap to be filtered.\n\n    Returns:\n        A new ChoiceMap containing only the addresses selected by this Selection.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        selection = Selection.at[\"x\"]\n\n        chm = ChoiceMap.kw(x=1, y=2)\n        filtered_chm = selection.filter(chm)\n\n        assert \"x\" in filtered_chm\n        assert \"y\" not in filtered_chm\n        ```\n    \"\"\"\n    return sample.filter(self)\n</code></pre>"},{"location":"library/core.html#genjax.core.Selection.filter(sample)","title":"<code>sample</code>","text":""},{"location":"library/core.html#genjax.core.Selection.leaf","title":"leaf  <code>staticmethod</code>","text":"<pre><code>leaf() -&gt; Selection\n</code></pre> <p>Returns a Selection that selects only leaf addresses.</p> <p>A leaf address is an address that doesn't have any sub-addresses. This selection is useful when you want to target only the final elements in a nested structure.</p> <p>Returns:</p> Type Description <code>Selection</code> <p>A Selection that selects only leaf addresses.</p> Example <pre><code>leaf_selection = Selection.leaf().extend(\"a\", \"b\")\nassert leaf_selection[\"a\", \"b\"]\nassert not leaf_selection[\"a\", \"b\", \"anything\"]\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef leaf() -&gt; \"Selection\":\n    \"\"\"\n    Returns a Selection that selects only leaf addresses.\n\n    A leaf address is an address that doesn't have any sub-addresses.\n    This selection is useful when you want to target only the final elements in a nested structure.\n\n    Returns:\n        A Selection that selects only leaf addresses.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        leaf_selection = Selection.leaf().extend(\"a\", \"b\")\n        assert leaf_selection[\"a\", \"b\"]\n        assert not leaf_selection[\"a\", \"b\", \"anything\"]\n        ```\n    \"\"\"\n    return LeafSel()\n</code></pre>"},{"location":"library/core.html#genjax.core.Selection.none","title":"none  <code>staticmethod</code>","text":"<pre><code>none() -&gt; Selection\n</code></pre> <p>Returns a Selection that selects no addresses.</p> <p>Returns:</p> Type Description <code>Selection</code> <p>A Selection that selects nothing.</p> Example <pre><code>none_selection = Selection.none()\nassert none_selection[\"any_address\"] == False\n</code></pre> Source code in <code>src/genjax/_src/core/generative/choice_map.py</code> <pre><code>@staticmethod\ndef none() -&gt; \"Selection\":\n    \"\"\"\n    Returns a Selection that selects no addresses.\n\n    Returns:\n        A Selection that selects nothing.\n\n    Example:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"choicemap\"\n        none_selection = Selection.none()\n        assert none_selection[\"any_address\"] == False\n        ```\n    \"\"\"\n    return NoneSel()\n</code></pre>"},{"location":"library/core.html#jax-compatible-data-via-pytree","title":"JAX compatible data via <code>Pytree</code>","text":"<p>JAX natively works with arrays, and with instances of Python classes which can be broken down into lists of arrays. JAX's <code>Pytree</code> system provides a way to register a class with methods that can break instances of the class down into a list of arrays (canonically referred to as flattening), and build an instance back up given a list of arrays (canonically referred to as unflattening).</p> <p>GenJAX provides an abstract class called <code>Pytree</code> which automates the implementation of the <code>flatten</code> / <code>unflatten</code> methods for a class. GenJAX's <code>Pytree</code> inherits from <code>penzai.Struct</code>, to support pretty printing, and some convenient methods to annotate what data should be part of the <code>Pytree</code> type (static fields, won't be broken down into a JAX array) and what data should be considered dynamic.</p>"},{"location":"library/core.html#genjax.core.Pytree","title":"genjax.core.Pytree","text":"<p>               Bases: <code>Struct</code></p> <p><code>Pytree</code> is an abstract base class which registers a class with JAX's <code>Pytree</code> system. JAX's <code>Pytree</code> system tracks how data classes should behave across JAX-transformed function boundaries, like <code>jax.jit</code> or <code>jax.vmap</code>.</p> <p>Inheriting this class provides the implementor with the freedom to declare how the subfields of a class should behave:</p> <ul> <li><code>Pytree.static(...)</code>: the value of the field cannot be a JAX traced value, it must be a Python literal, or a constant). The values of static fields are embedded in the <code>PyTreeDef</code> of any instance of the class.</li> <li><code>Pytree.field(...)</code> or no annotation: the value may be a JAX traced value, and JAX will attempt to convert it to tracer values inside of its transformations.</li> </ul> <p>If a field points to another <code>Pytree</code>, it should not be declared as <code>Pytree.static()</code>, as the <code>Pytree</code> interface will automatically handle the <code>Pytree</code> fields as dynamic fields.</p> <p>Methods:</p> Name Description <code>dataclass</code> <p>Denote that a class (which is inheriting <code>Pytree</code>) should be treated as a dataclass, meaning it can hold data in fields which are declared as part of the class.</p> <code>static</code> <p>Declare a field of a <code>Pytree</code> dataclass to be static. Users can provide additional keyword argument options,</p> <code>field</code> <p>Declare a field of a <code>Pytree</code> dataclass to be dynamic. Alternatively, one can leave the annotation off in the declaration.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>class Pytree(pz.Struct):\n    \"\"\"`Pytree` is an abstract base class which registers a class with JAX's `Pytree`\n    system. JAX's `Pytree` system tracks how data classes should behave across JAX-transformed function boundaries, like `jax.jit` or `jax.vmap`.\n\n    Inheriting this class provides the implementor with the freedom to declare how the subfields of a class should behave:\n\n    * `Pytree.static(...)`: the value of the field cannot be a JAX traced value, it must be a Python literal, or a constant). The values of static fields are embedded in the `PyTreeDef` of any instance of the class.\n    * `Pytree.field(...)` or no annotation: the value may be a JAX traced value, and JAX will attempt to convert it to tracer values inside of its transformations.\n\n    If a field _points to another `Pytree`_, it should not be declared as `Pytree.static()`, as the `Pytree` interface will automatically handle the `Pytree` fields as dynamic fields.\n\n    \"\"\"\n\n    @staticmethod\n    @overload\n    def dataclass(\n        incoming: None = None,\n        /,\n        **kwargs,\n    ) -&gt; Callable[[type[R]], type[R]]: ...\n\n    @staticmethod\n    @overload\n    def dataclass(\n        incoming: type[R],\n        /,\n        **kwargs,\n    ) -&gt; type[R]: ...\n\n    @dataclass_transform(\n        frozen_default=True,\n    )\n    @staticmethod\n    def dataclass(\n        incoming: type[R] | None = None,\n        /,\n        **kwargs,\n    ) -&gt; type[R] | Callable[[type[R]], type[R]]:\n        \"\"\"\n        Denote that a class (which is inheriting `Pytree`) should be treated as a dataclass, meaning it can hold data in fields which are declared as part of the class.\n\n        A dataclass is to be distinguished from a \"methods only\" `Pytree` class, which does not have fields, but may define methods.\n        The latter cannot be instantiated, but can be inherited from, while the former can be instantiated:\n        the `Pytree.dataclass` declaration informs the system _how to instantiate_ the class as a dataclass,\n        and how to automatically define JAX's `Pytree` interfaces (`tree_flatten`, `tree_unflatten`, etc.) for the dataclass, based on the fields declared in the class, and possibly `Pytree.static(...)` or `Pytree.field(...)` annotations (or lack thereof, the default is that all fields are `Pytree.field(...)`).\n\n        All `Pytree` dataclasses support pretty printing, as well as rendering to HTML.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            from genjax import Pytree\n            from genjax.typing import FloatArray\n            import jax.numpy as jnp\n\n\n            @Pytree.dataclass\n            # Enforces type annotations on instantiation.\n            class MyClass(Pytree):\n                my_static_field: int = Pytree.static()\n                my_dynamic_field: FloatArray\n\n\n            print(MyClass(10, jnp.array(5.0)).render_html())\n            ```\n        \"\"\"\n\n        return pz.pytree_dataclass(\n            incoming,\n            overwrite_parent_init=True,\n            **kwargs,\n        )\n\n    @staticmethod\n    def static(**kwargs):\n        \"\"\"Declare a field of a `Pytree` dataclass to be static. Users can provide additional keyword argument options,\n        like `default` or `default_factory`, to customize how the field is instantiated when an instance of\n        the dataclass is instantiated.` Fields which are provided with default values must come after required fields in the dataclass declaration.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            @Pytree.dataclass\n            # Enforces type annotations on instantiation.\n            class MyClass(Pytree):\n                my_dynamic_field: FloatArray\n                my_static_field: int = Pytree.static(default=0)\n\n\n            print(MyClass(jnp.array(5.0)).render_html())\n            ```\n\n        \"\"\"\n        return field(metadata={\"pytree_node\": False}, **kwargs)\n\n    @staticmethod\n    def field(**kwargs):\n        \"Declare a field of a `Pytree` dataclass to be dynamic. Alternatively, one can leave the annotation off in the declaration.\"\n        return field(**kwargs)\n\n    ##############################\n    # Utility class constructors #\n    ##############################\n\n    @staticmethod\n    def const(v):\n        # The value must be concrete!\n        # It cannot be a JAX traced value.\n        assert static_check_is_concrete(v)\n        if isinstance(v, Const):\n            return v\n        else:\n            return Const(v)\n\n    # Safe: will not wrap a Const in another Const, and will not\n    # wrap dynamic values.\n    @staticmethod\n    def tree_const(v):\n        def _inner(v):\n            if isinstance(v, Const):\n                return v\n            elif static_check_is_concrete(v):\n                return Const(v)\n            else:\n                return v\n\n        return jtu.tree_map(\n            _inner,\n            v,\n            is_leaf=lambda v: isinstance(v, Const),\n        )\n\n    @staticmethod\n    def tree_const_unwrap(v):\n        def _inner(v):\n            if isinstance(v, Const):\n                return v.val\n            else:\n                return v\n\n        return jtu.tree_map(\n            _inner,\n            v,\n            is_leaf=lambda v: isinstance(v, Const),\n        )\n\n    @staticmethod\n    def partial(*args) -&gt; Callable[[Callable[..., R]], \"Closure[R]\"]:\n        return lambda fn: Closure[R](args, fn)\n\n    def treedef(self):\n        return jtu.tree_structure(self)\n\n    #################\n    # Static checks #\n    #################\n\n    @staticmethod\n    def static_check_tree_structure_equivalence(trees: list[Any]):\n        if not trees:\n            return True\n        else:\n            fst, *rest = trees\n            treedef = jtu.tree_structure(fst)\n            check = all(map(lambda v: treedef == jtu.tree_structure(v), rest))\n            return check\n\n    def treescope_color(self) -&gt; str:\n        \"\"\"Computes a CSS color to display for this object in treescope.\n\n        This function can be overridden to change the color for a particular object\n        in treescope, without having to register a new handler.\n\n        (note that we are overriding the Penzai base class's implementation so that ALL structs receive colors, not just classes with `__call__` implemented.)\n\n        Returns:\n          A CSS color string to use as a background/highlight color for this object.\n          Alternatively, a tuple of (border, fill) CSS colors.\n        \"\"\"\n        type_string = type(self).__module__ + \".\" + type(self).__qualname__\n        return formatting_util.color_from_string(type_string)\n\n    def render_html(self):\n        return treescope.render_to_html(\n            self,\n            roundtrip_mode=False,\n        )\n</code></pre>"},{"location":"library/core.html#genjax.core.Pytree.dataclass","title":"dataclass  <code>staticmethod</code>","text":"<pre><code>dataclass(\n    incoming: None = None, /, **kwargs\n) -&gt; Callable[[type[R]], type[R]]\n</code></pre><pre><code>dataclass(incoming: type[R], /, **kwargs) -&gt; type[R]\n</code></pre> <pre><code>dataclass(\n    incoming: type[R] | None = None, /, **kwargs\n) -&gt; type[R] | Callable[[type[R]], type[R]]\n</code></pre> <p>Denote that a class (which is inheriting <code>Pytree</code>) should be treated as a dataclass, meaning it can hold data in fields which are declared as part of the class.</p> <p>A dataclass is to be distinguished from a \"methods only\" <code>Pytree</code> class, which does not have fields, but may define methods. The latter cannot be instantiated, but can be inherited from, while the former can be instantiated: the <code>Pytree.dataclass</code> declaration informs the system how to instantiate the class as a dataclass, and how to automatically define JAX's <code>Pytree</code> interfaces (<code>tree_flatten</code>, <code>tree_unflatten</code>, etc.) for the dataclass, based on the fields declared in the class, and possibly <code>Pytree.static(...)</code> or <code>Pytree.field(...)</code> annotations (or lack thereof, the default is that all fields are <code>Pytree.field(...)</code>).</p> <p>All <code>Pytree</code> dataclasses support pretty printing, as well as rendering to HTML.</p> <p>Examples:</p> <pre><code>from genjax import Pytree\nfrom genjax.typing import FloatArray\nimport jax.numpy as jnp\n\n\n@Pytree.dataclass\n# Enforces type annotations on instantiation.\nclass MyClass(Pytree):\n    my_static_field: int = Pytree.static()\n    my_dynamic_field: FloatArray\n\n\nprint(MyClass(10, jnp.array(5.0)).render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@dataclass_transform(\n    frozen_default=True,\n)\n@staticmethod\ndef dataclass(\n    incoming: type[R] | None = None,\n    /,\n    **kwargs,\n) -&gt; type[R] | Callable[[type[R]], type[R]]:\n    \"\"\"\n    Denote that a class (which is inheriting `Pytree`) should be treated as a dataclass, meaning it can hold data in fields which are declared as part of the class.\n\n    A dataclass is to be distinguished from a \"methods only\" `Pytree` class, which does not have fields, but may define methods.\n    The latter cannot be instantiated, but can be inherited from, while the former can be instantiated:\n    the `Pytree.dataclass` declaration informs the system _how to instantiate_ the class as a dataclass,\n    and how to automatically define JAX's `Pytree` interfaces (`tree_flatten`, `tree_unflatten`, etc.) for the dataclass, based on the fields declared in the class, and possibly `Pytree.static(...)` or `Pytree.field(...)` annotations (or lack thereof, the default is that all fields are `Pytree.field(...)`).\n\n    All `Pytree` dataclasses support pretty printing, as well as rendering to HTML.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import Pytree\n        from genjax.typing import FloatArray\n        import jax.numpy as jnp\n\n\n        @Pytree.dataclass\n        # Enforces type annotations on instantiation.\n        class MyClass(Pytree):\n            my_static_field: int = Pytree.static()\n            my_dynamic_field: FloatArray\n\n\n        print(MyClass(10, jnp.array(5.0)).render_html())\n        ```\n    \"\"\"\n\n    return pz.pytree_dataclass(\n        incoming,\n        overwrite_parent_init=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"library/core.html#genjax.core.Pytree.static","title":"static  <code>staticmethod</code>","text":"<pre><code>static(**kwargs)\n</code></pre> <p>Declare a field of a <code>Pytree</code> dataclass to be static. Users can provide additional keyword argument options, like <code>default</code> or <code>default_factory</code>, to customize how the field is instantiated when an instance of the dataclass is instantiated.` Fields which are provided with default values must come after required fields in the dataclass declaration.</p> <p>Examples:</p> <pre><code>@Pytree.dataclass\n# Enforces type annotations on instantiation.\nclass MyClass(Pytree):\n    my_dynamic_field: FloatArray\n    my_static_field: int = Pytree.static(default=0)\n\n\nprint(MyClass(jnp.array(5.0)).render_html())\n</code></pre> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@staticmethod\ndef static(**kwargs):\n    \"\"\"Declare a field of a `Pytree` dataclass to be static. Users can provide additional keyword argument options,\n    like `default` or `default_factory`, to customize how the field is instantiated when an instance of\n    the dataclass is instantiated.` Fields which are provided with default values must come after required fields in the dataclass declaration.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        @Pytree.dataclass\n        # Enforces type annotations on instantiation.\n        class MyClass(Pytree):\n            my_dynamic_field: FloatArray\n            my_static_field: int = Pytree.static(default=0)\n\n\n        print(MyClass(jnp.array(5.0)).render_html())\n        ```\n\n    \"\"\"\n    return field(metadata={\"pytree_node\": False}, **kwargs)\n</code></pre>"},{"location":"library/core.html#genjax.core.Pytree.field","title":"field  <code>staticmethod</code>","text":"<pre><code>field(**kwargs)\n</code></pre> <p>Declare a field of a <code>Pytree</code> dataclass to be dynamic. Alternatively, one can leave the annotation off in the declaration.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@staticmethod\ndef field(**kwargs):\n    \"Declare a field of a `Pytree` dataclass to be dynamic. Alternatively, one can leave the annotation off in the declaration.\"\n    return field(**kwargs)\n</code></pre>"},{"location":"library/core.html#genjax.core.Const","title":"genjax.core.Const","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p>JAX-compatible way to tag a value as a constant. Valid constants include Python literals, strings, essentially anything that won't hold JAX arrays inside of a computation.</p> <p>Examples:</p> <p>Instances of <code>Const</code> can be created using a <code>Pytree</code> classmethod: <pre><code>from genjax import Pytree\n\nc = Pytree.const(5)\nprint(c.render_html())\n</code></pre> </p> <p>Constants can be freely used across <code>jax.jit</code> boundaries: <pre><code>from genjax import Pytree\n\n\ndef f(c):\n    if c.unwrap() == 5:\n        return 10.0\n    else:\n        return 5.0\n\n\nc = Pytree.const(5)\nr = jax.jit(f)(c)\nprint(r)\n</code></pre>  10.0  </p> <p>Methods:</p> Name Description <code>unwrap</code> <p>Unwrap a constant value from a <code>Const</code> instance.</p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@Pytree.dataclass\nclass Const(Generic[R], Pytree):\n    \"\"\"\n    JAX-compatible way to tag a value as a constant. Valid constants include Python literals, strings, essentially anything **that won't hold JAX arrays** inside of a computation.\n\n    Examples:\n        Instances of `Const` can be created using a `Pytree` classmethod:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import Pytree\n\n        c = Pytree.const(5)\n        print(c.render_html())\n        ```\n\n        Constants can be freely used across [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) boundaries:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import Pytree\n\n\n        def f(c):\n            if c.unwrap() == 5:\n                return 10.0\n            else:\n                return 5.0\n\n\n        c = Pytree.const(5)\n        r = jax.jit(f)(c)\n        print(r)\n        ```\n    \"\"\"\n\n    val: R = Pytree.static()\n\n    def __call__(self, *args):\n        assert isinstance(self.val, Callable), (\n            f\"Wrapped `val` {self.val} is not Callable.\"\n        )\n        return self.val(*args)\n\n    def unwrap(self: Any) -&gt; R:\n        \"\"\"Unwrap a constant value from a `Const` instance.\n\n        This method can be used as an instance method or as a static method. When used as a static method, it returns the input value unchanged if it is not a `Const` instance.\n\n        Returns:\n            R: The unwrapped value if self is a `Const`, otherwise returns self unchanged.\n\n        Examples:\n            ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n            from genjax import Pytree, Const\n\n            c = Pytree.const(5)\n            val = c.unwrap()  # Returns 5\n\n            # Can also be used as static method\n            val = Const.unwrap(10)  # Returns 10 unchanged\n            ```\n        \"\"\"\n        if isinstance(self, Const):\n            return self.val\n        else:\n            return self\n</code></pre>"},{"location":"library/core.html#genjax.core.Const.unwrap","title":"unwrap","text":"<pre><code>unwrap() -&gt; R\n</code></pre> <p>Unwrap a constant value from a <code>Const</code> instance.</p> <p>This method can be used as an instance method or as a static method. When used as a static method, it returns the input value unchanged if it is not a <code>Const</code> instance.</p> <p>Returns:</p> Name Type Description <code>R</code> <code>R</code> <p>The unwrapped value if self is a <code>Const</code>, otherwise returns self unchanged.</p> <p>Examples:</p> <pre><code>from genjax import Pytree, Const\n\nc = Pytree.const(5)\nval = c.unwrap()  # Returns 5\n\n# Can also be used as static method\nval = Const.unwrap(10)  # Returns 10 unchanged\n</code></pre> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>def unwrap(self: Any) -&gt; R:\n    \"\"\"Unwrap a constant value from a `Const` instance.\n\n    This method can be used as an instance method or as a static method. When used as a static method, it returns the input value unchanged if it is not a `Const` instance.\n\n    Returns:\n        R: The unwrapped value if self is a `Const`, otherwise returns self unchanged.\n\n    Examples:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import Pytree, Const\n\n        c = Pytree.const(5)\n        val = c.unwrap()  # Returns 5\n\n        # Can also be used as static method\n        val = Const.unwrap(10)  # Returns 10 unchanged\n        ```\n    \"\"\"\n    if isinstance(self, Const):\n        return self.val\n    else:\n        return self\n</code></pre>"},{"location":"library/core.html#genjax.core.Closure","title":"genjax.core.Closure","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p>JAX-compatible closure type. It's a closure as a <code>Pytree</code> - meaning the static source code / callable is separated from dynamic data (which must be tracked by JAX).</p> <p>Examples:</p> <p>Instances of <code>Closure</code> can be created using <code>Pytree.partial</code> -- note the order of the \"closed over\" arguments: <pre><code>from genjax import Pytree\n\n\ndef g(y):\n    @Pytree.partial(y)  # dynamic values come first\n    def f(v, x):\n        # v will be bound to the value of y\n        return x * (v * 5.0)\n\n    return f\n\n\nclos = jax.jit(g)(5.0)\nprint(clos.render_html())\n</code></pre> </p> <p>Closures can be invoked / JIT compiled in other code: <pre><code>r = jax.jit(lambda x: clos(x))(3.0)\nprint(r)\n</code></pre>  75.0  </p> Source code in <code>src/genjax/_src/core/pytree.py</code> <pre><code>@Pytree.dataclass\nclass Closure(Generic[R], Pytree):\n    \"\"\"\n    JAX-compatible closure type. It's a closure _as a [`Pytree`][genjax.core.Pytree]_ - meaning the static _source code_ / _callable_ is separated from dynamic data (which must be tracked by JAX).\n\n    Examples:\n        Instances of `Closure` can be created using `Pytree.partial` -- note the order of the \"closed over\" arguments:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        from genjax import Pytree\n\n\n        def g(y):\n            @Pytree.partial(y)  # dynamic values come first\n            def f(v, x):\n                # v will be bound to the value of y\n                return x * (v * 5.0)\n\n            return f\n\n\n        clos = jax.jit(g)(5.0)\n        print(clos.render_html())\n        ```\n\n        Closures can be invoked / JIT compiled in other code:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        r = jax.jit(lambda x: clos(x))(3.0)\n        print(r)\n        ```\n    \"\"\"\n\n    dyn_args: tuple[Any, ...]\n    fn: Callable[..., R] = Pytree.static()\n\n    def __call__(self, *args, **kwargs) -&gt; R:\n        return self.fn(*self.dyn_args, *args, **kwargs)\n</code></pre>"},{"location":"library/core.html#dynamism-in-jax-masks-and-sum-types","title":"Dynamism in JAX: masks and sum types","text":"<p>The semantics of Gen are defined independently of any particular computational substrate or implementation - but JAX (and XLA through JAX) is a unique substrate, offering high performance, the ability to transformation code ahead-of-time via program transformations, and ... a rather unique set of restrictions.</p>"},{"location":"library/core.html#jax-is-a-two-phase-system","title":"JAX is a two-phase system","text":"<p>While not yet formally modelled, it's appropriate to think of JAX as separating computation into two phases:</p> <ul> <li>The statics phase (which occurs at JAX tracing / transformation time).</li> <li>The runtime phase (which occurs when a computation written in JAX is actually deployed via XLA and executed on a physical device somewhere in the world).</li> </ul> <p>JAX has different rules for handling values depending on which phase we are in.</p> <p>For instance, JAX disallows usage of runtime values to resolve Python control flow at tracing time (intuition: we don't actually know the value yet!) and will error if the user attempts to trace through a Python program with incorrect usage of runtime values.</p> <p>In GenJAX, we take advantage of JAX's tracing to construct code which, when traced, produces specialized code depending on static information. At the same time, we are careful to encode Gen's interfaces to respect JAX's rules which govern how static / runtime values can be used.</p> <p>The most primitive way to encode runtime uncertainty about a piece of data is to attach a <code>bool</code> to it, which indicates whether the data is \"on\" or \"off\".</p> <p>GenJAX contains a system for tagging data with flags, to indicate if the data is valid or invalid during inference interface computations at runtime. The key data structure which supports this system is <code>genjax.core.Mask</code>.</p>"},{"location":"library/core.html#genjax.core.Mask","title":"genjax.core.Mask","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p>The <code>Mask</code> datatype wraps a value in a Boolean flag which denotes whether the data is valid or invalid to use in inference computations.</p> <p>Masks can be used in a variety of ways as part of generative computations - their primary role is to denote data which is valid under inference computations. Valid data can be used as <code>ChoiceMap</code> leaves, and participate in generative and inference computations (like scores, and importance weights or density ratios). A Mask with a False flag should be considered unusable, and should be handled with care.</p> <p>If a <code>flag</code> has a non-scalar shape, that implies that the mask is vectorized, and that the <code>ArrayLike</code> value, or each leaf in the pytree, must have the flag's shape as its prefix (i.e., must have been created with a <code>jax.vmap</code> call or via a GenJAX <code>vmap</code> combinator).</p>"},{"location":"library/core.html#genjax.core.Mask--encountering-mask-in-your-computation","title":"Encountering <code>Mask</code> in your computation","text":"<p>When users see <code>Mask</code> in their computations, they are expected to interact with them by either:</p> <ul> <li> <p>Unmasking them using the <code>Mask.unmask</code> interface, a potentially unsafe operation.</p> </li> <li> <p>Destructuring them manually, and handling the cases.</p> </li> </ul>"},{"location":"library/core.html#genjax.core.Mask--usage-of-invalid-data","title":"Usage of invalid data","text":"<p>If you use invalid <code>Mask(data, False)</code> data in inference computations, you may encounter silently incorrect results.</p> <p>Methods:</p> Name Description <code>unmask</code> <p>Unmask the <code>Mask</code>, returning the value within.</p> Source code in <code>src/genjax/_src/core/generative/functional_types.py</code> <pre><code>@Pytree.dataclass(match_args=True, init=False)\nclass Mask(Generic[R], Pytree):\n    \"\"\"The `Mask` datatype wraps a value in a Boolean flag which denotes whether the data is valid or invalid to use in inference computations.\n\n    Masks can be used in a variety of ways as part of generative computations - their primary role is to denote data which is valid under inference computations. Valid data can be used as `ChoiceMap` leaves, and participate in generative and inference computations (like scores, and importance weights or density ratios). A Mask with a False flag **should** be considered unusable, and should be handled with care.\n\n    If a `flag` has a non-scalar shape, that implies that the mask is vectorized, and that the `ArrayLike` value, or each leaf in the pytree, must have the flag's shape as its prefix (i.e., must have been created with a `jax.vmap` call or via a GenJAX `vmap` combinator).\n\n    ## Encountering `Mask` in your computation\n\n    When users see `Mask` in their computations, they are expected to interact with them by either:\n\n    * Unmasking them using the `Mask.unmask` interface, a potentially unsafe operation.\n\n    * Destructuring them manually, and handling the cases.\n\n    ## Usage of invalid data\n\n    If you use invalid `Mask(data, False)` data in inference computations, you may encounter silently incorrect results.\n    \"\"\"\n\n    value: R\n    flag: Flag | Diff[Flag]\n\n    ################\n    # Constructors #\n    ################\n\n    def __init__(self, value: R, flag: Flag | Diff[Flag] = True) -&gt; None:\n        assert not isinstance(value, Mask), (\n            f\"Mask should not be instantiated with another Mask! found {value}\"\n        )\n        Mask._validate_init(value, flag)\n\n        self.value, self.flag = value, flag  # pyright: ignore[reportAttributeAccessIssue]\n\n    @staticmethod\n    def _validate_init(value: R, flag: Flag | Diff[Flag]) -&gt; None:\n        \"\"\"Validates that non-scalar flags are only used with vectorized masks.\n\n        When a flag has a non-scalar shape (e.g. shape (3,)), this indicates the mask is vectorized.\n        In this case, each leaf value in the pytree must have the flag's shape as a prefix of its own shape.\n        For example, if flag has shape (3,), then array leaves must have shapes like (3,), (3,4), (3,2,1) etc.\n\n        This ensures that vectorized flags properly align with vectorized data.\n\n        Args:\n            value: The value to be masked, can be a pytree\n            flag: The flag to apply, either a scalar or array flag\n\n        Raises:\n            ValueError: If a non-scalar flag's shape is not a prefix of all leaf value shapes\n        \"\"\"\n        flag = flag.get_primal() if isinstance(flag, Diff) else flag\n        f_shape = jnp.shape(flag)\n        if f_shape == ():\n            return None\n\n        leaf_shapes = [jnp.shape(leaf) for leaf in jtu.tree_leaves(value)]\n        prefix_len = len(f_shape)\n\n        for shape in leaf_shapes:\n            if shape[:prefix_len] != f_shape:\n                raise ValueError(\n                    f\"Vectorized flag {flag}'s shape {f_shape} must be a prefix of all leaf shapes. Found {shape}.\"\n                )\n\n    @staticmethod\n    def _validate_leaf_shapes(this: R, other: R):\n        \"\"\"Validates that two values have matching shapes at each leaf.\n\n        Used by __or__, __xor__ etc. to ensure we only combine masks with values whose leaves have matching shapes.\n        Broadcasting is not supported - array shapes must match exactly.\n\n        Args:\n            this: First value to compare\n            other: Second value to compare\n\n        Raises:\n            ValueError: If any leaf shapes don't match exactly\n        \"\"\"\n\n        # Check array shapes match exactly (no broadcasting)\n        def check_leaf_shapes(x, y):\n            x_shape = jnp.shape(x)\n            y_shape = jnp.shape(y)\n            if x_shape != y_shape:\n                raise ValueError(\n                    f\"Cannot combine masks with different array shapes: {x_shape} vs {y_shape}\"\n                )\n            return None\n\n        jtu.tree_map(check_leaf_shapes, this, other)\n\n    def _validate_mask_shapes(self, other: \"Mask[R]\") -&gt; None:\n        \"\"\"Used by __or__, __xor__ etc. to ensure we only combine masks with matching pytree shape and matching leaf shapes.\"\"\"\n        if jtu.tree_structure(self.value) != jtu.tree_structure(other.value):\n            raise ValueError(\"Cannot combine masks with different tree structures!\")\n\n        Mask._validate_leaf_shapes(self, other)\n        return None\n\n    @staticmethod\n    def build(v: \"R | Mask[R]\", f: Flag | Diff[Flag] = True) -&gt; \"Mask[R]\":\n        \"\"\"\n        Create a Mask instance, potentially from an existing Mask or a raw value.\n\n        This method allows for the creation of a new Mask or the modification of an existing one. If the input is already a Mask, it combines the new flag with the existing one using a logical AND operation.\n\n        Args:\n            v: The value to be masked. Can be a raw value or an existing Mask.\n            f: The flag to be applied to the value.\n\n        Returns:\n            A new Mask instance with the given value and flag.\n\n        Note:\n            If `v` is already a Mask, the new flag is combined with the existing one using a logical AND, ensuring that the resulting Mask is only valid if both input flags are valid.\n        \"\"\"\n        match v:\n            case Mask(value, g):\n                assert not isinstance(f, Diff) and not isinstance(g, Diff)\n                assert FlagOp.is_scalar(f) or (jnp.shape(f) == jnp.shape(g)), (\n                    f\"Can't build a Mask with non-matching Flag shapes {jnp.shape(f)} and {jnp.shape(g)}\"\n                )\n                return Mask[R](value, FlagOp.and_(f, g))\n            case _:\n                return Mask[R](v, f)\n\n    @staticmethod\n    def maybe_mask(v: \"R | Mask[R]\", f: Flag) -&gt; \"R | Mask[R] | None\":\n        \"\"\"\n        Create a Mask instance or return the original value based on the flag.\n\n        This method is similar to `build`, but it handles concrete flag values differently. For concrete True flags, it returns the original value without wrapping it in a Mask. For concrete False flags, it returns None. For non-concrete flags, it creates a new Mask instance.\n\n        Args:\n            v: The value to be potentially masked. Can be a raw value or an existing Mask.\n            f: The flag to be applied to the value.\n\n        Returns:\n            - The original value `v` if `f` is concretely True.\n            - None if `f` is concretely False.\n            - A new Mask instance with the given value and flag if `f` is not concrete.\n        \"\"\"\n        return Mask.build(v, f).flatten()\n\n    #############\n    # Accessors #\n    #############\n\n    def __getitem__(self, path) -&gt; \"Mask[R]\":\n        path = path if isinstance(path, tuple) else (path,)\n\n        f = self.primal_flag()\n        if isinstance(f, Array) and f.shape:\n            # A non-scalar flag must have been produced via vectorization. Because a scalar flag can\n            # wrap a non-scalar value, only use the vectorized components of the path to index into the flag...\n            f = f[path[: len(f.shape)]]\n\n        # but the use full path to index into the value.\n        v_idx = jtu.tree_map(lambda v: v[path], self.value)\n\n        # Reconstruct Diff if needed\n        if isinstance(self.flag, Diff):\n            f = Diff(f, self.flag.tangent)\n\n        return Mask.build(v_idx, f)\n\n    def flatten(self) -&gt; \"R | Mask[R] | None\":\n        \"\"\"\n        Flatten a Mask instance into its underlying value or None.\n\n        \"Flattening\" occurs when the flag value is a concrete Boolean (True/False). In these cases, the Mask is simplified to either its raw value or None. If the flag is not concrete (i.e., a symbolic/traced value), the Mask remains intact.\n\n        This method evaluates the mask's flag and returns:\n        - None if the flag is concretely False or the value is None\n        - The raw value if the flag is concretely True\n        - The Mask instance itself if the flag is not concrete\n\n        Returns:\n            The flattened result based on the mask's flag state.\n        \"\"\"\n        flag = self.primal_flag()\n        if FlagOp.concrete_false(flag):\n            return None\n        elif FlagOp.concrete_true(flag):\n            return self.value\n        else:\n            return self\n\n    def unmask(self, default: R | None = None) -&gt; R:\n        \"\"\"\n        Unmask the `Mask`, returning the value within.\n\n        This operation is inherently unsafe with respect to inference semantics if no default value is provided. It is only valid if the `Mask` wraps valid data at runtime, or if a default value is supplied.\n\n        Args:\n            default: An optional default value to return if the mask is invalid.\n\n        Returns:\n            The unmasked value if valid, or the default value if provided and the mask is invalid.\n        \"\"\"\n        if default is None:\n\n            def _check():\n                checkify.check(\n                    jnp.all(self.primal_flag()),\n                    \"Attempted to unmask when a mask flag (or some flag in a vectorized mask) is False: the unmasked value is invalid.\\n\",\n                )\n\n            optional_check(_check)\n            return self.value\n        else:\n\n            def inner(true_v: ArrayLike, false_v: ArrayLike) -&gt; Array:\n                return jnp.where(self.primal_flag(), true_v, false_v)\n\n            return jtu.tree_map(inner, self.value, default)\n\n    def primal_flag(self) -&gt; Flag:\n        \"\"\"\n        Returns the primal flag of the mask.\n\n        This method retrieves the primal (non-`Diff`-wrapped) flag value. If the flag\n        is a Diff type (which contains both primal and tangent components), it returns\n        the primal component. Otherwise, it returns the flag as is.\n\n        Returns:\n            The primal flag value.\n        \"\"\"\n        match self.flag:\n            case Diff(primal, _):\n                return primal\n            case flag:\n                return flag\n\n    ###############\n    # Combinators #\n    ###############\n\n    def _or_idx(self, first: Flag, second: Flag):\n        \"\"\"Converts a pair of flag arrays into an array of indices for selecting between two values.\n\n        This function implements a truth table for selecting between two values based on their flags:\n\n        first | second | output | meaning\n        ------+--------+--------+------------------\n            0   |   0    |   -1   | neither valid\n            1   |   0    |    0   | first valid only\n            0   |   1    |    1   | second valid only\n            1   |   1    |    0   | both valid for OR, invalid for XOR\n\n        The output index is used to select between the corresponding values:\n           0 -&gt; select first value\n           1 -&gt; select second value\n\n        Args:\n            first: The flag for the first value\n            second: The flag for the second value\n\n        Returns:\n            An Array of indices (-1, 0, or 1) indicating which value to select from each side.\n        \"\"\"\n        # Note that the validation has already run to check that these flags have the same shape.\n        return first + 2 * FlagOp.and_(FlagOp.not_(first), second) - 1\n\n    def __or__(self, other: \"Mask[R]\") -&gt; \"Mask[R]\":\n        self._validate_mask_shapes(other)\n\n        match self.primal_flag(), other.primal_flag():\n            case True, _:\n                return self\n            case False, _:\n                return other\n            case self_flag, other_flag:\n                idx = self._or_idx(self_flag, other_flag)\n                return tree_choose(idx, [self, other])\n\n    def __xor__(self, other: \"Mask[R]\") -&gt; \"Mask[R]\":\n        self._validate_mask_shapes(other)\n\n        match self.primal_flag(), other.primal_flag():\n            case (False, False) | (True, True):\n                return Mask.build(self, False)\n            case True, False:\n                return self\n            case False, True:\n                return other\n            case self_flag, other_flag:\n                idx = self._or_idx(self_flag, other_flag)\n\n                # note that `idx` above will choose the correct side for the FF, FT and TF cases,\n                # but will equal 0 for TT flags. We use `FlagOp.xor_` to override this flag to equal\n                # False, since neither side in the TT case will provide a `False` flag for us.\n                chosen = tree_choose(idx, [self.value, other.value])\n                return Mask(chosen, FlagOp.xor_(self_flag, other_flag))\n\n    def __invert__(self) -&gt; \"Mask[R]\":\n        not_flag = jtu.tree_map(FlagOp.not_, self.flag)\n        return Mask(self.value, not_flag)\n\n    @staticmethod\n    def or_n(mask: \"Mask[R]\", *masks: \"Mask[R]\") -&gt; \"Mask[R]\":\n        \"\"\"Performs an n-ary OR operation on a sequence of Mask objects.\n\n        Args:\n            mask: The first mask to combine\n            *masks: Variable number of additional masks to combine with OR\n\n        Returns:\n            A new Mask combining all inputs with OR operations\n        \"\"\"\n        return functools.reduce(lambda a, b: a | b, masks, mask)\n\n    @staticmethod\n    def xor_n(mask: \"Mask[R]\", *masks: \"Mask[R]\") -&gt; \"Mask[R]\":\n        \"\"\"Performs an n-ary XOR operation on a sequence of Mask objects.\n\n        Args:\n            mask: The first mask to combine\n            *masks: Variable number of additional masks to combine with XOR\n\n        Returns:\n            A new Mask combining all inputs with XOR operations\n        \"\"\"\n        return functools.reduce(lambda a, b: a ^ b, masks, mask)\n</code></pre>"},{"location":"library/core.html#genjax.core.Mask.unmask","title":"unmask","text":"<pre><code>unmask(default: R | None = None) -&gt; R\n</code></pre> <p>Unmask the <code>Mask</code>, returning the value within.</p> <p>This operation is inherently unsafe with respect to inference semantics if no default value is provided. It is only valid if the <code>Mask</code> wraps valid data at runtime, or if a default value is supplied.</p> <p>Parameters:</p> Name Type Description Default <code>R | None</code> <p>An optional default value to return if the mask is invalid.</p> <code>None</code> <p>Returns:</p> Type Description <code>R</code> <p>The unmasked value if valid, or the default value if provided and the mask is invalid.</p> Source code in <code>src/genjax/_src/core/generative/functional_types.py</code> <pre><code>def unmask(self, default: R | None = None) -&gt; R:\n    \"\"\"\n    Unmask the `Mask`, returning the value within.\n\n    This operation is inherently unsafe with respect to inference semantics if no default value is provided. It is only valid if the `Mask` wraps valid data at runtime, or if a default value is supplied.\n\n    Args:\n        default: An optional default value to return if the mask is invalid.\n\n    Returns:\n        The unmasked value if valid, or the default value if provided and the mask is invalid.\n    \"\"\"\n    if default is None:\n\n        def _check():\n            checkify.check(\n                jnp.all(self.primal_flag()),\n                \"Attempted to unmask when a mask flag (or some flag in a vectorized mask) is False: the unmasked value is invalid.\\n\",\n            )\n\n        optional_check(_check)\n        return self.value\n    else:\n\n        def inner(true_v: ArrayLike, false_v: ArrayLike) -&gt; Array:\n            return jnp.where(self.primal_flag(), true_v, false_v)\n\n        return jtu.tree_map(inner, self.value, default)\n</code></pre>"},{"location":"library/core.html#genjax.core.Mask.unmask(default)","title":"<code>default</code>","text":""},{"location":"library/core.html#static-typing-with-genjaxtyping-aka-beartype","title":"Static typing with <code>genjax.typing</code> a.k.a \ud83d\udc3b<code>beartype</code>\ud83d\udc3b","text":"<p>GenJAX uses <code>beartype</code> to perform type checking during JAX tracing / compile time. This means that <code>beartype</code>, normally a fast runtime type checker, operates at JAX tracing time to ensure that the arguments and return values are correct, with zero runtime cost.</p>"},{"location":"library/core.html#generative-interface-types","title":"Generative interface types","text":""},{"location":"library/core.html#genjax.core.Arguments","title":"genjax.core.Arguments  <code>module-attribute</code>","text":"<pre><code>Arguments = tuple\n</code></pre> <p><code>Arguments</code> is the type of argument values to generative functions. It is a type alias for <code>Tuple</code>, and is used to improve readability and parsing of interface specifications.</p>"},{"location":"library/core.html#genjax.core.Score","title":"genjax.core.Score  <code>module-attribute</code>","text":"<pre><code>Score = FloatArray\n</code></pre> <p>A score is a density ratio, described fully in <code>simulate</code>.</p> <p>The type <code>Score</code> does not enforce any meaningful mathematical invariants, but is used to denote the type of scores in the GenJAX system, to improve readability and parsing of interface specifications.</p>"},{"location":"library/core.html#genjax.core.Weight","title":"genjax.core.Weight  <code>module-attribute</code>","text":"<pre><code>Weight = FloatArray\n</code></pre> <p>A weight is a density ratio which often occurs in the context of proper weighting for <code>Target</code> distributions, or in Gen's <code>edit</code> interface, whose mathematical content is described in <code>edit</code>.</p> <p>The type <code>Weight</code> does not enforce any meaningful mathematical invariants, but is used to denote the type of weights in GenJAX, to improve readability and parsing of interface specifications / expectations.</p>"},{"location":"library/core.html#genjax.core.Retdiff","title":"genjax.core.Retdiff  <code>module-attribute</code>","text":"<pre><code>Retdiff = Annotated[\n    R, Is[lambda x: static_check_tree_diff(x)]\n]\n</code></pre> <p><code>Retdiff</code> is the type of return values with an attached <code>ChangeType</code> (c.f. <code>edit</code>).</p> <p>When used under type checking, <code>Retdiff</code> assumes that the return value is a <code>Pytree</code> (either, defined via GenJAX's <code>Pytree</code> interface or registered with JAX's system). It checks that the leaves are <code>Diff</code> type with attached <code>ChangeType</code>.</p>"},{"location":"library/core.html#genjax.core.Argdiffs","title":"genjax.core.Argdiffs  <code>module-attribute</code>","text":"<pre><code>Argdiffs = Annotated[\n    tuple[Any, ...],\n    Is[lambda x: static_check_tree_diff(x)],\n]\n</code></pre> <p><code>Argdiffs</code> is the type of argument values with an attached <code>ChangeType</code> (c.f. <code>edit</code>).</p> <p>When used under type checking, <code>Retdiff</code> assumes that the argument values are <code>Pytree</code> (either, defined via GenJAX's <code>Pytree</code> interface or registered with JAX's system). For each argument, it checks that the leaves are <code>Diff</code> type with attached <code>ChangeType</code>.</p>"},{"location":"library/generative_functions.html","title":"The menagerie of <code>GenerativeFunction</code>","text":"<p>Generative functions are probabilistic building blocks. They allow you to express complex probability distributions, and automate several operations on them. GenJAX exports a standard library of generative functions, and this page catalogues them and their usage.</p>"},{"location":"library/generative_functions.html#the-venerable-reliable-distribution","title":"The venerable &amp; reliable <code>Distribution</code>","text":"<p>To start, distributions are generative functions.</p> <p>Distributions intentionally expose a permissive interface (<code>random_weighted</code> and <code>estimate_logpdf</code> which doesn't assume exact density evaluation. <code>genjax.ExactDensity</code> is a more restrictive interface, which assumes exact density evaluation.</p> <p>GenJAX exports a long list of exact density distributions, which uses the functionality of <code>tfp.distributions</code>. A list of these is shown below.</p>"},{"location":"library/generative_functions.html#genjax.Distribution","title":"genjax.Distribution","text":"<p>               Bases: <code>Generic[R]</code>, <code>GenerativeFunction[R]</code></p> <p>Methods:</p> Name Description <code>random_weighted</code> <code>estimate_logpdf</code> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>class Distribution(Generic[R], GenerativeFunction[R]):\n    @abstractmethod\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args,\n    ) -&gt; tuple[Score, R]:\n        pass\n\n    @abstractmethod\n    def estimate_logpdf(\n        self,\n        key: PRNGKey,\n        v: R,\n        *args,\n    ) -&gt; Score:\n        pass\n\n    def simulate(\n        self,\n        key: PRNGKey,\n        args: tuple[Any, ...],\n    ) -&gt; Trace[R]:\n        (w, v) = self.random_weighted(key, *args)\n        tr = DistributionTrace(self, args, v, w)\n        return tr\n\n    def generate_choice_map(\n        self,\n        key: PRNGKey,\n        chm: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Trace[R], Weight]:\n        v = chm.get_value()\n        match v:\n            case None:\n                tr = self.simulate(key, args)\n                return tr, jnp.array(0.0)\n\n            case Mask(value, flag):\n\n                def _simulate(key, v):\n                    score, new_v = self.random_weighted(key, *args)\n                    w = 0.0\n                    return (score, w, new_v)\n\n                def _importance(key, v):\n                    w = self.estimate_logpdf(key, v, *args)\n                    return (w, w, v)\n\n                score, w, new_v = jax.lax.cond(flag, _importance, _simulate, key, value)\n                tr = DistributionTrace(self, args, new_v, score)\n                return tr, w\n\n            case _:\n                w = self.estimate_logpdf(key, v, *args)\n                tr = DistributionTrace(self, args, v, w)\n                return tr, w\n\n    def generate(\n        self,\n        key: PRNGKey,\n        constraint: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Trace[R], Weight]:\n        match constraint:\n            case ChoiceMap():\n                tr, w = self.generate_choice_map(key, constraint, args)\n\n            case _:\n                raise Exception(\"Unhandled type.\")\n        return tr, w\n\n    def edit_empty(\n        self,\n        trace: Trace[R],\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], Update]:\n        sample = trace.get_choices()\n        primals = Diff.tree_primal(argdiffs)\n        new_score, _ = self.assess(sample, primals)\n        new_trace = DistributionTrace(self, primals, sample.get_value(), new_score)\n        return (\n            new_trace,\n            new_score - trace.get_score(),\n            Diff.no_change(trace.get_retval()),\n            Update(ChoiceMap.empty()),\n        )\n\n    def edit_update_with_constraint(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        constraint: ChoiceMap,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], Update]:\n        primals = Diff.tree_primal(argdiffs)\n        match constraint:\n            case ChoiceMap():\n                match constraint.get_value():\n                    case Mask() as masked_value:\n\n                        def _true_branch(key, new_value: R, _):\n                            fwd = self.estimate_logpdf(key, new_value, *primals)\n                            bwd = trace.get_score()\n                            w = fwd - bwd\n                            return (new_value, w, fwd)\n\n                        def _false_branch(key, _, old_value: R):\n                            fwd = self.estimate_logpdf(key, old_value, *primals)\n                            bwd = trace.get_score()\n                            w = fwd - bwd\n                            return (old_value, w, fwd)\n\n                        flag = masked_value.primal_flag()\n                        new_value: R = masked_value.value\n                        old_choices = trace.get_choices()\n                        old_value: R = old_choices.get_value()\n\n                        new_value, w, score = FlagOp.cond(\n                            flag,\n                            _true_branch,\n                            _false_branch,\n                            key,\n                            new_value,\n                            old_value,\n                        )\n                        return (\n                            DistributionTrace(self, primals, new_value, score),\n                            w,\n                            Diff.unknown_change(new_value),\n                            Update(\n                                old_choices.mask(flag),\n                            ),\n                        )\n                    case None:\n                        value_chm = trace.get_choices()\n                        v = value_chm.get_value()\n                        fwd = self.estimate_logpdf(key, v, *primals)\n                        bwd = trace.get_score()\n                        w = fwd - bwd\n                        new_tr = DistributionTrace(self, primals, v, fwd)\n                        retval_diff = Diff.no_change(v)\n                        return (new_tr, w, retval_diff, Update(ChoiceMap.empty()))\n\n                    case v:\n                        fwd = self.estimate_logpdf(key, v, *primals)\n                        bwd = trace.get_score()\n                        w = fwd - bwd\n                        new_tr = DistributionTrace(self, primals, v, fwd)\n                        discard = trace.get_choices()\n                        retval_diff = Diff.unknown_change(v)\n                        return (new_tr, w, retval_diff, Update(discard))\n            case _:\n                raise Exception(f\"Unhandled constraint in edit: {type(constraint)}.\")\n\n    def project(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        selection: Selection,\n    ) -&gt; Weight:\n        return jnp.where(\n            selection.check(),\n            trace.get_score(),\n            jnp.array(0.0),\n        )\n\n    def edit_regenerate(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        selection: Selection,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], EditRequest]:\n        check = () in selection\n        if FlagOp.concrete_true(check):\n            primals = Diff.tree_primal(argdiffs)\n            w, new_v = self.random_weighted(key, *primals)\n            incremental_w = w - trace.get_score()\n            old_v = trace.get_retval()\n            new_trace = DistributionTrace(self, primals, new_v, w)\n            return (\n                new_trace,\n                incremental_w,\n                Diff.unknown_change(new_v),\n                Update(ChoiceMap.choice(old_v)),\n            )\n        elif FlagOp.concrete_false(check):\n            if Diff.static_check_no_change(argdiffs):\n                return (\n                    trace,\n                    jnp.array(0.0),\n                    Diff.no_change(trace.get_retval()),\n                    Update(ChoiceMap.empty()),\n                )\n            else:\n                chm = trace.get_choices()\n                primals = Diff.tree_primal(argdiffs)\n                new_score, _ = self.assess(chm, primals)\n                new_trace = DistributionTrace(self, primals, chm.get_value(), new_score)\n                return (\n                    new_trace,\n                    new_score - trace.get_score(),\n                    Diff.no_change(trace.get_retval()),\n                    Update(\n                        ChoiceMap.empty(),\n                    ),\n                )\n        else:\n            raise NotImplementedError\n\n    def edit_update(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        constraint: ChoiceMap,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], Update]:\n        match constraint:\n            case ChoiceMap():\n                return self.edit_update_with_constraint(\n                    key, trace, constraint, argdiffs\n                )\n\n            case _:\n                raise Exception(f\"Not implement fwd problem: {constraint}.\")\n\n    def edit(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        edit_request: EditRequest,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[Trace[R], Weight, Retdiff[R], EditRequest]:\n        match edit_request:\n            case Update(chm):\n                return self.edit_update(\n                    key,\n                    trace,\n                    chm,\n                    argdiffs,\n                )\n            case Regenerate(selection):\n                return self.edit_regenerate(\n                    key,\n                    trace,\n                    selection,\n                    argdiffs,\n                )\n\n            case _:\n                raise NotSupportedEditRequest(edit_request)\n\n    def assess(\n        self,\n        sample: ChoiceMap,\n        args: tuple[Any, ...],\n    ):\n        raise NotImplementedError\n</code></pre>"},{"location":"library/generative_functions.html#genjax.Distribution.random_weighted","title":"random_weighted  <code>abstractmethod</code>","text":"<pre><code>random_weighted(key: PRNGKey, *args) -&gt; tuple[Score, R]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abstractmethod\ndef random_weighted(\n    self,\n    key: PRNGKey,\n    *args,\n) -&gt; tuple[Score, R]:\n    pass\n</code></pre>"},{"location":"library/generative_functions.html#genjax.Distribution.estimate_logpdf","title":"estimate_logpdf  <code>abstractmethod</code>","text":"<pre><code>estimate_logpdf(key: PRNGKey, v: R, *args) -&gt; Score\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>@abstractmethod\ndef estimate_logpdf(\n    self,\n    key: PRNGKey,\n    v: R,\n    *args,\n) -&gt; Score:\n    pass\n</code></pre>"},{"location":"library/generative_functions.html#genjax.ExactDensity","title":"genjax.ExactDensity","text":"<p>               Bases: <code>Generic[R]</code>, <code>Distribution[R]</code></p> <p>Methods:</p> Name Description <code>random_weighted</code> <p>Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.</p> <code>estimate_logpdf</code> <p>Given a sample and arguments to the distribution, return the exact log density of the sample.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>class ExactDensity(Generic[R], Distribution[R]):\n    @abstractmethod\n    def sample(self, key: PRNGKey, *args) -&gt; R:\n        pass\n\n    @abstractmethod\n    def logpdf(self, v: R, *args, **kwargs) -&gt; Score:\n        pass\n\n    def __abstract_call__(self, *args):\n        return to_shape_fn(self.sample, jnp.zeros)(_fake_key, *args)\n\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args,\n    ) -&gt; tuple[Score, R]:\n        \"\"\"\n        Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.\n        \"\"\"\n        v = self.sample(key, *args)\n        w = self.estimate_logpdf(key, v, *args)\n        return (w, v)\n\n    def estimate_logpdf(\n        self,\n        key: PRNGKey,\n        v: R,\n        *args,\n    ) -&gt; Weight:\n        \"\"\"\n        Given a sample and arguments to the distribution, return the exact log density of the sample.\n        \"\"\"\n        w = self.logpdf(v, *args)\n        if w.shape:\n            return jnp.sum(w)\n        else:\n            return w\n\n    def assess(\n        self,\n        sample: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Weight, R]:\n        key = jax.random.key(0)\n        v = sample.get_value()\n        match v:\n            case Mask(value, flag):\n\n                def _check():\n                    checkify.check(\n                        bool(flag),\n                        \"Attempted to unmask when a mask flag is False: the masked value is invalid.\\n\",\n                    )\n\n                optional_check(_check)\n                w = self.estimate_logpdf(key, value, *args)\n                return w, value\n            case _:\n                w = self.estimate_logpdf(key, v, *args)\n                return w, v\n</code></pre>"},{"location":"library/generative_functions.html#genjax.ExactDensity.random_weighted","title":"random_weighted","text":"<pre><code>random_weighted(key: PRNGKey, *args) -&gt; tuple[Score, R]\n</code></pre> <p>Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>def random_weighted(\n    self,\n    key: PRNGKey,\n    *args,\n) -&gt; tuple[Score, R]:\n    \"\"\"\n    Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.\n    \"\"\"\n    v = self.sample(key, *args)\n    w = self.estimate_logpdf(key, v, *args)\n    return (w, v)\n</code></pre>"},{"location":"library/generative_functions.html#genjax.ExactDensity.estimate_logpdf","title":"estimate_logpdf","text":"<pre><code>estimate_logpdf(key: PRNGKey, v: R, *args) -&gt; Weight\n</code></pre> <p>Given a sample and arguments to the distribution, return the exact log density of the sample.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>def estimate_logpdf(\n    self,\n    key: PRNGKey,\n    v: R,\n    *args,\n) -&gt; Weight:\n    \"\"\"\n    Given a sample and arguments to the distribution, return the exact log density of the sample.\n    \"\"\"\n    w = self.logpdf(v, *args)\n    if w.shape:\n        return jnp.sum(w)\n    else:\n        return w\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions","title":"genjax.generative_functions.distributions","text":"<p>Attributes:</p> Name Type Description <code>bernoulli</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Bernoulli</code> distribution from TensorFlow Probability distributions.</p> <code>beta</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Beta</code> distribution from TensorFlow Probability distributions.</p> <code>beta_binomial</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.BetaBinomial</code> distribution from TensorFlow Probability distributions.</p> <code>beta_quotient</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.BetaQuotient</code> distribution from TensorFlow Probability distributions.</p> <code>binomial</code> <code>ExactDensity[Array]</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Binomial</code> distribution from TensorFlow Probability distributions.</p> <code>categorical</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Categorical</code> distribution from TensorFlow Probability distributions.</p> <code>cauchy</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Cauchy</code> distribution from TensorFlow Probability distributions.</p> <code>chi</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Chi</code> distribution from TensorFlow Probability distributions.</p> <code>chi2</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Chi2</code> distribution from TensorFlow Probability distributions.</p> <code>dirichlet</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Dirichlet</code> distribution from TensorFlow Probability distributions.</p> <code>dirichlet_multinomial</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.DirichletMultinomial</code> distribution from TensorFlow Probability distributions.</p> <code>double_sided_maxwell</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.DoublesidedMaxwell</code> distribution from TensorFlow Probability distributions.</p> <code>exp_gamma</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.ExpGamma</code> distribution from TensorFlow Probability distributions.</p> <code>exp_inverse_gamma</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.ExpInverseGamma</code> distribution from TensorFlow Probability distributions.</p> <code>exponential</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Exponential</code> distribution from TensorFlow Probability distributions.</p> <code>flip</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Bernoulli</code> distribution from TensorFlow Probability distributions, but is constructed using a probability value and not a logit.</p> <code>gamma</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Gamma</code> distribution from TensorFlow Probability distributions.</p> <code>geometric</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Geometric</code> distribution from TensorFlow Probability distributions.</p> <code>gumbel</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Gumbel</code> distribution from TensorFlow Probability distributions.</p> <code>half_cauchy</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfCauchy</code> distribution from TensorFlow Probability distributions.</p> <code>half_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfNormal</code> distribution from TensorFlow Probability distributions.</p> <code>half_student_t</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfStudentT</code> distribution from TensorFlow Probability distributions.</p> <code>inverse_gamma</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.InverseGamma</code> distribution from TensorFlow Probability distributions.</p> <code>kumaraswamy</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Kumaraswamy</code> distribution from TensorFlow Probability distributions.</p> <code>lambert_w_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LambertWNormal</code> distribution from TensorFlow Probability distributions.</p> <code>laplace</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Laplace</code> distribution from TensorFlow Probability distributions.</p> <code>log_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LogNormal</code> distribution from TensorFlow Probability distributions.</p> <code>logit_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LogitNormal</code> distribution from TensorFlow Probability distributions.</p> <code>moyal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Moyal</code> distribution from TensorFlow Probability distributions.</p> <code>multinomial</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Multinomial</code> distribution from TensorFlow Probability distributions.</p> <code>mv_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.MultivariateNormalFullCovariance</code> distribution from TensorFlow Probability distributions.</p> <code>mv_normal_diag</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.MultivariateNormalDiag</code> distribution from TensorFlow Probability distributions.</p> <code>negative_binomial</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.NegativeBinomial</code> distribution from TensorFlow Probability distributions.</p> <code>non_central_chi2</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.NoncentralChi2</code> distribution from TensorFlow Probability distributions.</p> <code>normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Normal</code> distribution from TensorFlow Probability distributions.</p> <code>poisson</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Poisson</code> distribution from TensorFlow Probability distributions.</p> <code>power_spherical</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.PowerSpherical</code> distribution from TensorFlow Probability distributions.</p> <code>skellam</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Skellam</code> distribution from TensorFlow Probability distributions.</p> <code>student_t</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.StudentT</code> distribution from TensorFlow Probability distributions.</p> <code>truncated_cauchy</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.TruncatedCauchy</code> distribution from TensorFlow Probability distributions.</p> <code>truncated_normal</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.TruncatedNormal</code> distribution from TensorFlow Probability distributions.</p> <code>uniform</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Uniform</code> distribution from TensorFlow Probability distributions.</p> <code>von_mises</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.VonMises</code> distribution from TensorFlow Probability distributions.</p> <code>von_mises_fisher</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.VonMisesFisher</code> distribution from TensorFlow Probability distributions.</p> <code>weibull</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Weibull</code> distribution from TensorFlow Probability distributions.</p> <code>zipf</code> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Zipf</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.bernoulli","title":"bernoulli  <code>module-attribute</code>","text":"<pre><code>bernoulli = tfp_distribution(\n    implicit_logit_warning(Bernoulli), name=\"bernoulli\"\n)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Bernoulli</code> distribution from TensorFlow Probability distributions.</p> <p>Takes an N-D Tensor representing the log-odds of a 1 event. Each entry in the Tensor parameterizes an independent Bernoulli distribution where the probability of an event is sigmoid(logits).</p> <p>(Note that this is the <code>logits</code> argument to the <code>tfd.Bernoulli</code> constructor.)</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.beta","title":"beta  <code>module-attribute</code>","text":"<pre><code>beta = tfp_distribution(Beta)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Beta</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.beta_binomial","title":"beta_binomial  <code>module-attribute</code>","text":"<pre><code>beta_binomial = tfp_distribution(BetaBinomial)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.BetaBinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.beta_quotient","title":"beta_quotient  <code>module-attribute</code>","text":"<pre><code>beta_quotient = tfp_distribution(BetaQuotient)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.BetaQuotient</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.binomial","title":"binomial  <code>module-attribute</code>","text":"<pre><code>binomial: ExactDensity[Array] = tfp_distribution(Binomial)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Binomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.categorical","title":"categorical  <code>module-attribute</code>","text":"<pre><code>categorical = tfp_distribution(\n    implicit_logit_warning(Categorical), name=\"categorical\"\n)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Categorical</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.cauchy","title":"cauchy  <code>module-attribute</code>","text":"<pre><code>cauchy = tfp_distribution(Cauchy)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Cauchy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.chi","title":"chi  <code>module-attribute</code>","text":"<pre><code>chi = tfp_distribution(Chi)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Chi</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.chi2","title":"chi2  <code>module-attribute</code>","text":"<pre><code>chi2 = tfp_distribution(Chi2)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Chi2</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.dirichlet","title":"dirichlet  <code>module-attribute</code>","text":"<pre><code>dirichlet = tfp_distribution(Dirichlet)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Dirichlet</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.dirichlet_multinomial","title":"dirichlet_multinomial  <code>module-attribute</code>","text":"<pre><code>dirichlet_multinomial = tfp_distribution(\n    DirichletMultinomial\n)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.DirichletMultinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.double_sided_maxwell","title":"double_sided_maxwell  <code>module-attribute</code>","text":"<pre><code>double_sided_maxwell = tfp_distribution(DoublesidedMaxwell)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.DoublesidedMaxwell</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.exp_gamma","title":"exp_gamma  <code>module-attribute</code>","text":"<pre><code>exp_gamma = tfp_distribution(ExpGamma)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.ExpGamma</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.exp_inverse_gamma","title":"exp_inverse_gamma  <code>module-attribute</code>","text":"<pre><code>exp_inverse_gamma = tfp_distribution(ExpInverseGamma)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.ExpInverseGamma</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.exponential","title":"exponential  <code>module-attribute</code>","text":"<pre><code>exponential = tfp_distribution(Exponential)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Exponential</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.flip","title":"flip  <code>module-attribute</code>","text":"<pre><code>flip = tfp_distribution(\n    lambda p: Bernoulli(probs=p, dtype=bool_), name=\"flip\"\n)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Bernoulli</code> distribution from TensorFlow Probability distributions, but is constructed using a probability value and not a logit.</p> <p>Takes an N-D Tensor representing the probability of a 1 event. Each entry in the Tensor parameterizes an independent Bernoulli distribution.</p> <p>(Note that this is the <code>probs</code> argument to the <code>tfd.Bernoulli</code> constructor.)</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.gamma","title":"gamma  <code>module-attribute</code>","text":"<pre><code>gamma = tfp_distribution(Gamma)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Gamma</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.geometric","title":"geometric  <code>module-attribute</code>","text":"<pre><code>geometric = tfp_distribution(Geometric)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Geometric</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.gumbel","title":"gumbel  <code>module-attribute</code>","text":"<pre><code>gumbel = tfp_distribution(Gumbel)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Gumbel</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.half_cauchy","title":"half_cauchy  <code>module-attribute</code>","text":"<pre><code>half_cauchy = tfp_distribution(HalfCauchy)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfCauchy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.half_normal","title":"half_normal  <code>module-attribute</code>","text":"<pre><code>half_normal = tfp_distribution(HalfNormal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.half_student_t","title":"half_student_t  <code>module-attribute</code>","text":"<pre><code>half_student_t = tfp_distribution(HalfStudentT)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.HalfStudentT</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.inverse_gamma","title":"inverse_gamma  <code>module-attribute</code>","text":"<pre><code>inverse_gamma = tfp_distribution(InverseGamma)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.InverseGamma</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.kumaraswamy","title":"kumaraswamy  <code>module-attribute</code>","text":"<pre><code>kumaraswamy = tfp_distribution(Kumaraswamy)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Kumaraswamy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.lambert_w_normal","title":"lambert_w_normal  <code>module-attribute</code>","text":"<pre><code>lambert_w_normal = tfp_distribution(LambertWNormal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LambertWNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.laplace","title":"laplace  <code>module-attribute</code>","text":"<pre><code>laplace = tfp_distribution(Laplace)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Laplace</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.log_normal","title":"log_normal  <code>module-attribute</code>","text":"<pre><code>log_normal = tfp_distribution(LogNormal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LogNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.logit_normal","title":"logit_normal  <code>module-attribute</code>","text":"<pre><code>logit_normal = tfp_distribution(LogitNormal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.LogitNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.moyal","title":"moyal  <code>module-attribute</code>","text":"<pre><code>moyal = tfp_distribution(Moyal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Moyal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.multinomial","title":"multinomial  <code>module-attribute</code>","text":"<pre><code>multinomial = tfp_distribution(Multinomial)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Multinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.mv_normal","title":"mv_normal  <code>module-attribute</code>","text":"<pre><code>mv_normal = tfp_distribution(\n    MultivariateNormalFullCovariance\n)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.MultivariateNormalFullCovariance</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.mv_normal_diag","title":"mv_normal_diag  <code>module-attribute</code>","text":"<pre><code>mv_normal_diag = tfp_distribution(MultivariateNormalDiag)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.MultivariateNormalDiag</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.negative_binomial","title":"negative_binomial  <code>module-attribute</code>","text":"<pre><code>negative_binomial = tfp_distribution(NegativeBinomial)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.NegativeBinomial</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.non_central_chi2","title":"non_central_chi2  <code>module-attribute</code>","text":"<pre><code>non_central_chi2 = tfp_distribution(NoncentralChi2)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.NoncentralChi2</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.normal","title":"normal  <code>module-attribute</code>","text":"<pre><code>normal = tfp_distribution(Normal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Normal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.poisson","title":"poisson  <code>module-attribute</code>","text":"<pre><code>poisson = tfp_distribution(Poisson)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Poisson</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.power_spherical","title":"power_spherical  <code>module-attribute</code>","text":"<pre><code>power_spherical = tfp_distribution(PowerSpherical)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.PowerSpherical</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.skellam","title":"skellam  <code>module-attribute</code>","text":"<pre><code>skellam = tfp_distribution(Skellam)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Skellam</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.student_t","title":"student_t  <code>module-attribute</code>","text":"<pre><code>student_t = tfp_distribution(StudentT)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.StudentT</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.truncated_cauchy","title":"truncated_cauchy  <code>module-attribute</code>","text":"<pre><code>truncated_cauchy = tfp_distribution(TruncatedCauchy)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.TruncatedCauchy</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.truncated_normal","title":"truncated_normal  <code>module-attribute</code>","text":"<pre><code>truncated_normal = tfp_distribution(TruncatedNormal)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.TruncatedNormal</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.uniform","title":"uniform  <code>module-attribute</code>","text":"<pre><code>uniform = tfp_distribution(Uniform)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Uniform</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.von_mises","title":"von_mises  <code>module-attribute</code>","text":"<pre><code>von_mises = tfp_distribution(VonMises)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.VonMises</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.von_mises_fisher","title":"von_mises_fisher  <code>module-attribute</code>","text":"<pre><code>von_mises_fisher = tfp_distribution(VonMisesFisher)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.VonMisesFisher</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.weibull","title":"weibull  <code>module-attribute</code>","text":"<pre><code>weibull = tfp_distribution(Weibull)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Weibull</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.zipf","title":"zipf  <code>module-attribute</code>","text":"<pre><code>zipf = tfp_distribution(Zipf)\n</code></pre> <p>A <code>tfp_distribution</code> generative function which wraps the <code>tfd.Zipf</code> distribution from TensorFlow Probability distributions.</p>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.ExactDensity","title":"ExactDensity","text":"<p>               Bases: <code>Generic[R]</code>, <code>Distribution[R]</code></p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>class ExactDensity(Generic[R], Distribution[R]):\n    @abstractmethod\n    def sample(self, key: PRNGKey, *args) -&gt; R:\n        pass\n\n    @abstractmethod\n    def logpdf(self, v: R, *args, **kwargs) -&gt; Score:\n        pass\n\n    def __abstract_call__(self, *args):\n        return to_shape_fn(self.sample, jnp.zeros)(_fake_key, *args)\n\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args,\n    ) -&gt; tuple[Score, R]:\n        \"\"\"\n        Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.\n        \"\"\"\n        v = self.sample(key, *args)\n        w = self.estimate_logpdf(key, v, *args)\n        return (w, v)\n\n    def estimate_logpdf(\n        self,\n        key: PRNGKey,\n        v: R,\n        *args,\n    ) -&gt; Weight:\n        \"\"\"\n        Given a sample and arguments to the distribution, return the exact log density of the sample.\n        \"\"\"\n        w = self.logpdf(v, *args)\n        if w.shape:\n            return jnp.sum(w)\n        else:\n            return w\n\n    def assess(\n        self,\n        sample: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Weight, R]:\n        key = jax.random.key(0)\n        v = sample.get_value()\n        match v:\n            case Mask(value, flag):\n\n                def _check():\n                    checkify.check(\n                        bool(flag),\n                        \"Attempted to unmask when a mask flag is False: the masked value is invalid.\\n\",\n                    )\n\n                optional_check(_check)\n                w = self.estimate_logpdf(key, value, *args)\n                return w, value\n            case _:\n                w = self.estimate_logpdf(key, v, *args)\n                return w, v\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.ExactDensity.estimate_logpdf","title":"estimate_logpdf","text":"<pre><code>estimate_logpdf(key: PRNGKey, v: R, *args) -&gt; Weight\n</code></pre> <p>Given a sample and arguments to the distribution, return the exact log density of the sample.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>def estimate_logpdf(\n    self,\n    key: PRNGKey,\n    v: R,\n    *args,\n) -&gt; Weight:\n    \"\"\"\n    Given a sample and arguments to the distribution, return the exact log density of the sample.\n    \"\"\"\n    w = self.logpdf(v, *args)\n    if w.shape:\n        return jnp.sum(w)\n    else:\n        return w\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.ExactDensity.random_weighted","title":"random_weighted","text":"<pre><code>random_weighted(key: PRNGKey, *args) -&gt; tuple[Score, R]\n</code></pre> <p>Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>def random_weighted(\n    self,\n    key: PRNGKey,\n    *args,\n) -&gt; tuple[Score, R]:\n    \"\"\"\n    Given arguments to the distribution, sample from the distribution, and return the exact log density of the sample, and the sample.\n    \"\"\"\n    v = self.sample(key, *args)\n    w = self.estimate_logpdf(key, v, *args)\n    return (w, v)\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.exact_density","title":"exact_density","text":"<pre><code>exact_density(\n    sample: Callable[..., R],\n    logpdf: Callable[..., Score],\n    name: str | None = None,\n) -&gt; ExactDensity[R]\n</code></pre> <p>Construct a new type, a subclass of ExactDensity, with the given name, (with <code>genjax.</code> prepended, to avoid confusion with the underlying object, which may not share the same interface) and attach the supplied functions as the <code>sample</code> and <code>logpdf</code> methods. The return value is an instance of this new type, and should be treated as a singleton.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/distribution.py</code> <pre><code>def exact_density(\n    sample: Callable[..., R], logpdf: Callable[..., Score], name: str | None = None\n) -&gt; ExactDensity[R]:\n    \"\"\"Construct a new type, a subclass of ExactDensity, with the given name,\n    (with `genjax.` prepended, to avoid confusion with the underlying object,\n    which may not share the same interface) and attach the supplied functions\n    as the `sample` and `logpdf` methods. The return value is an instance of\n    this new type, and should be treated as a singleton.\"\"\"\n    if name is None:\n        warnings.warn(\"You should supply a name argument to exact_density\")\n        name = \"unknown\"\n\n    def kwargle(f, a0, args, kwargs):\n        \"\"\"Keyword arguments currently get unusual treatment in GenJAX: when\n        a keyword argument is provided to a generative function, the function\n        is asked to provide a new version of itself which receives a different\n        signature: `(args, kwargs)` instead of `(*args, **kwargs)`. The\n        replacement of the GF with a new object may cause JAX to believe that\n        the implementations are materially different. To avoid this, we\n        reply to the handle_kwargs request with self and infer kwargs handling\n        by seeing whether we were passed a 2-tuple with a dict in the [1] slot.\n        We are assuming that this will not represent a useful argument package\n        to any of the TF distributions.\"\"\"\n        if len(args) == 2 and isinstance(args[1], dict):\n            return f(a0, *args[0], **args[1])\n        else:\n            return f(a0, *args, **kwargs)\n\n    T = type(\n        canonicalize_distribution_name(name),\n        (ExactDensity,),\n        {\n            \"sample\": lambda self, key, *args, **kwargs: kwargle(\n                sample, key, args, kwargs\n            ),\n            \"logpdf\": lambda self, v, *args, **kwargs: kwargle(logpdf, v, args, kwargs),\n            \"handle_kwargs\": lambda self: self,\n        },\n    )\n\n    return Pytree.dataclass(T)()\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.tfp_distribution","title":"tfp_distribution","text":"<pre><code>tfp_distribution(\n    dist: Callable[..., Distribution],\n    name: str | None = None,\n) -&gt; ExactDensity[Array]\n</code></pre> <p>Creates a generative function from a TensorFlow Probability distribution.</p> <p>Parameters:</p> Name Type Description Default <code>Callable[..., Distribution]</code> <p>A callable that returns a TensorFlow Probability distribution.</p> required <p>Returns:</p> Type Description <code>ExactDensity[Array]</code> <p>A generative function wrapping the TensorFlow Probability distribution.</p> <p>This function creates a generative function that encapsulates the sampling and log probability computation of a TensorFlow Probability distribution. It uses the distribution's <code>sample</code> and <code>log_prob</code> methods to define the generative function's behavior.</p> Source code in <code>src/genjax/_src/generative_functions/distributions/tensorflow_probability/__init__.py</code> <pre><code>def tfp_distribution(\n    dist: Callable[..., \"dist.Distribution\"], name: str | None = None\n) -&gt; ExactDensity[Array]:\n    \"\"\"\n    Creates a generative function from a TensorFlow Probability distribution.\n\n    Args:\n        dist: A callable that returns a TensorFlow Probability distribution.\n\n    Returns:\n        A generative function wrapping the TensorFlow Probability distribution.\n\n    This function creates a generative function that encapsulates the sampling and log probability\n    computation of a TensorFlow Probability distribution. It uses the distribution's `sample` and\n    `log_prob` methods to define the generative function's behavior.\n    \"\"\"\n\n    def sampler(key, *args, **kwargs):\n        sample_shape = kwargs.pop(\"sample_shape\", ())\n        d = dist(*args, **kwargs)\n        return d.sample(seed=key, sample_shape=Const.unwrap(sample_shape))\n\n    def logpdf(v, *args, **kwargs):\n        # Remove unused kwarg to match sampler function behavior\n        kwargs.pop(\"sample_shape\", ())\n        d = dist(*args, **kwargs)\n\n        return d.log_prob(v)\n\n    return exact_density(sampler, logpdf, name or dist.__name__)\n</code></pre>"},{"location":"library/generative_functions.html#genjax.generative_functions.distributions.tfp_distribution(dist)","title":"<code>dist</code>","text":""},{"location":"library/generative_functions.html#staticgenerativefunction-a-programmatic-language","title":"<code>StaticGenerativeFunction</code>: a programmatic language","text":"<p>For any serious work, you'll want a way to combine generative functions together, mixing deterministic functions with sampling. <code>StaticGenerativeFunction</code> is a way to do that: it supports the use of a JAX compatible subset of Python to author generative functions. It also supports the ability to invoke other generative functions: instances of this type (and any other type of generative function) can then be used in larger generative programs.</p>"},{"location":"library/generative_functions.html#genjax.StaticGenerativeFunction","title":"genjax.StaticGenerativeFunction","text":"<p>               Bases: <code>Generic[R]</code>, <code>GenerativeFunction[R]</code></p> <p>A <code>StaticGenerativeFunction</code> is a generative function which relies on program transformations applied to JAX-compatible Python programs to implement the generative function interface.</p> <p>By virtue of the implementation, any source program which is provided to this generative function must be JAX traceable, meaning all the footguns for programs that JAX exposes apply to the source program.</p> <p>Language restrictions</p> <p>In addition to JAX footguns, there are a few more which are specific to the generative function interface semantics. Here is the full list of language restrictions (and capabilities):</p> <ul> <li> <p>One is allowed to use <code>jax.lax</code> control flow primitives so long as the functions provided to the primitives do not contain <code>trace</code> invocations. In other words, utilizing control flow primitives within the source of a <code>StaticGenerativeFunction</code>'s source program requires that the control flow primitives get deterministic computation.</p> </li> <li> <p>The above restriction also applies to <code>jax.vmap</code>.</p> </li> <li> <p>Source programs are allowed to utilize untraced randomness, although there are restrictions (which we discuss below). It is required to use <code>jax.random</code> and JAX's PRNG capabilities. To utilize untraced randomness, you'll need to pass in an extra key as an argument to your model.</p> <pre><code>@gen\ndef model(key: PRNGKey):\n    v = some_untraced_call(key)\n    x = trace(\"x\", genjax.normal)(v, 1.0)\n    return x\n</code></pre> </li> </ul> <p>Methods:</p> Name Description <code>simulate</code> <code>assess</code> <p>Attributes:</p> Name Type Description <code>source</code> <code>Closure[R]</code> <p>The source program of the generative function. This is a JAX-compatible Python program.</p> Source code in <code>src/genjax/_src/generative_functions/static.py</code> <pre><code>@Pytree.dataclass\nclass StaticGenerativeFunction(Generic[R], GenerativeFunction[R]):\n    \"\"\"A `StaticGenerativeFunction` is a generative function which relies on program\n    transformations applied to JAX-compatible Python programs to implement the generative\n    function interface.\n\n    By virtue of the implementation, any source program which is provided to this generative function *must* be JAX traceable, meaning [all the footguns for programs that JAX exposes](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) apply to the source program.\n\n    **Language restrictions**\n\n    In addition to JAX footguns, there are a few more which are specific to the generative function interface semantics. Here is the full list of language restrictions (and capabilities):\n\n    * One is allowed to use `jax.lax` control flow primitives _so long as the functions provided to the primitives do not contain `trace` invocations_. In other words, utilizing control flow primitives within the source of a `StaticGenerativeFunction`'s source program requires that the control flow primitives get *deterministic* computation.\n\n    * The above restriction also applies to `jax.vmap`.\n\n    * Source programs are allowed to utilize untraced randomness, although there are restrictions (which we discuss below). It is required to use [`jax.random`](https://jax.readthedocs.io/en/latest/jax.random.html) and JAX's PRNG capabilities. To utilize untraced randomness, you'll need to pass in an extra key as an argument to your model.\n\n        ```python\n        @gen\n        def model(key: PRNGKey):\n            v = some_untraced_call(key)\n            x = trace(\"x\", genjax.normal)(v, 1.0)\n            return x\n        ```\n    \"\"\"\n\n    source: Closure[R]\n    \"\"\"\n    The source program of the generative function. This is a JAX-compatible Python program.\n    \"\"\"\n\n    def __get__(self, instance, _klass) -&gt; \"StaticGenerativeFunction[R]\":\n        \"\"\"\n        This method allows the @genjax.gen decorator to transform instance methods, turning them into `StaticGenerativeFunction[R]` calls.\n\n        NOTE: if you assign an already-created `StaticGenerativeFunction` to a variable inside of a class, it will always receive the instance as its first method.\n        \"\"\"\n        return self.partial_apply(instance) if instance else self\n\n    # To get the type of return value, just invoke\n    # the source (with abstract tracer arguments).\n    def __abstract_call__(self, *args) -&gt; Any:\n        return to_shape_fn(self.source, jnp.zeros)(*args)\n\n    def __post_init__(self):\n        wrapped = self.source.fn\n        # Preserve the original function's docstring and name\n        for k in _WRAPPER_ASSIGNMENTS:\n            v = getattr(wrapped, k, None)\n            if v is not None:\n                object.__setattr__(self, k, v)\n\n        object.__setattr__(self, \"__wrapped__\", wrapped)\n\n    def handle_kwargs(self) -&gt; \"StaticGenerativeFunction[R]\":\n        @Pytree.partial()\n        def kwarged_source(args, kwargs):\n            return self.source(*args, **kwargs)\n\n        return StaticGenerativeFunction(kwarged_source)\n\n    def simulate(\n        self,\n        key: PRNGKey,\n        args: tuple[Any, ...],\n    ) -&gt; StaticTrace[R]:\n        (args, retval, traces) = simulate_transform(self.source)(key, args)\n        return StaticTrace(self, args, retval, traces)\n\n    def generate(\n        self,\n        key: PRNGKey,\n        constraint: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[StaticTrace[R], Weight]:\n        (\n            weight,\n            # Trace.\n            (\n                args,\n                retval,\n                traces,\n            ),\n        ) = generate_transform(self.source)(key, constraint, args)\n        return StaticTrace(self, args, retval, traces), weight\n\n    def project(\n        self,\n        key: PRNGKey,\n        trace: Trace[Any],\n        selection: Selection,\n    ) -&gt; Weight:\n        assert isinstance(trace, StaticTrace)\n\n        weight = jnp.array(0.0)\n        for addr in trace.subtraces.keys():\n            subprojection = selection(addr)\n            subtrace = trace.get_subtrace(addr)\n            weight += subtrace.project(key, subprojection)\n        return weight\n\n    def edit_update(\n        self,\n        key: PRNGKey,\n        trace: StaticTrace[R],\n        constraint: ChoiceMap,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[StaticTrace[R], Weight, Retdiff[R], EditRequest]:\n        (\n            (\n                retval_diffs,\n                weight,\n                (\n                    arg_primals,\n                    retval_primals,\n                    traces,\n                ),\n                bwd_requests,\n            ),\n        ) = update_transform(self.source)(key, trace, constraint, argdiffs)\n        if not Diff.static_check_tree_diff(retval_diffs):\n            retval_diffs = Diff.no_change(retval_diffs)\n\n        def make_bwd_request(traces, subconstraints):\n            addresses = traces.keys()\n            chm = ChoiceMap.from_mapping(zip(addresses, subconstraints))\n            return Update(chm)\n\n        bwd_request = make_bwd_request(traces, bwd_requests)\n        return (\n            StaticTrace(\n                self,\n                arg_primals,\n                retval_primals,\n                traces,\n            ),\n            weight,\n            retval_diffs,\n            bwd_request,\n        )\n\n    def edit_static_edit_request(\n        self,\n        key: PRNGKey,\n        trace: StaticTrace[R],\n        addressed: StaticDict,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[StaticTrace[R], Weight, Retdiff[R], EditRequest]:\n        (\n            (\n                retval_diffs,\n                weight,\n                (\n                    arg_primals,\n                    retval_primals,\n                    traces,\n                ),\n                bwd_requests,\n            ),\n        ) = static_edit_request_transform(self.source)(key, trace, addressed, argdiffs)\n\n        def make_bwd_request(\n            traces: dict[StaticAddress, Trace[R]],\n            subrequests: list[EditRequest],\n        ):\n            return StaticRequest(dict(zip(traces.keys(), subrequests)))\n\n        bwd_request = make_bwd_request(traces, bwd_requests)\n        return (\n            StaticTrace(\n                self,\n                arg_primals,\n                retval_primals,\n                traces,\n            ),\n            weight,\n            retval_diffs,\n            bwd_request,\n        )\n\n    def edit_regenerate(\n        self,\n        key: PRNGKey,\n        trace: StaticTrace[R],\n        selection: Selection,\n        edit_request: EditRequest,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[StaticTrace[R], Weight, Retdiff[R], EditRequest]:\n        (\n            (\n                retval_diffs,\n                weight,\n                (\n                    arg_primals,\n                    retval_primals,\n                    traces,\n                ),\n                bwd_requests,\n            ),\n        ) = regenerate_transform(self.source)(\n            key, trace, selection, edit_request, argdiffs\n        )\n\n        def make_bwd_request(\n            traces: dict[StaticAddress, Trace[R]],\n            subrequests: list[EditRequest],\n        ):\n            return StaticRequest(dict(zip(traces.keys(), subrequests)))\n\n        bwd_request = make_bwd_request(traces, bwd_requests)\n        return (\n            StaticTrace(\n                self,\n                arg_primals,\n                retval_primals,\n                traces,\n            ),\n            weight,\n            retval_diffs,\n            bwd_request,\n        )\n\n    def edit(\n        self,\n        key: PRNGKey,\n        trace: Trace[R],\n        edit_request: EditRequest,\n        argdiffs: Argdiffs,\n    ) -&gt; tuple[StaticTrace[R], Weight, Retdiff[R], EditRequest]:\n        assert isinstance(trace, StaticTrace)\n        match edit_request:\n            case Update(constraint):\n                return self.edit_update(\n                    key,\n                    trace,\n                    constraint,\n                    argdiffs,\n                )\n\n            case StaticRequest(addressed):\n                return self.edit_static_edit_request(\n                    key,\n                    trace,\n                    addressed,\n                    argdiffs,\n                )\n            case Regenerate(selection):\n                return self.edit_regenerate(\n                    key,\n                    trace,\n                    selection,\n                    edit_request,\n                    argdiffs,\n                )\n            case _:\n                raise NotSupportedEditRequest(edit_request)\n\n    def assess(\n        self,\n        sample: ChoiceMap,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Score, R]:\n        (retval, score) = assess_transform(self.source)(sample, args)\n        return (score, retval)\n\n    def inline(self, *args):\n        return self.source(*args)\n\n    @property\n    def partial_args(self) -&gt; tuple[Any, ...]:\n        \"\"\"\n        Returns the partially applied arguments of the generative function.\n\n        This method retrieves the dynamically applied arguments that were used to create\n        this StaticGenerativeFunction instance through partial application.\n\n        Returns:\n            tuple[Any, ...]: A tuple containing the partially applied arguments.\n\n        Note:\n            This method is particularly useful when working with partially applied\n            generative functions, allowing access to the pre-filled arguments.\n        \"\"\"\n        return self.source.dyn_args\n\n    def partial_apply(self, *args) -&gt; \"StaticGenerativeFunction[R]\":\n        \"\"\"\n        Returns a new [`StaticGenerativeFunction`][] with the given arguments partially applied.\n\n        This method creates a new [`StaticGenerativeFunction`][] that has some of its arguments pre-filled. When called, the new function will use the pre-filled arguments along with any additional arguments provided.\n\n        Args:\n            *args: Variable length argument list to be partially applied to the function.\n\n        Returns:\n            A new [`StaticGenerativeFunction`][] with partially applied arguments.\n\n        Example:\n            ```python\n            @gen\n            def my_model(x, y):\n                z = normal(x, 1.0) @ \"z\"\n                return y * z\n\n\n            partially_applied_model = my_model.partial_apply(2.0)\n            # Now `partially_applied_model` is equivalent to a model that only takes 'y' as an argument\n            ```\n        \"\"\"\n        all_args = self.source.dyn_args + args\n        return gen(Closure[R](all_args, self.source.fn))\n</code></pre>"},{"location":"library/generative_functions.html#genjax.StaticGenerativeFunction.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source: Closure[R]\n</code></pre> <p>The source program of the generative function. This is a JAX-compatible Python program.</p>"},{"location":"library/generative_functions.html#genjax.StaticGenerativeFunction.simulate","title":"simulate","text":"<pre><code>simulate(\n    key: PRNGKey, args: tuple[Any, ...]\n) -&gt; StaticTrace[R]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/static.py</code> <pre><code>def simulate(\n    self,\n    key: PRNGKey,\n    args: tuple[Any, ...],\n) -&gt; StaticTrace[R]:\n    (args, retval, traces) = simulate_transform(self.source)(key, args)\n    return StaticTrace(self, args, retval, traces)\n</code></pre>"},{"location":"library/generative_functions.html#genjax.StaticGenerativeFunction.assess","title":"assess","text":"<pre><code>assess(\n    sample: ChoiceMap, args: tuple[Any, ...]\n) -&gt; tuple[Score, R]\n</code></pre> Source code in <code>src/genjax/_src/generative_functions/static.py</code> <pre><code>def assess(\n    self,\n    sample: ChoiceMap,\n    args: tuple[Any, ...],\n) -&gt; tuple[Score, R]:\n    (retval, score) = assess_transform(self.source)(sample, args)\n    return (score, retval)\n</code></pre>"},{"location":"library/inference.html","title":"Inference","text":"<p>Conditioning probability distributions is a commonly desired operation, allowing users to express Bayesian inference problems. Conditioning is also a subroutine in other desired operations, like marginalization.</p>"},{"location":"library/inference.html#the-language-of-inference","title":"The language of inference","text":"<p>In GenJAX, inference problems are specified by constructing <code>Target</code> distributions. Their solutions are approximated using <code>Algorithm</code> families.</p> <p>Algorithms inherit from a class called <code>SampleDistribution</code> - these are objects which implement the stochastic probability interface [Lew23], meaning they expose methods to produce samples and samples from density estimators for density computations.</p> <p><code>Algorithm</code> families implement the stochastic probability interface. Their <code>Distribution</code> methods accept <code>Target</code> instances, and produce samples and density estimates for approximate posteriors.</p> <p>By virtue of the stochastic probability interface, GenJAX also exposes marginalization as a first class concept.</p>"},{"location":"library/inference.html#genjax.inference.Target","title":"genjax.inference.Target","text":"<p>               Bases: <code>Generic[R]</code>, <code>Pytree</code></p> <p>A <code>Target</code> represents an unnormalized target distribution induced by conditioning a generative function on a <code>genjax.ChoiceMap</code>.</p> <p>Targets are created by providing a generative function, arguments to the generative function, and a constraint.</p> <p>Examples:</p> <p>Creating a target from a generative function, by providing arguments and a constraint: <pre><code>import genjax\nfrom genjax import ChoiceMapBuilder as C\nfrom genjax.inference import Target\n\n\n@genjax.gen\ndef model():\n    x = genjax.normal(0.0, 1.0) @ \"x\"\n    y = genjax.normal(x, 1.0) @ \"y\"\n    return x\n\n\ntarget = Target(model, (), C[\"y\"].set(3.0))\nprint(target.render_html())\n</code></pre> </p> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>@Pytree.dataclass\nclass Target(Generic[R], Pytree):\n    \"\"\"\n    A `Target` represents an unnormalized target distribution induced by conditioning a generative function on a [`genjax.ChoiceMap`][].\n\n    Targets are created by providing a generative function, arguments to the generative function, and a constraint.\n\n    Examples:\n        Creating a target from a generative function, by providing arguments and a constraint:\n        ```python exec=\"yes\" html=\"true\" source=\"material-block\" session=\"core\"\n        import genjax\n        from genjax import ChoiceMapBuilder as C\n        from genjax.inference import Target\n\n\n        @genjax.gen\n        def model():\n            x = genjax.normal(0.0, 1.0) @ \"x\"\n            y = genjax.normal(x, 1.0) @ \"y\"\n            return x\n\n\n        target = Target(model, (), C[\"y\"].set(3.0))\n        print(target.render_html())\n        ```\n    \"\"\"\n\n    p: Annotated[GenerativeFunction[R], Is[validate_non_marginal]]\n    args: tuple[Any, ...]\n    constraint: ChoiceMap\n\n    def importance(\n        self, key: PRNGKey, constraint: ChoiceMap\n    ) -&gt; tuple[Trace[R], Weight]:\n        merged = self.constraint.merge(constraint)\n        return self.p.importance(key, merged, self.args)\n\n    def filter_to_unconstrained(self, choice_map):\n        selection = ~self.constraint.get_selection()\n        return choice_map.filter(selection)\n\n    def __getitem__(self, addr):\n        return self.constraint[addr]\n</code></pre>"},{"location":"library/inference.html#genjax.inference.SampleDistribution","title":"genjax.inference.SampleDistribution  <code>module-attribute</code>","text":"<pre><code>SampleDistribution = Distribution[ChoiceMap]\n</code></pre> <p>The abstract class <code>SampleDistribution</code> represents the type of distributions whose return value type is a <code>ChoiceMap</code>. This is the abstract base class of <code>Algorithm</code>, as well as <code>Marginal</code>.</p>"},{"location":"library/inference.html#genjax.inference.Algorithm","title":"genjax.inference.Algorithm","text":"<p>               Bases: <code>Generic[R]</code>, <code>SampleDistribution</code></p> <p><code>Algorithm</code> is the type of inference algorithms: probabilistic programs which provide interfaces for sampling from posterior approximations, and estimating densities.</p> <p>The stochastic probability interface for <code>Algorithm</code></p> <p>Inference algorithms implement the stochastic probability interface:</p> <ul> <li> <p><code>Algorithm.random_weighted</code> exposes sampling from the approximation which the algorithm represents: it accepts a <code>Target</code> as input, representing the unnormalized distribution, and returns a sample from an approximation to the normalized distribution, along with a density estimate of the normalized distribution.</p> </li> <li> <p><code>Algorithm.estimate_logpdf</code> exposes density estimation for the approximation which <code>Algorithm.random_weighted</code> samples from: it accepts a value on the support of the approximation, and the <code>Target</code> which induced the approximation as input, and returns an estimate of the density of the approximation.</p> </li> </ul> <p>Optional methods for gradient estimators</p> <p>Subclasses of type <code>Algorithm</code> can also implement two optional methods designed to support effective gradient estimators for variational objectives (<code>estimate_normalizing_constant</code> and <code>estimate_reciprocal_normalizing_constant</code>).</p> <p>Methods:</p> Name Description <code>random_weighted</code> <p>Given a <code>Target</code>, return a <code>ChoiceMap</code> from an approximation to the normalized distribution of the target, and a random <code>Weight</code> estimate of the normalized density of the target at the sample.</p> <code>estimate_logpdf</code> <p>Given a <code>ChoiceMap</code> and a <code>Target</code>, return a random <code>Weight</code> estimate of the normalized density of the target at the sample.</p> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>class Algorithm(Generic[R], SampleDistribution):\n    \"\"\"`Algorithm` is the type of inference\n    algorithms: probabilistic programs which provide interfaces for sampling from\n    posterior approximations, and estimating densities.\n\n    **The stochastic probability interface for `Algorithm`**\n\n    Inference algorithms implement the stochastic probability interface:\n\n    * `Algorithm.random_weighted` exposes sampling from the approximation\n    which the algorithm represents: it accepts a `Target` as input, representing the\n    unnormalized distribution, and returns a sample from an approximation to\n    the normalized distribution, along with a density estimate of the normalized distribution.\n\n    * `Algorithm.estimate_logpdf` exposes density estimation for the\n    approximation which `Algorithm.random_weighted` samples from:\n    it accepts a value on the support of the approximation, and the `Target` which\n    induced the approximation as input, and returns an estimate of the density of\n    the approximation.\n\n    **Optional methods for gradient estimators**\n\n    Subclasses of type `Algorithm` can also implement two optional methods\n    designed to support effective gradient estimators for variational objectives\n    (`estimate_normalizing_constant` and `estimate_reciprocal_normalizing_constant`).\n    \"\"\"\n\n    #########\n    # GenSP #\n    #########\n\n    @abstractmethod\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args: Any,\n    ) -&gt; tuple[Score, ChoiceMap]:\n        \"\"\"\n        Given a [`Target`][genjax.inference.Target], return a [`ChoiceMap`][genjax.core.ChoiceMap] from an approximation to the normalized distribution of the target, and a random [`Weight`][genjax.core.Weight] estimate of the normalized density of the target at the sample.\n\n        The `sample` is a sample on the support of `target.gen_fn` which _are not in_ `target.constraints`, produced by running the inference algorithm.\n\n        Let $T_P(a, c)$ denote the target, with $P$ the distribution on samples represented by `target.gen_fn`, and $S$ denote the sample. Let $w$ denote the weight `w`. The weight $w$ is a random weight such that $w$ satisfies:\n\n        $$\n        \\\\mathbb{E}\\\\big[\\\\frac{1}{w} \\\\mid S \\\\big] = \\\\frac{1}{P(S \\\\mid c; a)}\n        $$\n\n        This interface corresponds to **(Defn 3.2) Unbiased Density Sampler** in [[Lew23](https://dl.acm.org/doi/pdf/10.1145/3591290)].\n        \"\"\"\n        assert isinstance(args[0], Target)\n\n    @abstractmethod\n    def estimate_logpdf(\n        self, key: PRNGKey, v: ChoiceMap, *args: tuple[Any, ...]\n    ) -&gt; Score:\n        \"\"\"\n        Given a [`ChoiceMap`][genjax.core.ChoiceMap] and a [`Target`][genjax.inference.Target], return a random [`Weight`][genjax.core.Weight] estimate of the normalized density of the target at the sample.\n\n        Let $T_P(a, c)$ denote the target, with $P$ the distribution on samples represented by `target.gen_fn`, and $S$ denote the sample. Let $w$ denote the weight `w`. The weight $w$ is a random weight such that $w$ satisfies:\n\n        $$\n        \\\\mathbb{E}[w] = P(S \\\\mid c, a)\n        $$\n\n        This interface corresponds to **(Defn 3.1) Positive Unbiased Density Estimator** in [[Lew23](https://dl.acm.org/doi/pdf/10.1145/3591290)].\n        \"\"\"\n\n    ################\n    # VI via GRASP #\n    ################\n\n    @abstractmethod\n    def estimate_normalizing_constant(\n        self,\n        key: PRNGKey,\n        target: Target[R],\n    ) -&gt; Weight:\n        pass\n\n    @abstractmethod\n    def estimate_reciprocal_normalizing_constant(\n        self,\n        key: PRNGKey,\n        target: Target[R],\n        latent_choices: ChoiceMap,\n        w: Weight,\n    ) -&gt; Weight:\n        pass\n</code></pre>"},{"location":"library/inference.html#genjax.inference.Algorithm.random_weighted","title":"random_weighted  <code>abstractmethod</code>","text":"<pre><code>random_weighted(\n    key: PRNGKey, *args: Any\n) -&gt; tuple[Score, ChoiceMap]\n</code></pre> <p>Given a <code>Target</code>, return a <code>ChoiceMap</code> from an approximation to the normalized distribution of the target, and a random <code>Weight</code> estimate of the normalized density of the target at the sample.</p> <p>The <code>sample</code> is a sample on the support of <code>target.gen_fn</code> which are not in <code>target.constraints</code>, produced by running the inference algorithm.</p> <p>Let \\(T_P(a, c)\\) denote the target, with \\(P\\) the distribution on samples represented by <code>target.gen_fn</code>, and \\(S\\) denote the sample. Let \\(w\\) denote the weight <code>w</code>. The weight \\(w\\) is a random weight such that \\(w\\) satisfies:</p> \\[ \\mathbb{E}\\big[\\frac{1}{w} \\mid S \\big] = \\frac{1}{P(S \\mid c; a)} \\] <p>This interface corresponds to (Defn 3.2) Unbiased Density Sampler in [Lew23].</p> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>@abstractmethod\ndef random_weighted(\n    self,\n    key: PRNGKey,\n    *args: Any,\n) -&gt; tuple[Score, ChoiceMap]:\n    \"\"\"\n    Given a [`Target`][genjax.inference.Target], return a [`ChoiceMap`][genjax.core.ChoiceMap] from an approximation to the normalized distribution of the target, and a random [`Weight`][genjax.core.Weight] estimate of the normalized density of the target at the sample.\n\n    The `sample` is a sample on the support of `target.gen_fn` which _are not in_ `target.constraints`, produced by running the inference algorithm.\n\n    Let $T_P(a, c)$ denote the target, with $P$ the distribution on samples represented by `target.gen_fn`, and $S$ denote the sample. Let $w$ denote the weight `w`. The weight $w$ is a random weight such that $w$ satisfies:\n\n    $$\n    \\\\mathbb{E}\\\\big[\\\\frac{1}{w} \\\\mid S \\\\big] = \\\\frac{1}{P(S \\\\mid c; a)}\n    $$\n\n    This interface corresponds to **(Defn 3.2) Unbiased Density Sampler** in [[Lew23](https://dl.acm.org/doi/pdf/10.1145/3591290)].\n    \"\"\"\n    assert isinstance(args[0], Target)\n</code></pre>"},{"location":"library/inference.html#genjax.inference.Algorithm.estimate_logpdf","title":"estimate_logpdf  <code>abstractmethod</code>","text":"<pre><code>estimate_logpdf(\n    key: PRNGKey, v: ChoiceMap, *args: tuple[Any, ...]\n) -&gt; Score\n</code></pre> <p>Given a <code>ChoiceMap</code> and a <code>Target</code>, return a random <code>Weight</code> estimate of the normalized density of the target at the sample.</p> <p>Let \\(T_P(a, c)\\) denote the target, with \\(P\\) the distribution on samples represented by <code>target.gen_fn</code>, and \\(S\\) denote the sample. Let \\(w\\) denote the weight <code>w</code>. The weight \\(w\\) is a random weight such that \\(w\\) satisfies:</p> \\[ \\mathbb{E}[w] = P(S \\mid c, a) \\] <p>This interface corresponds to (Defn 3.1) Positive Unbiased Density Estimator in [Lew23].</p> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>@abstractmethod\ndef estimate_logpdf(\n    self, key: PRNGKey, v: ChoiceMap, *args: tuple[Any, ...]\n) -&gt; Score:\n    \"\"\"\n    Given a [`ChoiceMap`][genjax.core.ChoiceMap] and a [`Target`][genjax.inference.Target], return a random [`Weight`][genjax.core.Weight] estimate of the normalized density of the target at the sample.\n\n    Let $T_P(a, c)$ denote the target, with $P$ the distribution on samples represented by `target.gen_fn`, and $S$ denote the sample. Let $w$ denote the weight `w`. The weight $w$ is a random weight such that $w$ satisfies:\n\n    $$\n    \\\\mathbb{E}[w] = P(S \\\\mid c, a)\n    $$\n\n    This interface corresponds to **(Defn 3.1) Positive Unbiased Density Estimator** in [[Lew23](https://dl.acm.org/doi/pdf/10.1145/3591290)].\n    \"\"\"\n</code></pre>"},{"location":"library/inference.html#genjax.inference.Marginal","title":"genjax.inference.Marginal","text":"<p>               Bases: <code>Generic[R]</code>, <code>SampleDistribution</code></p> <p>The <code>Marginal</code> class represents the marginal distribution of a generative function over a selection of addresses.</p> <p>Methods:</p> Name Description <code>random_weighted</code> <code>estimate_logpdf</code> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>@Pytree.dataclass\nclass Marginal(Generic[R], SampleDistribution):\n    \"\"\"The `Marginal` class represents the marginal distribution of a generative function over\n    a selection of addresses.\n    \"\"\"\n\n    gen_fn: GenerativeFunction[R]\n    selection: Selection = Pytree.field(default=Selection.all())\n    algorithm: Algorithm[R] | None = Pytree.field(default=None)\n\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args: Any,\n    ) -&gt; tuple[Score, ChoiceMap]:\n        key, sub_key = jax.random.split(key)\n        tr = self.gen_fn.simulate(sub_key, args)\n        choices: ChoiceMap = tr.get_choices()\n        latent_choices = choices.filter(self.selection)\n        key, sub_key = jax.random.split(key)\n        bwd_request = ~self.selection\n        weight = tr.project(sub_key, bwd_request)\n        if self.algorithm is None:\n            return weight, latent_choices\n        else:\n            target = Target(self.gen_fn, args, latent_choices)\n            other_choices = choices.filter(~self.selection)\n            Z = self.algorithm.estimate_reciprocal_normalizing_constant(\n                key, target, other_choices, weight\n            )\n\n            return (Z, latent_choices)\n\n    def estimate_logpdf(\n        self,\n        key: PRNGKey,\n        v: ChoiceMap,\n        *args: tuple[Any, ...],\n    ) -&gt; Score:\n        if self.algorithm is None:\n            _, weight = self.gen_fn.importance(key, v, args)\n            return weight\n        else:\n            target = Target(self.gen_fn, args, v)\n            Z = self.algorithm.estimate_normalizing_constant(key, target)\n            return Z\n</code></pre>"},{"location":"library/inference.html#genjax.inference.Marginal.random_weighted","title":"random_weighted","text":"<pre><code>random_weighted(\n    key: PRNGKey, *args: Any\n) -&gt; tuple[Score, ChoiceMap]\n</code></pre> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>def random_weighted(\n    self,\n    key: PRNGKey,\n    *args: Any,\n) -&gt; tuple[Score, ChoiceMap]:\n    key, sub_key = jax.random.split(key)\n    tr = self.gen_fn.simulate(sub_key, args)\n    choices: ChoiceMap = tr.get_choices()\n    latent_choices = choices.filter(self.selection)\n    key, sub_key = jax.random.split(key)\n    bwd_request = ~self.selection\n    weight = tr.project(sub_key, bwd_request)\n    if self.algorithm is None:\n        return weight, latent_choices\n    else:\n        target = Target(self.gen_fn, args, latent_choices)\n        other_choices = choices.filter(~self.selection)\n        Z = self.algorithm.estimate_reciprocal_normalizing_constant(\n            key, target, other_choices, weight\n        )\n\n        return (Z, latent_choices)\n</code></pre>"},{"location":"library/inference.html#genjax.inference.Marginal.estimate_logpdf","title":"estimate_logpdf","text":"<pre><code>estimate_logpdf(\n    key: PRNGKey, v: ChoiceMap, *args: tuple[Any, ...]\n) -&gt; Score\n</code></pre> Source code in <code>src/genjax/_src/inference/sp.py</code> <pre><code>def estimate_logpdf(\n    self,\n    key: PRNGKey,\n    v: ChoiceMap,\n    *args: tuple[Any, ...],\n) -&gt; Score:\n    if self.algorithm is None:\n        _, weight = self.gen_fn.importance(key, v, args)\n        return weight\n    else:\n        target = Target(self.gen_fn, args, v)\n        Z = self.algorithm.estimate_normalizing_constant(key, target)\n        return Z\n</code></pre>"},{"location":"library/inference.html#the-smc-inference-library","title":"The SMC inference library","text":"<p>Sequential Monte Carlo (SMC) is a popular algorithm for performing approximate inference in probabilistic models.</p>"},{"location":"library/inference.html#genjax.inference.smc.SMCAlgorithm","title":"genjax.inference.smc.SMCAlgorithm","text":"<p>               Bases: <code>Generic[R]</code>, <code>Algorithm[R]</code></p> <p>Abstract class for SMC algorithms.</p> Source code in <code>src/genjax/_src/inference/smc.py</code> <pre><code>class SMCAlgorithm(Generic[R], Algorithm[R]):\n    \"\"\"Abstract class for SMC algorithms.\"\"\"\n\n    @abstractmethod\n    def get_num_particles(self) -&gt; int:\n        pass\n\n    @abstractmethod\n    def get_final_target(self) -&gt; Target[R]:\n        pass\n\n    @abstractmethod\n    def run_smc(\n        self,\n        key: PRNGKey,\n    ) -&gt; ParticleCollection[R]:\n        pass\n\n    @abstractmethod\n    def run_csmc(\n        self,\n        key: PRNGKey,\n        retained: ChoiceMap,\n    ) -&gt; ParticleCollection[R]:\n        pass\n\n    # Convenience method for returning an estimate of the normalizing constant\n    # of the target.\n    def log_marginal_likelihood_estimate(\n        self,\n        key: PRNGKey,\n        target: Target[R] | None = None,\n    ):\n        if target:\n            algorithm = ChangeTarget(self, target)\n        else:\n            algorithm = self\n        key, sub_key = jrandom.split(key)\n        particle_collection = algorithm.run_smc(sub_key)\n        return particle_collection.get_log_marginal_likelihood_estimate()\n\n    #########\n    # GenSP #\n    #########\n\n    def random_weighted(\n        self,\n        key: PRNGKey,\n        *args: Any,\n    ) -&gt; tuple[Score, ChoiceMap]:\n        assert isinstance(args[0], Target)\n\n        target: Target[R] = args[0]\n        algorithm = ChangeTarget(self, target)\n        key, sub_key = jrandom.split(key)\n        particle_collection = algorithm.run_smc(key)\n        particle = particle_collection.sample_particle(sub_key)\n        log_density_estimate = (\n            particle.get_score()\n            - particle_collection.get_log_marginal_likelihood_estimate()\n        )\n        chm = target.filter_to_unconstrained(particle.get_choices())\n        return log_density_estimate, chm\n\n    def estimate_logpdf(\n        self,\n        key: PRNGKey,\n        v: ChoiceMap,\n        *args: tuple[Any, ...],\n    ) -&gt; Score:\n        assert isinstance(args[0], Target)\n\n        target: Target[R] = args[0]\n        algorithm = ChangeTarget(self, target)\n        key, sub_key = jrandom.split(key)\n        particle_collection = algorithm.run_csmc(key, v)\n        particle = particle_collection.sample_particle(sub_key)\n        log_density_estimate = (\n            particle.get_score()\n            - particle_collection.get_log_marginal_likelihood_estimate()\n        )\n        return log_density_estimate\n\n    ################\n    # VI via GRASP #\n    ################\n\n    def estimate_normalizing_constant(\n        self,\n        key: PRNGKey,\n        target: Target[R],\n    ) -&gt; FloatArray:\n        algorithm = ChangeTarget(self, target)\n        key, sub_key = jrandom.split(key)\n        particle_collection = algorithm.run_smc(sub_key)\n        return particle_collection.get_log_marginal_likelihood_estimate()\n\n    def estimate_reciprocal_normalizing_constant(\n        self,\n        key: PRNGKey,\n        target: Target[R],\n        latent_choices: ChoiceMap,\n        w: FloatArray,\n    ) -&gt; FloatArray:\n        algorithm = ChangeTarget(self, target)\n        # Special, for ChangeTarget -- to avoid a redundant reweighting step,\n        # when we have `w` which (with `latent_choices`) is already properly weighted\n        # for the `target`.\n        return algorithm.run_csmc_for_normalizing_constant(key, latent_choices, w)\n</code></pre>"},{"location":"library/inference.html#genjax.inference.smc.Importance","title":"genjax.inference.smc.Importance","text":"<p>               Bases: <code>Generic[R]</code>, <code>SMCAlgorithm[R]</code></p> <p>Accepts as input a <code>target: Target</code> and, optionally, a proposal <code>q: SampleDistribution</code>. <code>q</code> should accept a <code>Target</code> as input and return a choicemap on a subset of the addresses in <code>target.gen_fn</code> not in <code>target.constraints</code>.</p> <p>This initializes a 1-particle <code>ParticleCollection</code> by importance sampling from <code>target</code> using <code>q</code>.</p> <p>Any choices in <code>target.p</code> not in <code>q</code> will be sampled from the internal proposal distribution of <code>p</code>, given <code>target.constraints</code> and the choices sampled by <code>q</code>.</p> Source code in <code>src/genjax/_src/inference/smc.py</code> <pre><code>@Pytree.dataclass\nclass Importance(Generic[R], SMCAlgorithm[R]):\n    \"\"\"Accepts as input a `target: Target` and, optionally, a proposal `q: SampleDistribution`.\n    `q` should accept a `Target` as input and return a choicemap on a subset\n    of the addresses in `target.gen_fn` not in `target.constraints`.\n\n    This initializes a 1-particle `ParticleCollection` by importance sampling from `target` using `q`.\n\n    Any choices in `target.p` not in `q` will be sampled from the internal proposal distribution of `p`,\n    given `target.constraints` and the choices sampled by `q`.\n    \"\"\"\n\n    target: Target[R]\n    q: SampleDistribution | None = Pytree.field(default=None)\n\n    def get_num_particles(self):\n        return 1\n\n    def get_final_target(self):\n        return self.target\n\n    def run_smc(self, key: PRNGKey):\n        key, sub_key = jrandom.split(key)\n        if self.q is not None:\n            log_weight, choice = self.q.random_weighted(sub_key, self.target)\n            tr, target_score = self.target.importance(key, choice)\n        else:\n            log_weight = 0.0\n            tr, target_score = self.target.importance(key, ChoiceMap.empty())\n        return ParticleCollection(\n            jtu.tree_map(lambda v: jnp.expand_dims(v, axis=0), tr),\n            jnp.array([target_score - log_weight]),\n            jnp.array(True),\n        )\n\n    def run_csmc(self, key: PRNGKey, retained: ChoiceMap):\n        key, sub_key = jrandom.split(key)\n        if self.q:\n            q_score = self.q.estimate_logpdf(sub_key, retained, self.target)\n        else:\n            q_score = 0.0\n        target_trace, target_score = self.target.importance(key, retained)\n        return ParticleCollection(\n            jtu.tree_map(lambda v: jnp.expand_dims(v, axis=0), target_trace),\n            jnp.array([target_score - q_score]),\n            jnp.array(True),\n        )\n</code></pre>"},{"location":"library/inference.html#genjax.inference.smc.ImportanceK","title":"genjax.inference.smc.ImportanceK","text":"<p>               Bases: <code>Generic[R]</code>, <code>SMCAlgorithm[R]</code></p> <p>Given a <code>target: Target</code> and a proposal <code>q: SampleDistribution</code>, as well as the number of particles <code>k_particles: int</code>, initialize a particle collection using importance sampling.</p> Source code in <code>src/genjax/_src/inference/smc.py</code> <pre><code>@Pytree.dataclass\nclass ImportanceK(Generic[R], SMCAlgorithm[R]):\n    \"\"\"Given a `target: Target` and a proposal `q: SampleDistribution`, as well as the\n    number of particles `k_particles: int`, initialize a particle collection using\n    importance sampling.\"\"\"\n\n    target: Target[R]\n    q: SampleDistribution | None = Pytree.field(default=None)\n    k_particles: int = Pytree.static(default=2)\n\n    def get_num_particles(self):\n        return self.k_particles\n\n    def get_final_target(self):\n        return self.target\n\n    def run_smc(self, key: PRNGKey):\n        key, sub_key = jrandom.split(key)\n        sub_keys = jrandom.split(sub_key, self.get_num_particles())\n        if self.q is not None:\n            log_weights, choices = vmap(self.q.random_weighted, in_axes=(0, None))(\n                sub_keys, self.target\n            )\n            trs, target_scores = vmap(self.target.importance)(sub_keys, choices)\n        else:\n            log_weights = 0.0\n            trs, target_scores = vmap(self.target.importance, in_axes=(0, None))(\n                sub_keys, ChoiceMap.empty()\n            )\n        return ParticleCollection(\n            trs,\n            target_scores - log_weights,\n            jnp.array(True),\n        )\n\n    def run_csmc(self, key: PRNGKey, retained: ChoiceMap):\n        key, sub_key = jrandom.split(key)\n        sub_keys = jrandom.split(sub_key, self.get_num_particles() - 1)\n        if self.q:\n            log_scores, choices = vmap(self.q.random_weighted, in_axes=(0, None))(\n                sub_keys, self.target\n            )\n            retained_choice_score = self.q.estimate_logpdf(key, retained, self.target)\n            stacked_choices = jtu.tree_map(stack_to_first_dim, choices, retained)\n            stacked_scores = jtu.tree_map(\n                stack_to_first_dim, log_scores, retained_choice_score\n            )\n            sub_keys = jrandom.split(key, self.get_num_particles())\n            target_traces, target_scores = vmap(self.target.importance)(\n                sub_keys, stacked_choices\n            )\n        else:\n            ignored_traces, ignored_scores = vmap(\n                self.target.importance, in_axes=(0, None)\n            )(sub_keys, ChoiceMap.empty())\n            retained_trace, retained_choice_score = self.target.importance(\n                key, retained\n            )\n            target_scores = jtu.tree_map(\n                stack_to_first_dim, ignored_scores, retained_choice_score\n            )\n            stacked_scores = 0.0\n            target_traces = jtu.tree_map(\n                stack_to_first_dim, ignored_traces, retained_trace\n            )\n        return ParticleCollection(\n            target_traces,\n            target_scores - stacked_scores,\n            jnp.array(True),\n        )\n</code></pre>"},{"location":"library/inference.html#the-vi-inference-library","title":"The VI inference library","text":"<p>Variational inference is an approach to inference which involves solving optimization problems over spaces of distributions. For a posterior inference problem, the goal is to find the distribution in some parametrized family of distributions (often called the guide family) which is close to the posterior under some notion of distance.</p> <p>Variational inference problems typically involve optimization functions which are defined as expectations, and these expectations and their analytic gradients are often intractable to compute. Therefore, unbiased gradient estimators are used to approximate the true gradients.</p> <p>The <code>genjax.vi</code> inference module provides automation for constructing variational losses, and deriving gradient estimators. The architecture is shown below.</p> Fig. 1: How variational inference works in GenJAX."},{"location":"library/inference.html#genjax.inference.vi.adev_distribution","title":"genjax.inference.vi.adev_distribution","text":"<pre><code>adev_distribution(\n    adev_primitive: ADEVPrimitive,\n    differentiable_logpdf: Callable[..., Any],\n    name: str,\n) -&gt; ExactDensity[Any]\n</code></pre> <p>Return an <code>ExactDensity</code> distribution whose sampler invokes an ADEV sampling primitive, with a provided differentiable log density function.</p> <p>Exact densities created using this function can be used as distributions in variational guide programs.</p> Source code in <code>src/genjax/_src/inference/vi.py</code> <pre><code>def adev_distribution(\n    adev_primitive: ADEVPrimitive, differentiable_logpdf: Callable[..., Any], name: str\n) -&gt; ExactDensity[Any]:\n    \"\"\"\n    Return an [`ExactDensity`][genjax.ExactDensity] distribution whose sampler invokes an ADEV sampling primitive, with a provided differentiable log density function.\n\n    Exact densities created using this function can be used as distributions in variational guide programs.\n    \"\"\"\n\n    def sampler(key: PRNGKey, *args: Any) -&gt; Any:\n        return sample_primitive(adev_primitive, *args, key=key)\n\n    def logpdf(v: Any, *args: Any) -&gt; FloatArray:\n        lp = differentiable_logpdf(v, *args)\n        # Branching here is statically resolved.\n        if lp.shape:\n            return jnp.sum(lp)\n        else:\n            return lp\n\n    return exact_density(sampler, logpdf, name)\n</code></pre>"},{"location":"library/inference.html#genjax.inference.vi.ELBO","title":"genjax.inference.vi.ELBO","text":"<pre><code>ELBO(\n    guide: SampleDistribution,\n    make_target: Callable[..., Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]\n</code></pre> <p>Return a function that computes the gradient estimate of the ELBO loss term.</p> Source code in <code>src/genjax/_src/inference/vi.py</code> <pre><code>def ELBO(\n    guide: SampleDistribution,\n    make_target: Callable[..., Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]:\n    \"\"\"\n    Return a function that computes the gradient estimate of the ELBO loss term.\n    \"\"\"\n\n    def grad_estimate(\n        key: PRNGKey,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Any, ...]:\n        # In the source language of ADEV.\n        @expectation\n        def _loss(*args):\n            target = make_target(*args)\n            guide_alg = Importance(target, guide)\n            w = guide_alg.estimate_normalizing_constant(key, target)\n            return -w\n\n        return _loss.grad_estimate(key, args)\n\n    return grad_estimate\n</code></pre>"},{"location":"library/inference.html#genjax.inference.vi.IWELBO","title":"genjax.inference.vi.IWELBO","text":"<pre><code>IWELBO(\n    proposal: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n    N: int,\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]\n</code></pre> <p>Return a function that computes the gradient estimate of the IWELBO loss term.</p> Source code in <code>src/genjax/_src/inference/vi.py</code> <pre><code>def IWELBO(\n    proposal: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n    N: int,\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]:\n    \"\"\"\n    Return a function that computes the gradient estimate of the IWELBO loss term.\n    \"\"\"\n\n    def grad_estimate(\n        key: PRNGKey,\n        args: Arguments,\n    ) -&gt; GradientEstimate:\n        # In the source language of ADEV.\n        @expectation\n        def _loss(*args):\n            target = make_target(*args)\n            guide = ImportanceK(target, proposal, N)\n            w = guide.estimate_normalizing_constant(key, target)\n            return -w\n\n        return _loss.grad_estimate(key, args)\n\n    return grad_estimate\n</code></pre>"},{"location":"library/inference.html#genjax.inference.vi.PWake","title":"genjax.inference.vi.PWake","text":"<pre><code>PWake(\n    posterior_approx: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]\n</code></pre> <p>Return a function that computes the gradient estimate of the PWake loss term.</p> Source code in <code>src/genjax/_src/inference/vi.py</code> <pre><code>def PWake(\n    posterior_approx: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]:\n    \"\"\"\n    Return a function that computes the gradient estimate of the PWake loss term.\n    \"\"\"\n\n    def grad_estimate(\n        key: PRNGKey,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Any, ...]:\n        key, sub_key1, sub_key2 = jax.random.split(key, 3)\n\n        # In the source language of ADEV.\n        @expectation\n        def _loss(*target_args):\n            target = make_target(*target_args)\n            _, sample = posterior_approx.random_weighted(sub_key1, target)\n            tr, _ = target.importance(sub_key2, sample)\n            return -tr.get_score()\n\n        return _loss.grad_estimate(key, args)\n\n    return grad_estimate\n</code></pre>"},{"location":"library/inference.html#genjax.inference.vi.QWake","title":"genjax.inference.vi.QWake","text":"<pre><code>QWake(\n    proposal: SampleDistribution,\n    posterior_approx: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]\n</code></pre> <p>Return a function that computes the gradient estimate of the QWake loss term.</p> Source code in <code>src/genjax/_src/inference/vi.py</code> <pre><code>def QWake(\n    proposal: SampleDistribution,\n    posterior_approx: SampleDistribution,\n    make_target: Callable[[Any], Target[Any]],\n) -&gt; Callable[[PRNGKey, Arguments], GradientEstimate]:\n    \"\"\"\n    Return a function that computes the gradient estimate of the QWake loss term.\n    \"\"\"\n\n    def grad_estimate(\n        key: PRNGKey,\n        args: tuple[Any, ...],\n    ) -&gt; tuple[Any, ...]:\n        key, sub_key1, sub_key2 = jax.random.split(key, 3)\n\n        # In the source language of ADEV.\n        @expectation\n        def _loss(*target_args):\n            target = make_target(*target_args)\n            _, sample = posterior_approx.random_weighted(sub_key1, target)\n            w = proposal.estimate_logpdf(sub_key2, sample, target)\n            return -w\n\n        return _loss.grad_estimate(key, args)\n\n    return grad_estimate\n</code></pre>"}]}