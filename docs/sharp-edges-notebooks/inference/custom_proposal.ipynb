{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm doing importance sampling as advised but it's bad, what can I do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing one can do is write a custom proposal for importance sampling.\n",
    "The idea is to sample from this one instead of the default one used by genjax when using `model.importance`.\n",
    "The default one is only informed by the structure of the model, and not by the posterior defined by both the model and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define a simple model with a broad normal prior and some observations\n",
    "from genjax import gen, normal\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from jax import random, jit, vmap\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "@gen\n",
    "def model():\n",
    "    # Initially, the prior is a pretty broad normal distribution centred at 0\n",
    "    x = normal(0.0, 100.0) @ \"x\"\n",
    "    # We add some observations, which will shift the posterior towards these values\n",
    "    _ = normal(x, 1.0) @ \"obs1\"\n",
    "    _ = normal(x, 1.0) @ \"obs2\"\n",
    "    _ = normal(x, 1.0) @ \"obs3\"\n",
    "    return x\n",
    "\n",
    "\n",
    "# We create some data, 3 observed values at 234\n",
    "obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)\n",
    "\n",
    "# We then run importance sampling with a default proposal\n",
    "# And print the average weight of the samples, to give us a sense of how well the proposal is doing\n",
    "key = random.PRNGKey(0)\n",
    "key, *sub_keys = random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args = ()\n",
    "jitted = jit(vmap(model.importance, in_axes=(0, None, None)))\n",
    "trace, weight = jitted(sub_keys, obs, args)\n",
    "\n",
    "# We can see that both the average and even maximum weight are quite low, which means that the proposal is not doing a great job.\n",
    "# Ideally the weight should center around 1 and be quite concentrated around that value.\n",
    "print(\n",
    "    \"The average weight is\", jax.scipy.special.logsumexp(weight) - jnp.log(len(weight))\n",
    ")\n",
    "print(\"The maximum weight is\", weight.max())\n",
    "\n",
    "# We now define a custom proposal, which will be a normal distribution centred around the observed values\n",
    "\n",
    "\n",
    "@gen\n",
    "def proposal(obs):\n",
    "    avg_val = jnp.array(obs).mean()\n",
    "    std = jnp.array(obs).std()\n",
    "    x = (\n",
    "        normal(avg_val, 0.1 + std) @ \"x\"\n",
    "    )  # To avoid a degenerate proposal, we add a small value to the standard deviation\n",
    "    # Note that this is not very elegant as we'd like to only propose the latent variable `x`, but we need to add the observations to get a full trace\n",
    "    _ = normal(x, 1.0) @ \"obs1\"\n",
    "    _ = normal(x, 1.0) @ \"obs2\"\n",
    "    _ = normal(x, 1.0) @ \"obs3\"\n",
    "    return x\n",
    "\n",
    "\n",
    "# To do things by hand first, let's reimplement the importance function\n",
    "# It samples from the proposal and then computes the importance weight\n",
    "def importance_sample(hard, easy):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        trace = easy.simulate(key, *easy_args)\n",
    "        chm = trace.get_sample()\n",
    "        easy_logpdf = trace.get_score()\n",
    "        hard_logpdf, _ = hard.assess(chm, *hard_args)\n",
    "        importance_weight = hard_logpdf - easy_logpdf\n",
    "        return (trace, importance_weight)\n",
    "\n",
    "    return _inner\n",
    "\n",
    "\n",
    "# We then run importance sampling with the custom proposal\n",
    "key = random.PRNGKey(0)\n",
    "key, *sub_keys = random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args_for_model = ()\n",
    "args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\n",
    "jitted = jit(vmap(importance_sample(model, proposal), in_axes=(0, None, None)))\n",
    "trace, new_weight = jitted(sub_keys, (args_for_model,), (args_for_proposal,))\n",
    "# We see that the new values, both average and maximum, are much higher than before, which means that the custom proposal is doing a much better job\n",
    "print(\n",
    "    \"The new average weight is\",\n",
    "    jax.scipy.special.logsumexp(new_weight) - jnp.log(len(new_weight)),\n",
    ")\n",
    "print(\"The new maximum weight is\", new_weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An equivalent way to do this is to use the importance function from the genjax library\n",
    "\n",
    "from genjax import Target, smc\n",
    "from jax import random, vmap\n",
    "\n",
    "k_particles = 1000\n",
    "args_for_model = (0.0, 1.0)\n",
    "args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\n",
    "key = random.PRNGKey(0)\n",
    "target_posterior = Target(model, (args_for_model,), obs)\n",
    "# TODO: find a way to see the proposal as a SampleDistribution.\n",
    "# TODO: find a more elegant way to define the proposal only on the latent variable x\n",
    "proposal = Target(proposal, args_for_proposal, ())\n",
    "alg = smc.ImportanceK(target_posterior, proposal, k_particles=k_particles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
