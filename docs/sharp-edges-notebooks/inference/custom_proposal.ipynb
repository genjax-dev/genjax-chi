{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm doing importance sampling as advised but it's bad, what can I do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing one can do is write a custom proposal for importance sampling.\n",
    "The idea is to sample from this one instead of the default one used by genjax when using `model.importance`.\n",
    "The default one is only informed by the structure of the model, and not by the posterior defined by both the model and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average weight is -9.114648\n",
      "The maximum weight is -9.121351\n",
      "The new average weight is -0.808337\n",
      "The new maximum weight is -1.6441832\n"
     ]
    }
   ],
   "source": [
    "# Let's first define a simple model with a broad normal prior and some observations\n",
    "from genjax import gen, normal\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from jax import random, jit, vmap\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "@gen\n",
    "def model():\n",
    "    # Initially, the prior is a pretty broad normal distribution centred at 0\n",
    "    x = normal(0.0, 100.0) @ \"x\"\n",
    "    # We add some observations, which will shift the posterior towards these values\n",
    "    _ = normal(x, 1.0) @ \"obs1\"\n",
    "    _ = normal(x, 1.0) @ \"obs2\"\n",
    "    _ = normal(x, 1.0) @ \"obs3\"\n",
    "    return x\n",
    "\n",
    "# We create some data, 3 observed values at 234\n",
    "obs = C[\"obs1\"].set(234.0) ^ C[\"obs2\"].set(234.0) ^ C[\"obs3\"].set(234.0)\n",
    "\n",
    "# We then run importance sampling with a default proposal\n",
    "# And print the average weight of the samples, to give us a sense of how well the proposal is doing\n",
    "key = random.PRNGKey(0)\n",
    "key, *sub_keys = random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args = ()\n",
    "jitted = jit(vmap(model.importance, in_axes=(0,None,None)))\n",
    "trace, weight = jitted(sub_keys, obs, args)\n",
    "\n",
    "# We can see that both the average and even maximum weight are quite low, which means that the proposal is not doing a great job.\n",
    "# Ideally the weight should center around 1 and be quite concentrated around that value.\n",
    "print(\"The average weight is\", jax.scipy.special.logsumexp(weight))\n",
    "print(\"The maximum weight is\", weight.max())\n",
    "\n",
    "# We now define a custom proposal, which will be a normal distribution centred around the observed values\n",
    "\n",
    "@gen\n",
    "def proposal(obs):\n",
    "    avg_val = jnp.array(obs).mean()\n",
    "    std = jnp.array(obs).std()\n",
    "    x = normal(avg_val, 0.1 + std) @ \"x\" # To avoid a degenerate proposal, we add a small value to the standard deviation\n",
    "    # Note that this is not very elegant as we'd like to only propose the latent variable `x`, but we need to add the observations to get a full trace\n",
    "    _ = normal(x, 1.0) @ \"obs1\"\n",
    "    _ = normal(x, 1.0) @ \"obs2\"\n",
    "    _ = normal(x, 1.0) @ \"obs3\"\n",
    "    return x\n",
    "\n",
    "# To do things by hand first, let's reimplement the importance function\n",
    "# It samples from the proposal and then computes the importance weight\n",
    "def importance_sample(hard, easy):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        trace = easy.simulate(key, *easy_args)   \n",
    "        chm = trace.get_sample()\n",
    "        easy_logpdf = trace.get_score()  \n",
    "        hard_logpdf, _ = hard.assess(chm, *hard_args) \n",
    "        importance_weight = hard_logpdf - easy_logpdf\n",
    "        return (trace, importance_weight) \n",
    "    return _inner\n",
    "\n",
    "# We then run importance sampling with the custom proposal\n",
    "key = random.PRNGKey(0)\n",
    "key, *sub_keys = random.split(key, 1000 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "args_for_model = ()\n",
    "args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\n",
    "jitted = jit(vmap(importance_sample(model, proposal), in_axes=(0, None, None)))\n",
    "trace, new_weight = jitted(sub_keys, (args_for_model,), (args_for_proposal,))\n",
    "# We see that the new values, both average and maximum, are much higher than before, which means that the custom proposal is doing a much better job\n",
    "print(\"The new average weight is\", jax.scipy.special.logsumexp(new_weight))\n",
    "print(\"The new maximum weight is\", new_weight.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# An equivalent way to do this is to use the importance function from the genjax library\n",
    "\n",
    "from genjax import Target, smc\n",
    "from jax import random, vmap\n",
    "\n",
    "k_particles = 1000\n",
    "args_for_model = (0.0, 1.0)\n",
    "args_for_proposal = (jnp.array([obs[\"obs1\"], obs[\"obs2\"], obs[\"obs3\"]]),)\n",
    "key = random.PRNGKey(0)\n",
    "target_posterior = Target(model, (args_for_model,), obs)\n",
    "# TODO: find a way to see the proposal as a SampleDistribution.\n",
    "# TODO: find a more elegant way to define the proposal only on the latent variable x\n",
    "proposal = Target(proposal, args_for_proposal, ())\n",
    "alg = smc.ImportanceK(target_posterior, proposal, k_particles=k_particles)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
